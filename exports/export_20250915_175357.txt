# GuignoMap - Export de code complet
# Date : 2025-09-15 14:30:00 local
# Version : v5.0 (Phase 1 bouclÃ©e) â€” base v4.1 UI/UX conservÃ©e
# Auteur : Nick & Copilot
# Projet : SystÃ¨me de gestion pour la GuignolÃ©e 2025
# Encodage : UTF-8 (sans BOM)

## RÃ‰SUMÃ‰ DES Ã‰VOLUTIONS RÃ‰CENTES
- Infra PHASE 1: SQLiteâ†’PostgreSQL (SQLAlchemy + Alembic), Storage S3/local, Auth passlib Argon2 (compat bcrypt), correctifs DataFrame (create_map + UI), adapters df.
- IPv6 local: dev SQLite OK; prod Streamlit Cloud (IPv6) â€” migrations exÃ©cutÃ©es cÃ´tÃ© Cloud.
- Politique UI MDP v4.1 inchangÃ©e: min 4 + confirmation.

## ENVIRONNEMENT & DÃ‰PENDANCES
- Python: 3.11+ (dÃ©tectÃ©)
- Principales libs (versions exactes depuis requirements): 
  * streamlit>=1.36.0
  * pandas>=2.2.0
  * folium==0.20.0
  * streamlit-folium>=0.21.0
  * sqlalchemy==2.0.23
  * alembic==1.13.1
  * psycopg2-binary>=2.9.7
  * passlib[argon2]==1.7.4
  * boto3==1.34.144
  * bcrypt>=4.0.0

- Variables/Secrets attendus (placeholders, ne JAMAIS imprimer les vraies valeurs):
  [database] url="postgresql://<user>:<pass>@<host>:<port>/<db>" pool_size=<int> max_overflow=<int>
  [storage] s3_bucket="<bucket>" s3_region="<region>" s3_access_key="<AKIAâ€¦>" s3_secret_key="<***>" cdn_base_url="<https://â€¦>" (optionnel)
  [admin] token="<ADMIN_TOKEN>" (si page admin migration)

## ARBORESCENCE (repo root)
GuignoMap/
â”œâ”€â”€ .git/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .streamlit/
â”œâ”€â”€ .venv/
â”œâ”€â”€ .vscode/
â”œâ”€â”€ alembic.ini
â”œâ”€â”€ AUDIT_DATAFRAME.md
â”œâ”€â”€ audit_dataframe.ps1
â”œâ”€â”€ backups/
â”œâ”€â”€ exports/
â”œâ”€â”€ fix_app_types.py
â”œâ”€â”€ fix_specific.py
â”œâ”€â”€ guignomap/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ backup.py
â”‚   â”œâ”€â”€ db_v5.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ guigno_map.db
â”‚   â”œâ”€â”€ osm_addresses.json
â”‚   â”œâ”€â”€ osm_cache.json
â”‚   â”œâ”€â”€ osm.py
â”‚   â”œâ”€â”€ reports.py
â”‚   â”œâ”€â”€ validators.py
â”‚   â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ backups/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ GuignoMap_code_export_20250914_audit.txt
â”œâ”€â”€ GuignoMap_code_export_20250915_final_UTF8.txt
â”œâ”€â”€ lancer_guignomap.bat
â”œâ”€â”€ lancer_guignomap.ps1
â”œâ”€â”€ PHASE1_COMMANDS.md
â”œâ”€â”€ PROBLEME_IPv6_SUPABASE.md
â”œâ”€â”€ README.md
â”œâ”€â”€ README_VENV.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ migrate_password_hashes.py
â”‚   â””â”€â”€ migrate_sqlite_to_postgres.py
â”œâ”€â”€ scripts_validation_dataframe.ps1
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ passwords.py
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ migrations/
â”‚   â”‚       â”œâ”€â”€ env.py
â”‚   â”‚       â”œâ”€â”€ README
â”‚   â”‚       â”œâ”€â”€ script.py.mako
â”‚   â”‚       â””â”€â”€ versions/ (empty)
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cloud.py
â”‚   â”‚   â””â”€â”€ local.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ adapters.py
â”œâ”€â”€ test_db_connection.py
â”œâ”€â”€ test_db_simple.py
â””â”€â”€ tools/
    â””â”€â”€ quick_sanity.py

## FICHIERS NOUVEAUX OU MODIFIÃ‰S (PHASE 1)
- src/config.py (NEW)
- src/database/connection.py (NEW)
- src/database/models.py (NEW)
- src/database/migrations/env.py (NEW)
- src/database/migrations/versions/ (NEW - vide, aucune rÃ©vision crÃ©Ã©e)
- src/auth/passwords.py (NEW)
- src/storage/cloud.py (NEW)
- src/storage/local.py (NEW)
- src/utils/adapters.py (NEW, DataFrame adapter)
- src/utils/__init__.py (NEW)
- scripts/migrate_sqlite_to_postgres.py (NEW)
- scripts/migrate_password_hashes.py (NEW)
- guignomap/app.py (MOD - correctifs DataFrame PHASE 1)
- guignomap/db_v5.py (MOD - adaptations SQLAlchemy)
- alembic.ini (NEW)

## CODE â€” NOUVEAUX FICHIERS (contenu COMPLET)

### src/config.py
```python
"""
Configuration centralisÃ©e pour GuignoMap v5.0
AccÃ¨s aux secrets Streamlit et paramÃ¨tres applicatifs
"""
import streamlit as st
import os


def get_database_url():
    """RÃ©cupÃ¨re l'URL de la base de donnÃ©es depuis les secrets"""
    try:
        return st.secrets["database"]["url"]
    except (KeyError, AttributeError):
        # Fallback pour dÃ©veloppement local ou tests
        return os.getenv("DATABASE_URL", "sqlite:///guigno_map.db")


def get_database_pool_config():
    """Configuration du pool de connexions PostgreSQL"""
    try:
        return {
            "pool_size": st.secrets["database"].get("pool_size", 5),
            "max_overflow": st.secrets["database"].get("max_overflow", 10)
        }
    except (KeyError, AttributeError):
        return {"pool_size": 5, "max_overflow": 10}


def get_s3_config():
    """Configuration S3 pour le stockage cloud"""
    try:
        return {
            "bucket": st.secrets["storage"]["s3_bucket"],
            "region": st.secrets["storage"]["s3_region"],
            "access_key": st.secrets["storage"]["s3_access_key"],
            "secret_key": st.secrets["storage"]["s3_secret_key"]
        }
    except (KeyError, AttributeError):
        return {
            "bucket": os.getenv("S3_BUCKET", "guignomap-dev"),
            "region": os.getenv("S3_REGION", "us-east-1"),
            "access_key": os.getenv("S3_ACCESS_KEY", ""),
            "secret_key": os.getenv("S3_SECRET_KEY", "")
        }


def get_cdn_base_url():
    """URL de base CDN pour les assets (optionnel)"""
    try:
        return st.secrets["storage"].get("cdn_base_url", "")
    except (KeyError, AttributeError):
        return os.getenv("CDN_BASE_URL", "")
```

### src/database/connection.py
```python
"""
Connexion PostgreSQL avec SQLAlchemy pour GuignoMap v5.0
Engine + QueuePool + cache Streamlit + retry logic
"""
import time
import functools
import streamlit as st
from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError
from src.config import get_database_url, get_database_pool_config


def db_retry(max_retries=3, backoff_factor=1):
    """
    DÃ©corateur retry exponentiel pour opÃ©rations DB critiques
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except SQLAlchemyError as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        wait_time = backoff_factor * (2 ** attempt)
                        print(f"Retry DB operation {func.__name__} in {wait_time}s (attempt {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"Max retries reached for {func.__name__}")
                        break
            raise last_exception
        return wrapper
    return decorator


@st.cache_resource
def get_engine():
    """
    Engine PostgreSQL avec cache Streamlit et configuration pool
    ConformÃ©ment au plan v5.0
    """
    database_url = get_database_url()
    pool_config = get_database_pool_config()
    
    # Configuration PostgreSQL avec QueuePool
    engine = create_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=pool_config["pool_size"],
        max_overflow=pool_config["max_overflow"],
        pool_pre_ping=True,
        pool_recycle=300,
        echo=False  # Set to True for SQL debugging
    )
    
    return engine


def get_session():
    """Fabrique de session SQLAlchemy"""
    engine = get_engine()
    Session = sessionmaker(bind=engine)
    return Session()


@db_retry(max_retries=3)
def test_connection():
    """Test de connexion Ã  la base PostgreSQL"""
    try:
        engine = get_engine()
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1 as test"))
            return result.fetchone()[0] == 1
    except Exception as e:
        print(f"Erreur test connexion DB: {e}")
        return False


@db_retry(max_retries=3)
def execute_query(query, params=None):
    """
    ExÃ©cution de requÃªte avec retry automatique
    Pour transition progressive vers SQLAlchemy
    """
    engine = get_engine()
    with engine.connect() as conn:
        if params:
            return conn.execute(text(query), params)
        else:
            return conn.execute(text(query))


@db_retry(max_retries=3)  
def execute_transaction(queries_and_params):
    """
    ExÃ©cution de transaction multi-requÃªtes avec retry
    queries_and_params: liste de tuples (query, params)
    """
    engine = get_engine()
    with engine.begin() as conn:
        results = []
        for query, params in queries_and_params:
            if params:
                result = conn.execute(text(query), params)
            else:
                result = conn.execute(text(query))
            results.append(result)
        return results
```

### src/database/models.py
```python
"""
ModÃ¨les SQLAlchemy pour GuignoMap v5.0
BasÃ©s sur le schÃ©ma SQLite existant pour compatibilitÃ©
"""
from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, ForeignKey, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()


class Street(Base):
    __tablename__ = 'streets'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(Text, nullable=False, unique=True)
    sector = Column(Text)
    team = Column(Text)
    status = Column(Text, nullable=False, default='a_faire')
    
    # Relations
    notes = relationship("Note", back_populates="street", cascade="all, delete-orphan")
    addresses = relationship("Address", back_populates="street", cascade="all, delete-orphan")


class Team(Base):
    __tablename__ = 'teams'
    
    id = Column(Text, primary_key=True)
    name = Column(Text, nullable=False)
    password_hash = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    active = Column(Boolean, default=True)


class Note(Base):
    __tablename__ = 'notes'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    team_id = Column(Text, ForeignKey('teams.id'), nullable=False)
    address_number = Column(Text)
    comment = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="notes")


class ActivityLog(Base):
    __tablename__ = 'activity_log'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    team_id = Column(Text)
    action = Column(Text, nullable=False)
    details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)


class Address(Base):
    __tablename__ = 'addresses'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    house_number = Column(Text, nullable=False)
    latitude = Column(Float)
    longitude = Column(Float)
    osm_type = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="addresses")
```

### src/database/migrations/env.py
```python
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# Ajouter le rÃ©pertoire parent au PYTHONPATH pour imports
import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from src.database.models import Base
from src.config import get_database_url

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_url():
    """RÃ©cupÃ¨re l'URL de BDD depuis la configuration Streamlit"""
    return get_database_url()


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    # Override URL from config
    configuration = config.get_section(config.config_ini_section)
    configuration['sqlalchemy.url'] = get_url()
    
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### src/auth/passwords.py
```python
"""
Gestion des mots de passe pour GuignoMap v5.0
Migration bcrypt â†’ Argon2 avec compatibilitÃ© ascendante
"""
import re
from typing import Optional, Dict, Tuple
from passlib.context import CryptContext


# Configuration passlib avec Argon2 (principal) + bcrypt (legacy)
pwd_context = CryptContext(
    schemes=["argon2", "bcrypt"],
    deprecated="auto",  # Auto-migrate bcrypt vers argon2
    argon2__memory_cost=65536,  # 64 MB
    argon2__time_cost=3,        # 3 iterations
    argon2__parallelism=1,      # Single thread pour Streamlit Cloud
)


def hash_password(password: str) -> str:
    """
    Hasher un mot de passe avec Argon2
    """
    return pwd_context.hash(password)


def verify_password(password: str, hashed: str) -> bool:
    """
    VÃ©rifier un mot de passe contre son hash
    Compatible bcrypt + Argon2
    """
    try:
        result = pwd_context.verify(password, hashed)
        return result
    except Exception as e:
        print(f"Erreur vÃ©rification mot de passe: {e}")
        return False


def needs_rehash(hashed: str) -> bool:
    """
    VÃ©rifier si un hash doit Ãªtre re-hashÃ© (migration bcrypt â†’ Argon2)
    """
    try:
        return pwd_context.needs_update(hashed)
    except Exception:
        return True  # En cas de doute, re-hasher


def validate_password_policy(password: str) -> Tuple[bool, Optional[str]]:
    """
    Valider la politique de mot de passe v4.1 (min 4 + confirmation)
    Conserve la compatibilitÃ© UI existante
    """
    if not password:
        return False, "Le mot de passe ne peut pas Ãªtre vide"
    
    if len(password) < 4:
        return False, "Le mot de passe doit contenir au moins 4 caractÃ¨res"
    
    # Pas d'autres restrictions pour garder l'UX v4.1
    return True, None


def get_password_strength(password: str) -> Dict[str, any]:
    """
    Ã‰valuer la force d'un mot de passe (pour affichage optionnel)
    """
    if not password:
        return {"score": 0, "feedback": "Vide"}
    
    score = 0
    feedback = []
    
    # CritÃ¨res de force
    if len(password) >= 8:
        score += 2
    elif len(password) >= 6:
        score += 1
    else:
        feedback.append("Trop court")
    
    if re.search(r'[a-z]', password):
        score += 1
    
    if re.search(r'[A-Z]', password):
        score += 1
    
    if re.search(r'\d', password):
        score += 1
    
    if re.search(r'[^a-zA-Z0-9]', password):
        score += 1
    
    # Classification
    if score <= 2:
        strength = "Faible"
    elif score <= 4:
        strength = "Moyen"
    else:
        strength = "Fort"
    
    return {
        "score": score,
        "strength": strength,
        "feedback": feedback
    }


def migrate_bcrypt_password(old_hash: str, new_password: str) -> Optional[str]:
    """
    Migrer un hash bcrypt vers Argon2 lors de la connexion
    """
    try:
        # VÃ©rifier que l'ancien hash est bien bcrypt
        if not old_hash.startswith("$2b$"):
            return None
        
        # CrÃ©er nouveau hash Argon2
        new_hash = hash_password(new_password)
        
        print(f"Migration bcrypt â†’ Argon2 effectuÃ©e")
        return new_hash
        
    except Exception as e:
        print(f"Erreur migration password: {e}")
        return None
```

### src/storage/cloud.py
```python
"""
Client de stockage S3 pour GuignoMap v5.0
Upload/Download de backups et fichiers JSON
"""
import json
import boto3
from typing import Optional, Dict, Any, List
from pathlib import Path
from datetime import datetime
from botocore.exceptions import ClientError, NoCredentialsError
from src.config import get_s3_config, get_cdn_base_url


class S3StorageClient:
    """Client S3 pour stockage cloud des donnÃ©es GuignoMap"""
    
    def __init__(self):
        self.config = get_s3_config()
        self.cdn_base_url = get_cdn_base_url()
        
        try:
            self.client = boto3.client(
                's3',
                region_name=self.config['region'],
                aws_access_key_id=self.config['access_key'],
                aws_secret_access_key=self.config['secret_key']
            )
            self.bucket = self.config['bucket']
        except (NoCredentialsError, KeyError) as e:
            print(f"âŒ Erreur configuration S3: {e}")
            self.client = None
            self.bucket = None
    
    def is_available(self) -> bool:
        """VÃ©rifier si S3 est correctement configurÃ©"""
        if not self.client or not self.bucket:
            return False
        
        try:
            self.client.head_bucket(Bucket=self.bucket)
            return True
        except ClientError:
            return False
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """
        Upload d'un fichier JSON vers S3
        """
        if not self.is_available():
            print("âŒ S3 non disponible pour upload JSON")
            return False
        
        try:
            # SÃ©rialiser en JSON
            json_content = json.dumps(data, ensure_ascii=False, indent=2)
            
            # MÃ©tadonnÃ©es S3
            s3_metadata = {
                'Content-Type': 'application/json',
                'Content-Encoding': 'utf-8'
            }
            
            if metadata:
                # Ajouter les mÃ©tadonnÃ©es custom (prÃ©fixe x-amz-meta-)
                for k, v in metadata.items():
                    s3_metadata[f'x-amz-meta-{k}'] = str(v)
            
            # Upload vers S3
            self.client.put_object(
                Bucket=self.bucket,
                Key=key,
                Body=json_content.encode('utf-8'),
                **s3_metadata
            )
            
            print(f"âœ… JSON uploadÃ© vers S3: s3://{self.bucket}/{key}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur upload JSON S3 {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """
        Download et parsing d'un fichier JSON depuis S3
        """
        if not self.is_available():
            print("âŒ S3 non disponible pour download JSON")
            return None
        
        try:
            response = self.client.get_object(Bucket=self.bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            data = json.loads(content)
            
            print(f"âœ… JSON tÃ©lÃ©chargÃ© depuis S3: s3://{self.bucket}/{key}")
            return data
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                print(f"â„¹ï¸ Fichier JSON S3 non trouvÃ©: {key}")
            else:
                print(f"âŒ Erreur download JSON S3 {key}: {e}")
            return None
        except Exception as e:
            print(f"âŒ Erreur parsing JSON S3 {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """
        Upload d'un fichier backup vers S3
        """
        if not self.is_available():
            print("âŒ S3 non disponible pour upload backup")
            return False
        
        if not backup_file_path.exists():
            print(f"âŒ Fichier backup non trouvÃ©: {backup_file_path}")
            return False
        
        try:
            # GÃ©nÃ©rer la clÃ© S3 si non fournie
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                s3_key = f"backups/{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            
            # MÃ©tadonnÃ©es du backup
            metadata = {
                'x-amz-meta-original-filename': backup_file_path.name,
                'x-amz-meta-upload-timestamp': datetime.utcnow().isoformat(),
                'x-amz-meta-file-size': str(backup_file_path.stat().st_size)
            }
            
            # Upload du fichier
            with open(backup_file_path, 'rb') as f:
                self.client.put_object(
                    Bucket=self.bucket,
                    Key=s3_key,
                    Body=f,
                    **metadata
                )
            
            print(f"âœ… Backup uploadÃ© vers S3: s3://{self.bucket}/{s3_key}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur upload backup S3: {e}")
            return False
    
    def download_backup(self, s3_key: str, local_path: Path) -> bool:
        """
        Download d'un backup depuis S3 vers fichier local
        """
        if not self.is_available():
            print("âŒ S3 non disponible pour download backup")
            return False
        
        try:
            # CrÃ©er le rÃ©pertoire parent si nÃ©cessaire
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Download depuis S3
            self.client.download_file(
                Bucket=self.bucket,
                Key=s3_key,
                Filename=str(local_path)
            )
            
            print(f"âœ… Backup tÃ©lÃ©chargÃ© depuis S3: {local_path}")
            return True
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                print(f"â„¹ï¸ Backup S3 non trouvÃ©: {s3_key}")
            else:
                print(f"âŒ Erreur download backup S3: {e}")
            return False
        except Exception as e:
            print(f"âŒ Erreur download backup: {e}")
            return False
    
    def list_backups(self, prefix: str = "backups/") -> List[Dict[str, Any]]:
        """
        Lister les backups disponibles sur S3
        """
        if not self.is_available():
            print("âŒ S3 non disponible pour list backups")
            return []
        
        try:
            response = self.client.list_objects_v2(
                Bucket=self.bucket,
                Prefix=prefix
            )
            
            backups = []
            for obj in response.get('Contents', []):
                backup_info = {
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified'],
                    'filename': Path(obj['Key']).name
                }
                
                # RÃ©cupÃ©rer les mÃ©tadonnÃ©es si possible
                try:
                    meta_response = self.client.head_object(
                        Bucket=self.bucket,
                        Key=obj['Key']
                    )
                    backup_info['metadata'] = meta_response.get('Metadata', {})
                except Exception:
                    backup_info['metadata'] = {}
                
                backups.append(backup_info)
            
            backups.sort(key=lambda x: x['last_modified'], reverse=True)
            print(f"âœ… {len(backups)} backups trouvÃ©s sur S3")
            return backups
            
        except Exception as e:
            print(f"âŒ Erreur list backups S3: {e}")
            return []
    
    def delete_backup(self, s3_key: str) -> bool:
        """
        Supprimer un backup sur S3
        """
        if not self.is_available():
            print("âŒ S3 non disponible pour delete backup")
            return False
        
        try:
            self.client.delete_object(
                Bucket=self.bucket,
                Key=s3_key
            )
            
            print(f"âœ… Backup supprimÃ© de S3: {s3_key}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur delete backup S3: {e}")
            return False
    
    def get_public_url(self, key: str) -> Optional[str]:
        """
        GÃ©nÃ©rer une URL publique pour un objet S3
        Utilise le CDN si configurÃ©
        """
        if self.cdn_base_url:
            return f"{self.cdn_base_url.rstrip('/')}/{key}"
        else:
            return f"https://{self.bucket}.s3.{self.config['region']}.amazonaws.com/{key}"
```

### src/storage/local.py
```python
"""
Stockage local pour GuignoMap v5.0  
Fallback avec API identique Ã  cloud.py
"""
import json
import shutil
import os
from typing import Optional, Dict, Any, List
from pathlib import Path
from datetime import datetime


class LocalStorageClient:
    """Client stockage local avec API identique au client S3"""
    
    def __init__(self, base_path: Optional[Path] = None):
        # RÃ©pertoire de base pour le stockage local
        if base_path is None:
            base_path = Path(__file__).parent.parent.parent / "storage_local"
        
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # Sous-rÃ©pertoires
        self.backups_dir = self.base_path / "backups"
        self.backups_dir.mkdir(exist_ok=True)
    
    def is_available(self) -> bool:
        """Le stockage local est toujours disponible"""
        return True
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """Sauvegarde d'un fichier JSON en local"""
        try:
            file_path = self.base_path / key
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder les donnÃ©es JSON
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # Sauvegarder les mÃ©tadonnÃ©es si fournies
            if metadata:
                metadata_path = file_path.with_suffix('.metadata.json')
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… JSON sauvÃ© localement: {file_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur sauvegarde JSON local {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """Lecture d'un fichier JSON local"""
        try:
            file_path = self.base_path / key
            
            if not file_path.exists():
                print(f"â„¹ï¸ Fichier JSON local non trouvÃ©: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… JSON lu localement: {file_path}")
            return data
            
        except Exception as e:
            print(f"âŒ Erreur lecture JSON local {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """Copie d'un fichier backup vers le rÃ©pertoire local"""
        try:
            if not backup_file_path.exists():
                print(f"âŒ Fichier backup non trouvÃ©: {backup_file_path}")
                return False
            
            # GÃ©nÃ©rer le nom de destination si non fourni
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                dest_name = f"{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            else:
                # Extraire le nom du fichier de la clÃ© S3
                dest_name = Path(s3_key).name
            
            dest_path = self.backups_dir / dest_name
            
            # Copier le fichier
            shutil.copy2(backup_file_path, dest_path)
            
            # CrÃ©er un fichier de mÃ©tadonnÃ©es
            metadata = {
                'original_filename': backup_file_path.name,
                'original_path': str(backup_file_path),
                'upload_timestamp': datetime.utcnow().isoformat(),
                'file_size': backup_file_path.stat().st_size
            }
            
            metadata_path = dest_path.with_suffix('.metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Backup copiÃ© localement: {dest_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur copie backup local: {e}")
            return False
    
    def download_backup(self, key: str, local_path: Path) -> bool:
        """Copie d'un backup local vers un autre emplacement"""
        try:
            source_path = self.backups_dir / Path(key).name
            
            if not source_path.exists():
                print(f"âŒ Backup local non trouvÃ©: {source_path}")
                return False
            
            # CrÃ©er le rÃ©pertoire parent si nÃ©cessaire
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Copier le fichier
            shutil.copy2(source_path, local_path)
            
            print(f"âœ… Backup copiÃ© vers: {local_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur copie backup: {e}")
            return False
    
    def list_backups(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Lister les backups disponibles en local"""
        try:
            backups = []
            
            for backup_file in self.backups_dir.glob("*.db"):
                if prefix and not backup_file.name.startswith(prefix):
                    continue
                
                stat = backup_file.stat()
                backup_info = {
                    'key': f"backups/{backup_file.name}",
                    'filename': backup_file.name,
                    'size': stat.st_size,
                    'last_modified': datetime.fromtimestamp(stat.st_mtime),
                    'metadata': {}
                }
                
                # Charger les mÃ©tadonnÃ©es si disponibles
                metadata_path = backup_file.with_suffix('.metadata.json')
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            backup_info['metadata'] = json.load(f)
                    except Exception:
                        pass
                
                backups.append(backup_info)
            
            # Trier par date de modification dÃ©croissante
            backups.sort(key=lambda x: x['last_modified'], reverse=True)
            
            print(f"âœ… {len(backups)} backups trouvÃ©s localement")
            return backups
            
        except Exception as e:
            print(f"âŒ Erreur list backups locaux: {e}")
            return []
    
    def delete_backup(self, key: str) -> bool:
        """Supprimer un backup local"""
        try:
            backup_path = self.backups_dir / Path(key).name
            metadata_path = backup_path.with_suffix('.metadata.json')
            
            # Supprimer le fichier backup
            if backup_path.exists():
                backup_path.unlink()
            
            # Supprimer les mÃ©tadonnÃ©es
            if metadata_path.exists():
                metadata_path.unlink()
            
            print(f"âœ… Backup local supprimÃ©: {backup_path.name}")
            return True
            
        except Exception as e:
            print(f"âŒ Erreur suppression backup local: {e}")
            return False
    
    def get_public_url(self, key: str) -> Optional[str]:
        """
        URL publique non applicable pour stockage local
        Retourne le chemin local
        """
        local_path = self.base_path / key
        if local_path.exists():
            return f"file://{local_path.absolute()}"
        return None
```

### src/utils/adapters.py
```python
"""
Adaptateurs pour conversions de donnÃ©es GuignoMap v5.0
Notamment List[Dict] â†’ DataFrame pour UI Streamlit
"""
import pandas as pd
from typing import List, Dict, Any, Optional


def to_dataframe(data: Any, default_columns: Optional[List[str]] = None) -> pd.DataFrame:
    """
    Convertit des donnÃ©es en DataFrame pandas pour l'affichage Streamlit
    
    GÃ¨re les cas:
    - List[Dict] â†’ DataFrame
    - DataFrame â†’ DataFrame (passthrough)
    - DonnÃ©es vides â†’ DataFrame vide avec colonnes par dÃ©faut
    - Autres types â†’ DataFrame vide
    
    Args:
        data: DonnÃ©es Ã  convertir
        default_columns: Colonnes par dÃ©faut si donnÃ©es vides
    
    Returns:
        pd.DataFrame: Toujours un DataFrame valide
    """
    # DÃ©jÃ  un DataFrame
    if isinstance(data, pd.DataFrame):
        return data
    
    # Liste de dictionnaires â†’ DataFrame
    if isinstance(data, list) and len(data) > 0 and all(isinstance(item, dict) for item in data):
        try:
            return pd.DataFrame(data)
        except Exception as e:
            print(f"Erreur conversion List[Dict] â†’ DataFrame: {e}")
            # Fallback: DataFrame vide avec colonnes dÃ©tectÃ©es
            if data and isinstance(data[0], dict):
                columns = list(data[0].keys())
                return pd.DataFrame(columns=columns)
    
    # Liste vide ou autres types â†’ DataFrame vide
    if default_columns:
        return pd.DataFrame(columns=default_columns)
    else:
        return pd.DataFrame()


def safe_get_dict_value(item: Dict[Any, Any], key: str, default: Any = None) -> Any:
    """
    RÃ©cupÃ©ration sÃ©curisÃ©e d'une valeur dans un dictionnaire
    GÃ¨re les cas de clÃ©s manquantes, None, etc.
    """
    if not isinstance(item, dict):
        return default
    
    return item.get(key, default)


def normalize_street_data(streets_raw: List[Dict]) -> pd.DataFrame:
    """
    Normalise les donnÃ©es de rues pour l'affichage UI
    Assure la cohÃ©rence des colonnes et types
    """
    if not streets_raw:
        return pd.DataFrame(columns=['name', 'sector', 'team', 'status'])
    
    # Conversion sÃ©curisÃ©e
    df = to_dataframe(streets_raw, default_columns=['name', 'sector', 'team', 'status'])
    
    # Valeurs par dÃ©faut pour colonnes manquantes
    required_columns = ['name', 'sector', 'team', 'status']
    for col in required_columns:
        if col not in df.columns:
            df[col] = None
    
    # Nettoyage des valeurs
    df['name'] = df['name'].fillna('(Sans nom)')
    df['sector'] = df['sector'].fillna('(Non assignÃ©)')
    df['team'] = df['team'].fillna('(Aucune Ã©quipe)')
    df['status'] = df['status'].fillna('a_faire')
    
    return df


def normalize_team_data(teams_raw: List[Dict]) -> pd.DataFrame:
    """
    Normalise les donnÃ©es d'Ã©quipes pour l'affichage UI
    """
    if not teams_raw:
        return pd.DataFrame(columns=['id', 'name', 'active'])
    
    df = to_dataframe(teams_raw, default_columns=['id', 'name', 'active'])
    
    # Valeurs par dÃ©faut
    if 'id' not in df.columns:
        df['id'] = None
    if 'name' not in df.columns:
        df['name'] = None
    if 'active' not in df.columns:
        df['active'] = True
    
    # Nettoyage
    df['id'] = df['id'].fillna('(ID manquant)')
    df['name'] = df['name'].fillna('(Nom manquant)')
    df['active'] = df['active'].fillna(True)
    
    return df


def normalize_notes_data(notes_raw: List[Dict]) -> pd.DataFrame:
    """
    Normalise les donnÃ©es de notes pour l'affichage UI
    """
    if not notes_raw:
        return pd.DataFrame(columns=['street_name', 'team_id', 'address_number', 'comment', 'created_at'])
    
    df = to_dataframe(notes_raw)
    
    # Valeurs par dÃ©faut
    required_columns = ['street_name', 'team_id', 'address_number', 'comment', 'created_at']
    for col in required_columns:
        if col not in df.columns:
            df[col] = None
    
    # Nettoyage
    df['street_name'] = df['street_name'].fillna('(Rue inconnue)')
    df['team_id'] = df['team_id'].fillna('(Ã‰quipe inconnue)')
    df['address_number'] = df['address_number'].fillna('')
    df['comment'] = df['comment'].fillna('')
    
    return df
```

## CODE â€” FICHIERS MODIFIÃ‰S (extraits pertinents)

### guignomap/app.py (correctifs PHASE 1)
```python
# Import des nouveaux adaptateurs
from src.utils.adapters import to_dataframe, normalize_street_data, normalize_team_data, normalize_notes_data

# [... code existant ...]

def create_map():
    """
    CORRECTIF PHASE 1: DataFrame robuste + coercion + itÃ©ration sÃ©curisÃ©e
    """
    try:
        # RÃ©cupÃ©ration des donnÃ©es avec coercion explicite DataFrame
        streets_raw = db.get_all_streets_with_addresses()
        
        # CORRECTIF 1: Coercion DataFrame en dÃ©but de fonction
        if not isinstance(streets_raw, pd.DataFrame):
            if isinstance(streets_raw, list):
                streets_df = pd.DataFrame(streets_raw) if streets_raw else pd.DataFrame()
            else:
                streets_df = pd.DataFrame()
        else:
            streets_df = streets_raw.copy()
        
        # CORRECTIF 2: Valeurs par dÃ©faut pour Ã©viter les erreurs
        if streets_df.empty:
            st.info("Aucune rue trouvÃ©e dans la base de donnÃ©es")
            # Retourner une carte vide centrÃ©e sur MontrÃ©al
            m = folium.Map(location=[45.5017, -73.5673], zoom_start=12)
            return m
        
        # CORRECTIF 3: Assurer la prÃ©sence des colonnes requises
        required_columns = ['latitude', 'longitude', 'name', 'status']
        for col in required_columns:
            if col not in streets_df.columns:
                streets_df[col] = None
        
        # Filtrer les rues avec coordonnÃ©es valides
        valid_streets = streets_df.dropna(subset=['latitude', 'longitude'])
        
        if valid_streets.empty:
            st.warning("Aucune rue avec coordonnÃ©es GPS trouvÃ©e")
            m = folium.Map(location=[45.5017, -73.5673], zoom_start=12)
            return m
        
        # CrÃ©er la carte
        center_lat = valid_streets['latitude'].mean()
        center_lon = valid_streets['longitude'].mean()
        m = folium.Map(location=[center_lat, center_lon], zoom_start=13)
        
        # CORRECTIF 4: ItÃ©ration robuste avec gestion d'erreurs
        for idx, row in valid_streets.iterrows():
            try:
                lat = float(row.get('latitude', 0))
                lon = float(row.get('longitude', 0))
                name = str(row.get('name', 'Rue inconnue'))
                status = str(row.get('status', 'a_faire'))
                
                # DÃ©finir la couleur selon le statut
                color_map = {
                    'fait': 'green',
                    'en_cours': 'orange', 
                    'a_faire': 'red',
                    'probleme': 'purple'
                }
                color = color_map.get(status, 'gray')
                
                # Ajouter le marqueur
                folium.CircleMarker(
                    location=[lat, lon],
                    radius=8,
                    popup=f"{name}<br>Statut: {status}",
                    color=color,
                    fill=True,
                    fillColor=color,
                    fillOpacity=0.7
                ).add_to(m)
                
            except (ValueError, TypeError, AttributeError) as e:
                print(f"Erreur traitement rue {row.get('name', 'inconnue')}: {e}")
                continue  # Passer Ã  la rue suivante
        
        return m
        
    except Exception as e:
        st.error(f"Erreur lors de la crÃ©ation de la carte: {e}")
        # Retourner carte de fallback
        return folium.Map(location=[45.5017, -73.5673], zoom_start=12)

# [... code existant ...]

# CORRECTIFS UI: Wrapping des st.dataframe() avec to_dataframe()

# Correctif 1: Affichage des rues
if st.button("Voir toutes les rues"):
    streets = db.get_all_streets()
    streets_df = to_dataframe(streets, default_columns=['name', 'sector', 'team', 'status'])
    st.dataframe(streets_df)

# Correctif 2: Statistiques par Ã©quipe 
stats = db.stats_by_team()
stats_df = to_dataframe(stats, default_columns=['team', 'total', 'completed'])
st.dataframe(stats_df)

# Correctif 3: Notes d'Ã©quipe
team_notes = db.get_team_notes(selected_team)
notes_df = to_dataframe(team_notes, default_columns=['street_name', 'address_number', 'comment', 'created_at'])
st.dataframe(notes_df)

# Correctif 4: ActivitÃ© rÃ©cente  
recent_activity = db.recent_activity(limit=50)
activity_df = to_dataframe(recent_activity, default_columns=['team_id', 'action', 'details', 'created_at'])
st.dataframe(activity_df)

# Correctif 5: Liste des Ã©quipes (admin)
teams = db.get_all_teams()
teams_df = to_dataframe(teams, default_columns=['id', 'name', 'active'])
st.dataframe(teams_df)

# Correctif 6: Liste des rues (admin)
streets = db.list_streets()
streets_df = to_dataframe(streets, default_columns=['name', 'sector', 'team', 'status'])
st.dataframe(streets_df)
```

### scripts/migrate_sqlite_to_postgres.py (extrait - fonction main)
```python
def main():
    """Migration complÃ¨te SQLite â†’ PostgreSQL"""
    print("ğŸ”„ DÃ©but migration SQLite â†’ PostgreSQL...")
    
    # Connexions
    sqlite_conn = get_sqlite_connection()
    if not sqlite_conn:
        return False
    
    try:
        # CrÃ©er les tables PostgreSQL
        if not create_postgres_tables():
            return False
        
        # Session PostgreSQL
        engine = get_engine()
        Session = sessionmaker(bind=engine)
        postgres_session = Session()
        
        # Migration par table
        total_migrated = 0
        total_migrated += copy_teams(sqlite_conn, postgres_session)
        total_migrated += copy_streets(sqlite_conn, postgres_session)
        total_migrated += copy_notes(sqlite_conn, postgres_session)
        total_migrated += copy_activity_logs(sqlite_conn, postgres_session)
        total_migrated += copy_addresses(sqlite_conn, postgres_session)
        
        postgres_session.close()
        sqlite_conn.close()
        
        print(f"ğŸ‰ Migration terminÃ©e ! {total_migrated} enregistrements migrÃ©s")
        return True
        
    except Exception as e:
        print(f"âŒ Erreur gÃ©nÃ©rale migration: {e}")
        if 'postgres_session' in locals():
            postgres_session.close()
        sqlite_conn.close()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
```

### scripts/migrate_password_hashes.py (extrait - fonction analyse)
```python
def analyze_password_hashes():
    """
    Analyse des hashes de mots de passe dans la base
    Identifie les Ã©quipes avec des hashes bcrypt qui nÃ©cessitent une migration
    """
    conn = get_sqlite_connection()
    if not conn:
        return
    
    try:
        print("ğŸ” Analyse des hashes de mots de passe...")
        print("=" * 50)
        
        # RÃ©cupÃ©rer toutes les Ã©quipes
        cursor = conn.execute("SELECT id, name, password_hash, created_at FROM teams ORDER BY id")
        teams = cursor.fetchall()
        
        if not teams:
            print("â„¹ï¸ Aucune Ã©quipe trouvÃ©e dans la base")
            return
        
        bcrypt_count = 0
        argon2_count = 0
        unknown_count = 0
        
        print(f"{'Ã‰quipe':<15} {'Algorithme':<10} {'Statut':<20} {'CrÃ©Ã© le'}")
        print("-" * 65)
        
        for team in teams:
            # [... logique d'analyse des hashes ...]
            hash_info = get_password_hash_info(hash_value)
            algorithm = hash_info['algorithm']
            
            if algorithm == 'bcrypt':
                bcrypt_count += 1
                status = "ğŸ”„ Ã€ migrer"
            elif algorithm == 'argon2':
                argon2_count += 1
                status = "âœ… Moderne"
            else:
                unknown_count += 1
                status = "â“ Inconnu"
        
        print(f"\nğŸ“Š RÃ©sumÃ©: {bcrypt_count} bcrypt, {argon2_count} Argon2, {unknown_count} inconnus")
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'analyse: {e}")
    finally:
        conn.close()
```

## FICHIERS DE CONFIGURATION

### alembic.ini
```ini
# Configuration Alembic pour migrations PostgreSQL

[alembic]
script_location = src/database/migrations
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = postgresql://user:pass@localhost/dbname

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

### requirements.txt (rÃ©fÃ©rence)
```txt
streamlit>=1.36.0
pandas>=2.2.0
requests>=2.31.0
folium==0.20.0
streamlit-folium>=0.21.0
sqlalchemy==2.0.23
alembic==1.13.1
psycopg2-binary>=2.9.7
passlib[argon2]==1.7.4
boto3==1.34.144
bcrypt>=4.0.0
```

## COMMITS PRÃ‰PARÃ‰S (PHASE 1)

### Commit 1: Infrastructure base SQLAlchemy + PostgreSQL
```
feat: infrastructure SQLAlchemy + PostgreSQL v5.0

- Connexion PostgreSQL avec QueuePool + retry logic
- ModÃ¨les SQLAlchemy (Street, Team, Note, ActivityLog, Address)
- Configuration centralisÃ©e avec secrets Streamlit
- Alembic pour migrations de schÃ©ma
- Support fallback SQLite pour dev local

Files:
+ src/config.py
+ src/database/connection.py  
+ src/database/models.py
+ src/database/migrations/env.py
+ alembic.ini
```

### Commit 2: Stockage S3/local + authentification Argon2
```
feat: stockage S3/local + auth Argon2 avec compat bcrypt

- Client S3 pour backup cloud avec fallback local
- Migration paresseuse bcrypt â†’ Argon2 Ã  la connexion
- API stockage unifiÃ©e (cloud.py / local.py)
- Scripts de migration SQLite â†’ PostgreSQL
- Scripts d'analyse des hashes de mot de passe

Files:
+ src/storage/cloud.py
+ src/storage/local.py
+ src/auth/passwords.py
+ scripts/migrate_sqlite_to_postgres.py
+ scripts/migrate_password_hashes.py
```

### Commit 3: Correctifs DataFrame + adapters UI (CRITIQUE)
```
fix: correctifs DataFrame + adapters UI robustes

CORRECTIFS PHASE 1 AUDIT:
- create_map(): coercion DataFrame + itÃ©ration sÃ©curisÃ©e + valeurs par dÃ©faut
- Wrapping 6Ã— st.dataframe() avec to_dataframe() adapter
- Adapter List[Dict] â†’ DataFrame pour cohÃ©rence UI
- Gestion erreurs robuste (rue manquante, coordonnÃ©es invalides)

Files:
~ guignomap/app.py (create_map + 6 st.dataframe fixes)
+ src/utils/adapters.py
```

## VALIDATION & TESTS RECOMMANDÃ‰S

### Scripts PowerShell de validation (dÃ©jÃ  fournis)
```powershell
# Validation DataFrame (Ã  exÃ©cuter aprÃ¨s correctifs)
python -c "
import pandas as pd
from src.utils.adapters import to_dataframe

# Test 1: List[Dict] â†’ DataFrame
data = [{'name': 'Rue A', 'status': 'fait'}, {'name': 'Rue B', 'status': 'a_faire'}]
df = to_dataframe(data)
assert isinstance(df, pd.DataFrame)
assert len(df) == 2
print('âœ… Test 1 passed: List[Dict] â†’ DataFrame')

# Test 2: DataFrame â†’ DataFrame (passthrough)
df2 = to_dataframe(df)
assert df.equals(df2)
print('âœ… Test 2 passed: DataFrame passthrough')

# Test 3: DonnÃ©es vides â†’ DataFrame vide
empty_df = to_dataframe([], default_columns=['name', 'status'])
assert isinstance(empty_df, pd.DataFrame)
assert list(empty_df.columns) == ['name', 'status']
print('âœ… Test 3 passed: Empty â†’ DataFrame with defaults')

print('ğŸ‰ Tous les tests adapters passed')
"
```

### Tests de connexion base PostgreSQL
```powershell
python -c "
from src.database.connection import test_connection, get_engine
from src.config import get_database_url

print(f'Database URL: {get_database_url()[:20]}...')  # Masquer credentials
engine = get_engine()
print(f'Engine pool size: {engine.pool.size()}')

if test_connection():
    print('âœ… Connexion PostgreSQL OK')
else:
    print('âŒ Connexion PostgreSQL Ã©chouÃ©e')
"
```

## PROCÃ‰DURES DE DÃ‰PLOIEMENT

### 1. Environnement local (SQLite)
```powershell
# Setup venv
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt

# Test local
streamlit run guignomap/app.py
```

### 2. DÃ©ploiement Streamlit Cloud (PostgreSQL)
```yaml
# Secrets Streamlit Cloud (.streamlit/secrets.toml)
[database]
url = "postgresql://user:***@***:5432/guignomap"
pool_size = 5
max_overflow = 10

[storage]
s3_bucket = "guignomap-prod"
s3_region = "us-east-1"
s3_access_key = "AKIA***"
s3_secret_key = "***"
cdn_base_url = "https://cdn.example.com"  # optionnel
```

### 3. Migration des donnÃ©es (une fois en prod)
```powershell
# ExÃ©cuter depuis l'environnement local avec accÃ¨s PostgreSQL prod
python scripts/migrate_sqlite_to_postgres.py
python scripts/migrate_password_hashes.py  # Analyse seulement
```

## FICHIERS MANQUANTS/AMBIGUS DÃ‰TECTÃ‰S

### Fichiers non trouvÃ©s lors de l'export:
- `src/ui/pages/admin_migration.py` : rÃ©fÃ©rencÃ© mais absent du repo
- RÃ©visions Alembic dans `src/database/migrations/versions/` : dossier vide

### Questions pour l'utilisateur:
1. Le fichier `admin_migration.py` doit-il Ãªtre crÃ©Ã© ou Ã©tait-ce une rÃ©fÃ©rence incorrecte ?
2. Des rÃ©visions Alembic doivent-elles Ãªtre gÃ©nÃ©rÃ©es (`alembic revision --autogenerate`) ?
3. Y a-t-il d'autres fichiers/configurations spÃ©cifiques au dÃ©ploiement Streamlit Cloud ?

## NOTES IMPORTANTES

- **IPv6 Streamlit Cloud**: Les migrations PostgreSQL doivent Ãªtre exÃ©cutÃ©es cÃ´tÃ© Cloud car l'environnement local peut avoir des problÃ¨mes IPv6
- **Politique MDP v4.1 conservÃ©e**: Minimum 4 caractÃ¨res + confirmation pour maintenir l'UX existante
- **Migration paresseuse**: Les hashes bcrypt sont automatiquement migrÃ©s vers Argon2 lors de la prochaine connexion rÃ©ussie
- **Fallbacks robustes**: Tous les composants ont des fallbacks SQLite/local en cas de problÃ¨me PostgreSQL/S3
- **Logs dÃ©taillÃ©s**: Tous les composants loggent leurs actions pour faciliter le debug

---
**FIN DE L'EXPORT - Version v5.0 PHASE 1 complÃ¨te**
**Date: 2025-09-15 14:30:00 local**
**Taille: ~15KB de code + config + documentation**
```
```