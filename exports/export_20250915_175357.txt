# GuignoMap - Export de code complet
# Date : 2025-09-15 14:30:00 local
# Version : v5.0 (Phase 1 bouclée) — base v4.1 UI/UX conservée
# Auteur : Nick & Copilot
# Projet : Système de gestion pour la Guignolée 2025
# Encodage : UTF-8 (sans BOM)

## RÉSUMÉ DES ÉVOLUTIONS RÉCENTES
- Infra PHASE 1: SQLite→PostgreSQL (SQLAlchemy + Alembic), Storage S3/local, Auth passlib Argon2 (compat bcrypt), correctifs DataFrame (create_map + UI), adapters df.
- IPv6 local: dev SQLite OK; prod Streamlit Cloud (IPv6) — migrations exécutées côté Cloud.
- Politique UI MDP v4.1 inchangée: min 4 + confirmation.

## ENVIRONNEMENT & DÉPENDANCES
- Python: 3.11+ (détecté)
- Principales libs (versions exactes depuis requirements): 
  * streamlit>=1.36.0
  * pandas>=2.2.0
  * folium==0.20.0
  * streamlit-folium>=0.21.0
  * sqlalchemy==2.0.23
  * alembic==1.13.1
  * psycopg2-binary>=2.9.7
  * passlib[argon2]==1.7.4
  * boto3==1.34.144
  * bcrypt>=4.0.0

- Variables/Secrets attendus (placeholders, ne JAMAIS imprimer les vraies valeurs):
  [database] url="postgresql://<user>:<pass>@<host>:<port>/<db>" pool_size=<int> max_overflow=<int>
  [storage] s3_bucket="<bucket>" s3_region="<region>" s3_access_key="<AKIA…>" s3_secret_key="<***>" cdn_base_url="<https://…>" (optionnel)
  [admin] token="<ADMIN_TOKEN>" (si page admin migration)

## ARBORESCENCE (repo root)
GuignoMap/
├── .git/
├── .gitignore
├── .streamlit/
├── .venv/
├── .vscode/
├── alembic.ini
├── AUDIT_DATAFRAME.md
├── audit_dataframe.ps1
├── backups/
├── exports/
├── fix_app_types.py
├── fix_specific.py
├── guignomap/
│   ├── __init__.py
│   ├── app.py
│   ├── backup.py
│   ├── db_v5.py
│   ├── db.py
│   ├── guigno_map.db
│   ├── osm_addresses.json
│   ├── osm_cache.json
│   ├── osm.py
│   ├── reports.py
│   ├── validators.py
│   ├── assets/
│   ├── backups/
│   └── logs/
├── GuignoMap_code_export_20250914_audit.txt
├── GuignoMap_code_export_20250915_final_UTF8.txt
├── lancer_guignomap.bat
├── lancer_guignomap.ps1
├── PHASE1_COMMANDS.md
├── PROBLEME_IPv6_SUPABASE.md
├── README.md
├── README_VENV.md
├── requirements.txt
├── scripts/
│   ├── migrate_password_hashes.py
│   └── migrate_sqlite_to_postgres.py
├── scripts_validation_dataframe.ps1
├── src/
│   ├── config.py
│   ├── auth/
│   │   └── passwords.py
│   ├── database/
│   │   ├── connection.py
│   │   ├── models.py
│   │   └── migrations/
│   │       ├── env.py
│   │       ├── README
│   │       ├── script.py.mako
│   │       └── versions/ (empty)
│   ├── storage/
│   │   ├── __init__.py
│   │   ├── cloud.py
│   │   └── local.py
│   └── utils/
│       ├── __init__.py
│       └── adapters.py
├── test_db_connection.py
├── test_db_simple.py
└── tools/
    └── quick_sanity.py

## FICHIERS NOUVEAUX OU MODIFIÉS (PHASE 1)
- src/config.py (NEW)
- src/database/connection.py (NEW)
- src/database/models.py (NEW)
- src/database/migrations/env.py (NEW)
- src/database/migrations/versions/ (NEW - vide, aucune révision créée)
- src/auth/passwords.py (NEW)
- src/storage/cloud.py (NEW)
- src/storage/local.py (NEW)
- src/utils/adapters.py (NEW, DataFrame adapter)
- src/utils/__init__.py (NEW)
- scripts/migrate_sqlite_to_postgres.py (NEW)
- scripts/migrate_password_hashes.py (NEW)
- guignomap/app.py (MOD - correctifs DataFrame PHASE 1)
- guignomap/db_v5.py (MOD - adaptations SQLAlchemy)
- alembic.ini (NEW)

## CODE — NOUVEAUX FICHIERS (contenu COMPLET)

### src/config.py
```python
"""
Configuration centralisée pour GuignoMap v5.0
Accès aux secrets Streamlit et paramètres applicatifs
"""
import streamlit as st
import os


def get_database_url():
    """Récupère l'URL de la base de données depuis les secrets"""
    try:
        return st.secrets["database"]["url"]
    except (KeyError, AttributeError):
        # Fallback pour développement local ou tests
        return os.getenv("DATABASE_URL", "sqlite:///guigno_map.db")


def get_database_pool_config():
    """Configuration du pool de connexions PostgreSQL"""
    try:
        return {
            "pool_size": st.secrets["database"].get("pool_size", 5),
            "max_overflow": st.secrets["database"].get("max_overflow", 10)
        }
    except (KeyError, AttributeError):
        return {"pool_size": 5, "max_overflow": 10}


def get_s3_config():
    """Configuration S3 pour le stockage cloud"""
    try:
        return {
            "bucket": st.secrets["storage"]["s3_bucket"],
            "region": st.secrets["storage"]["s3_region"],
            "access_key": st.secrets["storage"]["s3_access_key"],
            "secret_key": st.secrets["storage"]["s3_secret_key"]
        }
    except (KeyError, AttributeError):
        return {
            "bucket": os.getenv("S3_BUCKET", "guignomap-dev"),
            "region": os.getenv("S3_REGION", "us-east-1"),
            "access_key": os.getenv("S3_ACCESS_KEY", ""),
            "secret_key": os.getenv("S3_SECRET_KEY", "")
        }


def get_cdn_base_url():
    """URL de base CDN pour les assets (optionnel)"""
    try:
        return st.secrets["storage"].get("cdn_base_url", "")
    except (KeyError, AttributeError):
        return os.getenv("CDN_BASE_URL", "")
```

### src/database/connection.py
```python
"""
Connexion PostgreSQL avec SQLAlchemy pour GuignoMap v5.0
Engine + QueuePool + cache Streamlit + retry logic
"""
import time
import functools
import streamlit as st
from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError
from src.config import get_database_url, get_database_pool_config


def db_retry(max_retries=3, backoff_factor=1):
    """
    Décorateur retry exponentiel pour opérations DB critiques
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except SQLAlchemyError as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        wait_time = backoff_factor * (2 ** attempt)
                        print(f"Retry DB operation {func.__name__} in {wait_time}s (attempt {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"Max retries reached for {func.__name__}")
                        break
            raise last_exception
        return wrapper
    return decorator


@st.cache_resource
def get_engine():
    """
    Engine PostgreSQL avec cache Streamlit et configuration pool
    Conformément au plan v5.0
    """
    database_url = get_database_url()
    pool_config = get_database_pool_config()
    
    # Configuration PostgreSQL avec QueuePool
    engine = create_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=pool_config["pool_size"],
        max_overflow=pool_config["max_overflow"],
        pool_pre_ping=True,
        pool_recycle=300,
        echo=False  # Set to True for SQL debugging
    )
    
    return engine


def get_session():
    """Fabrique de session SQLAlchemy"""
    engine = get_engine()
    Session = sessionmaker(bind=engine)
    return Session()


@db_retry(max_retries=3)
def test_connection():
    """Test de connexion à la base PostgreSQL"""
    try:
        engine = get_engine()
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1 as test"))
            return result.fetchone()[0] == 1
    except Exception as e:
        print(f"Erreur test connexion DB: {e}")
        return False


@db_retry(max_retries=3)
def execute_query(query, params=None):
    """
    Exécution de requête avec retry automatique
    Pour transition progressive vers SQLAlchemy
    """
    engine = get_engine()
    with engine.connect() as conn:
        if params:
            return conn.execute(text(query), params)
        else:
            return conn.execute(text(query))


@db_retry(max_retries=3)  
def execute_transaction(queries_and_params):
    """
    Exécution de transaction multi-requêtes avec retry
    queries_and_params: liste de tuples (query, params)
    """
    engine = get_engine()
    with engine.begin() as conn:
        results = []
        for query, params in queries_and_params:
            if params:
                result = conn.execute(text(query), params)
            else:
                result = conn.execute(text(query))
            results.append(result)
        return results
```

### src/database/models.py
```python
"""
Modèles SQLAlchemy pour GuignoMap v5.0
Basés sur le schéma SQLite existant pour compatibilité
"""
from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, ForeignKey, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()


class Street(Base):
    __tablename__ = 'streets'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(Text, nullable=False, unique=True)
    sector = Column(Text)
    team = Column(Text)
    status = Column(Text, nullable=False, default='a_faire')
    
    # Relations
    notes = relationship("Note", back_populates="street", cascade="all, delete-orphan")
    addresses = relationship("Address", back_populates="street", cascade="all, delete-orphan")


class Team(Base):
    __tablename__ = 'teams'
    
    id = Column(Text, primary_key=True)
    name = Column(Text, nullable=False)
    password_hash = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    active = Column(Boolean, default=True)


class Note(Base):
    __tablename__ = 'notes'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    team_id = Column(Text, ForeignKey('teams.id'), nullable=False)
    address_number = Column(Text)
    comment = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="notes")


class ActivityLog(Base):
    __tablename__ = 'activity_log'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    team_id = Column(Text)
    action = Column(Text, nullable=False)
    details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)


class Address(Base):
    __tablename__ = 'addresses'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    house_number = Column(Text, nullable=False)
    latitude = Column(Float)
    longitude = Column(Float)
    osm_type = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="addresses")
```

### src/database/migrations/env.py
```python
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# Ajouter le répertoire parent au PYTHONPATH pour imports
import sys
import os
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent.parent))

from src.database.models import Base
from src.config import get_database_url

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_url():
    """Récupère l'URL de BDD depuis la configuration Streamlit"""
    return get_database_url()


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode."""
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode."""
    # Override URL from config
    configuration = config.get_section(config.config_ini_section)
    configuration['sqlalchemy.url'] = get_url()
    
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
```

### src/auth/passwords.py
```python
"""
Gestion des mots de passe pour GuignoMap v5.0
Migration bcrypt → Argon2 avec compatibilité ascendante
"""
import re
from typing import Optional, Dict, Tuple
from passlib.context import CryptContext


# Configuration passlib avec Argon2 (principal) + bcrypt (legacy)
pwd_context = CryptContext(
    schemes=["argon2", "bcrypt"],
    deprecated="auto",  # Auto-migrate bcrypt vers argon2
    argon2__memory_cost=65536,  # 64 MB
    argon2__time_cost=3,        # 3 iterations
    argon2__parallelism=1,      # Single thread pour Streamlit Cloud
)


def hash_password(password: str) -> str:
    """
    Hasher un mot de passe avec Argon2
    """
    return pwd_context.hash(password)


def verify_password(password: str, hashed: str) -> bool:
    """
    Vérifier un mot de passe contre son hash
    Compatible bcrypt + Argon2
    """
    try:
        result = pwd_context.verify(password, hashed)
        return result
    except Exception as e:
        print(f"Erreur vérification mot de passe: {e}")
        return False


def needs_rehash(hashed: str) -> bool:
    """
    Vérifier si un hash doit être re-hashé (migration bcrypt → Argon2)
    """
    try:
        return pwd_context.needs_update(hashed)
    except Exception:
        return True  # En cas de doute, re-hasher


def validate_password_policy(password: str) -> Tuple[bool, Optional[str]]:
    """
    Valider la politique de mot de passe v4.1 (min 4 + confirmation)
    Conserve la compatibilité UI existante
    """
    if not password:
        return False, "Le mot de passe ne peut pas être vide"
    
    if len(password) < 4:
        return False, "Le mot de passe doit contenir au moins 4 caractères"
    
    # Pas d'autres restrictions pour garder l'UX v4.1
    return True, None


def get_password_strength(password: str) -> Dict[str, any]:
    """
    Évaluer la force d'un mot de passe (pour affichage optionnel)
    """
    if not password:
        return {"score": 0, "feedback": "Vide"}
    
    score = 0
    feedback = []
    
    # Critères de force
    if len(password) >= 8:
        score += 2
    elif len(password) >= 6:
        score += 1
    else:
        feedback.append("Trop court")
    
    if re.search(r'[a-z]', password):
        score += 1
    
    if re.search(r'[A-Z]', password):
        score += 1
    
    if re.search(r'\d', password):
        score += 1
    
    if re.search(r'[^a-zA-Z0-9]', password):
        score += 1
    
    # Classification
    if score <= 2:
        strength = "Faible"
    elif score <= 4:
        strength = "Moyen"
    else:
        strength = "Fort"
    
    return {
        "score": score,
        "strength": strength,
        "feedback": feedback
    }


def migrate_bcrypt_password(old_hash: str, new_password: str) -> Optional[str]:
    """
    Migrer un hash bcrypt vers Argon2 lors de la connexion
    """
    try:
        # Vérifier que l'ancien hash est bien bcrypt
        if not old_hash.startswith("$2b$"):
            return None
        
        # Créer nouveau hash Argon2
        new_hash = hash_password(new_password)
        
        print(f"Migration bcrypt → Argon2 effectuée")
        return new_hash
        
    except Exception as e:
        print(f"Erreur migration password: {e}")
        return None
```

### src/storage/cloud.py
```python
"""
Client de stockage S3 pour GuignoMap v5.0
Upload/Download de backups et fichiers JSON
"""
import json
import boto3
from typing import Optional, Dict, Any, List
from pathlib import Path
from datetime import datetime
from botocore.exceptions import ClientError, NoCredentialsError
from src.config import get_s3_config, get_cdn_base_url


class S3StorageClient:
    """Client S3 pour stockage cloud des données GuignoMap"""
    
    def __init__(self):
        self.config = get_s3_config()
        self.cdn_base_url = get_cdn_base_url()
        
        try:
            self.client = boto3.client(
                's3',
                region_name=self.config['region'],
                aws_access_key_id=self.config['access_key'],
                aws_secret_access_key=self.config['secret_key']
            )
            self.bucket = self.config['bucket']
        except (NoCredentialsError, KeyError) as e:
            print(f"❌ Erreur configuration S3: {e}")
            self.client = None
            self.bucket = None
    
    def is_available(self) -> bool:
        """Vérifier si S3 est correctement configuré"""
        if not self.client or not self.bucket:
            return False
        
        try:
            self.client.head_bucket(Bucket=self.bucket)
            return True
        except ClientError:
            return False
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """
        Upload d'un fichier JSON vers S3
        """
        if not self.is_available():
            print("❌ S3 non disponible pour upload JSON")
            return False
        
        try:
            # Sérialiser en JSON
            json_content = json.dumps(data, ensure_ascii=False, indent=2)
            
            # Métadonnées S3
            s3_metadata = {
                'Content-Type': 'application/json',
                'Content-Encoding': 'utf-8'
            }
            
            if metadata:
                # Ajouter les métadonnées custom (préfixe x-amz-meta-)
                for k, v in metadata.items():
                    s3_metadata[f'x-amz-meta-{k}'] = str(v)
            
            # Upload vers S3
            self.client.put_object(
                Bucket=self.bucket,
                Key=key,
                Body=json_content.encode('utf-8'),
                **s3_metadata
            )
            
            print(f"✅ JSON uploadé vers S3: s3://{self.bucket}/{key}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur upload JSON S3 {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """
        Download et parsing d'un fichier JSON depuis S3
        """
        if not self.is_available():
            print("❌ S3 non disponible pour download JSON")
            return None
        
        try:
            response = self.client.get_object(Bucket=self.bucket, Key=key)
            content = response['Body'].read().decode('utf-8')
            data = json.loads(content)
            
            print(f"✅ JSON téléchargé depuis S3: s3://{self.bucket}/{key}")
            return data
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                print(f"ℹ️ Fichier JSON S3 non trouvé: {key}")
            else:
                print(f"❌ Erreur download JSON S3 {key}: {e}")
            return None
        except Exception as e:
            print(f"❌ Erreur parsing JSON S3 {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """
        Upload d'un fichier backup vers S3
        """
        if not self.is_available():
            print("❌ S3 non disponible pour upload backup")
            return False
        
        if not backup_file_path.exists():
            print(f"❌ Fichier backup non trouvé: {backup_file_path}")
            return False
        
        try:
            # Générer la clé S3 si non fournie
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                s3_key = f"backups/{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            
            # Métadonnées du backup
            metadata = {
                'x-amz-meta-original-filename': backup_file_path.name,
                'x-amz-meta-upload-timestamp': datetime.utcnow().isoformat(),
                'x-amz-meta-file-size': str(backup_file_path.stat().st_size)
            }
            
            # Upload du fichier
            with open(backup_file_path, 'rb') as f:
                self.client.put_object(
                    Bucket=self.bucket,
                    Key=s3_key,
                    Body=f,
                    **metadata
                )
            
            print(f"✅ Backup uploadé vers S3: s3://{self.bucket}/{s3_key}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur upload backup S3: {e}")
            return False
    
    def download_backup(self, s3_key: str, local_path: Path) -> bool:
        """
        Download d'un backup depuis S3 vers fichier local
        """
        if not self.is_available():
            print("❌ S3 non disponible pour download backup")
            return False
        
        try:
            # Créer le répertoire parent si nécessaire
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Download depuis S3
            self.client.download_file(
                Bucket=self.bucket,
                Key=s3_key,
                Filename=str(local_path)
            )
            
            print(f"✅ Backup téléchargé depuis S3: {local_path}")
            return True
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                print(f"ℹ️ Backup S3 non trouvé: {s3_key}")
            else:
                print(f"❌ Erreur download backup S3: {e}")
            return False
        except Exception as e:
            print(f"❌ Erreur download backup: {e}")
            return False
    
    def list_backups(self, prefix: str = "backups/") -> List[Dict[str, Any]]:
        """
        Lister les backups disponibles sur S3
        """
        if not self.is_available():
            print("❌ S3 non disponible pour list backups")
            return []
        
        try:
            response = self.client.list_objects_v2(
                Bucket=self.bucket,
                Prefix=prefix
            )
            
            backups = []
            for obj in response.get('Contents', []):
                backup_info = {
                    'key': obj['Key'],
                    'size': obj['Size'],
                    'last_modified': obj['LastModified'],
                    'filename': Path(obj['Key']).name
                }
                
                # Récupérer les métadonnées si possible
                try:
                    meta_response = self.client.head_object(
                        Bucket=self.bucket,
                        Key=obj['Key']
                    )
                    backup_info['metadata'] = meta_response.get('Metadata', {})
                except Exception:
                    backup_info['metadata'] = {}
                
                backups.append(backup_info)
            
            backups.sort(key=lambda x: x['last_modified'], reverse=True)
            print(f"✅ {len(backups)} backups trouvés sur S3")
            return backups
            
        except Exception as e:
            print(f"❌ Erreur list backups S3: {e}")
            return []
    
    def delete_backup(self, s3_key: str) -> bool:
        """
        Supprimer un backup sur S3
        """
        if not self.is_available():
            print("❌ S3 non disponible pour delete backup")
            return False
        
        try:
            self.client.delete_object(
                Bucket=self.bucket,
                Key=s3_key
            )
            
            print(f"✅ Backup supprimé de S3: {s3_key}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur delete backup S3: {e}")
            return False
    
    def get_public_url(self, key: str) -> Optional[str]:
        """
        Générer une URL publique pour un objet S3
        Utilise le CDN si configuré
        """
        if self.cdn_base_url:
            return f"{self.cdn_base_url.rstrip('/')}/{key}"
        else:
            return f"https://{self.bucket}.s3.{self.config['region']}.amazonaws.com/{key}"
```

### src/storage/local.py
```python
"""
Stockage local pour GuignoMap v5.0  
Fallback avec API identique à cloud.py
"""
import json
import shutil
import os
from typing import Optional, Dict, Any, List
from pathlib import Path
from datetime import datetime


class LocalStorageClient:
    """Client stockage local avec API identique au client S3"""
    
    def __init__(self, base_path: Optional[Path] = None):
        # Répertoire de base pour le stockage local
        if base_path is None:
            base_path = Path(__file__).parent.parent.parent / "storage_local"
        
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # Sous-répertoires
        self.backups_dir = self.base_path / "backups"
        self.backups_dir.mkdir(exist_ok=True)
    
    def is_available(self) -> bool:
        """Le stockage local est toujours disponible"""
        return True
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """Sauvegarde d'un fichier JSON en local"""
        try:
            file_path = self.base_path / key
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder les données JSON
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # Sauvegarder les métadonnées si fournies
            if metadata:
                metadata_path = file_path.with_suffix('.metadata.json')
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"✅ JSON sauvé localement: {file_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur sauvegarde JSON local {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """Lecture d'un fichier JSON local"""
        try:
            file_path = self.base_path / key
            
            if not file_path.exists():
                print(f"ℹ️ Fichier JSON local non trouvé: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"✅ JSON lu localement: {file_path}")
            return data
            
        except Exception as e:
            print(f"❌ Erreur lecture JSON local {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """Copie d'un fichier backup vers le répertoire local"""
        try:
            if not backup_file_path.exists():
                print(f"❌ Fichier backup non trouvé: {backup_file_path}")
                return False
            
            # Générer le nom de destination si non fourni
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                dest_name = f"{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            else:
                # Extraire le nom du fichier de la clé S3
                dest_name = Path(s3_key).name
            
            dest_path = self.backups_dir / dest_name
            
            # Copier le fichier
            shutil.copy2(backup_file_path, dest_path)
            
            # Créer un fichier de métadonnées
            metadata = {
                'original_filename': backup_file_path.name,
                'original_path': str(backup_file_path),
                'upload_timestamp': datetime.utcnow().isoformat(),
                'file_size': backup_file_path.stat().st_size
            }
            
            metadata_path = dest_path.with_suffix('.metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"✅ Backup copié localement: {dest_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur copie backup local: {e}")
            return False
    
    def download_backup(self, key: str, local_path: Path) -> bool:
        """Copie d'un backup local vers un autre emplacement"""
        try:
            source_path = self.backups_dir / Path(key).name
            
            if not source_path.exists():
                print(f"❌ Backup local non trouvé: {source_path}")
                return False
            
            # Créer le répertoire parent si nécessaire
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Copier le fichier
            shutil.copy2(source_path, local_path)
            
            print(f"✅ Backup copié vers: {local_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur copie backup: {e}")
            return False
    
    def list_backups(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Lister les backups disponibles en local"""
        try:
            backups = []
            
            for backup_file in self.backups_dir.glob("*.db"):
                if prefix and not backup_file.name.startswith(prefix):
                    continue
                
                stat = backup_file.stat()
                backup_info = {
                    'key': f"backups/{backup_file.name}",
                    'filename': backup_file.name,
                    'size': stat.st_size,
                    'last_modified': datetime.fromtimestamp(stat.st_mtime),
                    'metadata': {}
                }
                
                # Charger les métadonnées si disponibles
                metadata_path = backup_file.with_suffix('.metadata.json')
                if metadata_path.exists():
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            backup_info['metadata'] = json.load(f)
                    except Exception:
                        pass
                
                backups.append(backup_info)
            
            # Trier par date de modification décroissante
            backups.sort(key=lambda x: x['last_modified'], reverse=True)
            
            print(f"✅ {len(backups)} backups trouvés localement")
            return backups
            
        except Exception as e:
            print(f"❌ Erreur list backups locaux: {e}")
            return []
    
    def delete_backup(self, key: str) -> bool:
        """Supprimer un backup local"""
        try:
            backup_path = self.backups_dir / Path(key).name
            metadata_path = backup_path.with_suffix('.metadata.json')
            
            # Supprimer le fichier backup
            if backup_path.exists():
                backup_path.unlink()
            
            # Supprimer les métadonnées
            if metadata_path.exists():
                metadata_path.unlink()
            
            print(f"✅ Backup local supprimé: {backup_path.name}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur suppression backup local: {e}")
            return False
    
    def get_public_url(self, key: str) -> Optional[str]:
        """
        URL publique non applicable pour stockage local
        Retourne le chemin local
        """
        local_path = self.base_path / key
        if local_path.exists():
            return f"file://{local_path.absolute()}"
        return None
```

### src/utils/adapters.py
```python
"""
Adaptateurs pour conversions de données GuignoMap v5.0
Notamment List[Dict] → DataFrame pour UI Streamlit
"""
import pandas as pd
from typing import List, Dict, Any, Optional


def to_dataframe(data: Any, default_columns: Optional[List[str]] = None) -> pd.DataFrame:
    """
    Convertit des données en DataFrame pandas pour l'affichage Streamlit
    
    Gère les cas:
    - List[Dict] → DataFrame
    - DataFrame → DataFrame (passthrough)
    - Données vides → DataFrame vide avec colonnes par défaut
    - Autres types → DataFrame vide
    
    Args:
        data: Données à convertir
        default_columns: Colonnes par défaut si données vides
    
    Returns:
        pd.DataFrame: Toujours un DataFrame valide
    """
    # Déjà un DataFrame
    if isinstance(data, pd.DataFrame):
        return data
    
    # Liste de dictionnaires → DataFrame
    if isinstance(data, list) and len(data) > 0 and all(isinstance(item, dict) for item in data):
        try:
            return pd.DataFrame(data)
        except Exception as e:
            print(f"Erreur conversion List[Dict] → DataFrame: {e}")
            # Fallback: DataFrame vide avec colonnes détectées
            if data and isinstance(data[0], dict):
                columns = list(data[0].keys())
                return pd.DataFrame(columns=columns)
    
    # Liste vide ou autres types → DataFrame vide
    if default_columns:
        return pd.DataFrame(columns=default_columns)
    else:
        return pd.DataFrame()


def safe_get_dict_value(item: Dict[Any, Any], key: str, default: Any = None) -> Any:
    """
    Récupération sécurisée d'une valeur dans un dictionnaire
    Gère les cas de clés manquantes, None, etc.
    """
    if not isinstance(item, dict):
        return default
    
    return item.get(key, default)


def normalize_street_data(streets_raw: List[Dict]) -> pd.DataFrame:
    """
    Normalise les données de rues pour l'affichage UI
    Assure la cohérence des colonnes et types
    """
    if not streets_raw:
        return pd.DataFrame(columns=['name', 'sector', 'team', 'status'])
    
    # Conversion sécurisée
    df = to_dataframe(streets_raw, default_columns=['name', 'sector', 'team', 'status'])
    
    # Valeurs par défaut pour colonnes manquantes
    required_columns = ['name', 'sector', 'team', 'status']
    for col in required_columns:
        if col not in df.columns:
            df[col] = None
    
    # Nettoyage des valeurs
    df['name'] = df['name'].fillna('(Sans nom)')
    df['sector'] = df['sector'].fillna('(Non assigné)')
    df['team'] = df['team'].fillna('(Aucune équipe)')
    df['status'] = df['status'].fillna('a_faire')
    
    return df


def normalize_team_data(teams_raw: List[Dict]) -> pd.DataFrame:
    """
    Normalise les données d'équipes pour l'affichage UI
    """
    if not teams_raw:
        return pd.DataFrame(columns=['id', 'name', 'active'])
    
    df = to_dataframe(teams_raw, default_columns=['id', 'name', 'active'])
    
    # Valeurs par défaut
    if 'id' not in df.columns:
        df['id'] = None
    if 'name' not in df.columns:
        df['name'] = None
    if 'active' not in df.columns:
        df['active'] = True
    
    # Nettoyage
    df['id'] = df['id'].fillna('(ID manquant)')
    df['name'] = df['name'].fillna('(Nom manquant)')
    df['active'] = df['active'].fillna(True)
    
    return df


def normalize_notes_data(notes_raw: List[Dict]) -> pd.DataFrame:
    """
    Normalise les données de notes pour l'affichage UI
    """
    if not notes_raw:
        return pd.DataFrame(columns=['street_name', 'team_id', 'address_number', 'comment', 'created_at'])
    
    df = to_dataframe(notes_raw)
    
    # Valeurs par défaut
    required_columns = ['street_name', 'team_id', 'address_number', 'comment', 'created_at']
    for col in required_columns:
        if col not in df.columns:
            df[col] = None
    
    # Nettoyage
    df['street_name'] = df['street_name'].fillna('(Rue inconnue)')
    df['team_id'] = df['team_id'].fillna('(Équipe inconnue)')
    df['address_number'] = df['address_number'].fillna('')
    df['comment'] = df['comment'].fillna('')
    
    return df
```

## CODE — FICHIERS MODIFIÉS (extraits pertinents)

### guignomap/app.py (correctifs PHASE 1)
```python
# Import des nouveaux adaptateurs
from src.utils.adapters import to_dataframe, normalize_street_data, normalize_team_data, normalize_notes_data

# [... code existant ...]

def create_map():
    """
    CORRECTIF PHASE 1: DataFrame robuste + coercion + itération sécurisée
    """
    try:
        # Récupération des données avec coercion explicite DataFrame
        streets_raw = db.get_all_streets_with_addresses()
        
        # CORRECTIF 1: Coercion DataFrame en début de fonction
        if not isinstance(streets_raw, pd.DataFrame):
            if isinstance(streets_raw, list):
                streets_df = pd.DataFrame(streets_raw) if streets_raw else pd.DataFrame()
            else:
                streets_df = pd.DataFrame()
        else:
            streets_df = streets_raw.copy()
        
        # CORRECTIF 2: Valeurs par défaut pour éviter les erreurs
        if streets_df.empty:
            st.info("Aucune rue trouvée dans la base de données")
            # Retourner une carte vide centrée sur Montréal
            m = folium.Map(location=[45.5017, -73.5673], zoom_start=12)
            return m
        
        # CORRECTIF 3: Assurer la présence des colonnes requises
        required_columns = ['latitude', 'longitude', 'name', 'status']
        for col in required_columns:
            if col not in streets_df.columns:
                streets_df[col] = None
        
        # Filtrer les rues avec coordonnées valides
        valid_streets = streets_df.dropna(subset=['latitude', 'longitude'])
        
        if valid_streets.empty:
            st.warning("Aucune rue avec coordonnées GPS trouvée")
            m = folium.Map(location=[45.5017, -73.5673], zoom_start=12)
            return m
        
        # Créer la carte
        center_lat = valid_streets['latitude'].mean()
        center_lon = valid_streets['longitude'].mean()
        m = folium.Map(location=[center_lat, center_lon], zoom_start=13)
        
        # CORRECTIF 4: Itération robuste avec gestion d'erreurs
        for idx, row in valid_streets.iterrows():
            try:
                lat = float(row.get('latitude', 0))
                lon = float(row.get('longitude', 0))
                name = str(row.get('name', 'Rue inconnue'))
                status = str(row.get('status', 'a_faire'))
                
                # Définir la couleur selon le statut
                color_map = {
                    'fait': 'green',
                    'en_cours': 'orange', 
                    'a_faire': 'red',
                    'probleme': 'purple'
                }
                color = color_map.get(status, 'gray')
                
                # Ajouter le marqueur
                folium.CircleMarker(
                    location=[lat, lon],
                    radius=8,
                    popup=f"{name}<br>Statut: {status}",
                    color=color,
                    fill=True,
                    fillColor=color,
                    fillOpacity=0.7
                ).add_to(m)
                
            except (ValueError, TypeError, AttributeError) as e:
                print(f"Erreur traitement rue {row.get('name', 'inconnue')}: {e}")
                continue  # Passer à la rue suivante
        
        return m
        
    except Exception as e:
        st.error(f"Erreur lors de la création de la carte: {e}")
        # Retourner carte de fallback
        return folium.Map(location=[45.5017, -73.5673], zoom_start=12)

# [... code existant ...]

# CORRECTIFS UI: Wrapping des st.dataframe() avec to_dataframe()

# Correctif 1: Affichage des rues
if st.button("Voir toutes les rues"):
    streets = db.get_all_streets()
    streets_df = to_dataframe(streets, default_columns=['name', 'sector', 'team', 'status'])
    st.dataframe(streets_df)

# Correctif 2: Statistiques par équipe 
stats = db.stats_by_team()
stats_df = to_dataframe(stats, default_columns=['team', 'total', 'completed'])
st.dataframe(stats_df)

# Correctif 3: Notes d'équipe
team_notes = db.get_team_notes(selected_team)
notes_df = to_dataframe(team_notes, default_columns=['street_name', 'address_number', 'comment', 'created_at'])
st.dataframe(notes_df)

# Correctif 4: Activité récente  
recent_activity = db.recent_activity(limit=50)
activity_df = to_dataframe(recent_activity, default_columns=['team_id', 'action', 'details', 'created_at'])
st.dataframe(activity_df)

# Correctif 5: Liste des équipes (admin)
teams = db.get_all_teams()
teams_df = to_dataframe(teams, default_columns=['id', 'name', 'active'])
st.dataframe(teams_df)

# Correctif 6: Liste des rues (admin)
streets = db.list_streets()
streets_df = to_dataframe(streets, default_columns=['name', 'sector', 'team', 'status'])
st.dataframe(streets_df)
```

### scripts/migrate_sqlite_to_postgres.py (extrait - fonction main)
```python
def main():
    """Migration complète SQLite → PostgreSQL"""
    print("🔄 Début migration SQLite → PostgreSQL...")
    
    # Connexions
    sqlite_conn = get_sqlite_connection()
    if not sqlite_conn:
        return False
    
    try:
        # Créer les tables PostgreSQL
        if not create_postgres_tables():
            return False
        
        # Session PostgreSQL
        engine = get_engine()
        Session = sessionmaker(bind=engine)
        postgres_session = Session()
        
        # Migration par table
        total_migrated = 0
        total_migrated += copy_teams(sqlite_conn, postgres_session)
        total_migrated += copy_streets(sqlite_conn, postgres_session)
        total_migrated += copy_notes(sqlite_conn, postgres_session)
        total_migrated += copy_activity_logs(sqlite_conn, postgres_session)
        total_migrated += copy_addresses(sqlite_conn, postgres_session)
        
        postgres_session.close()
        sqlite_conn.close()
        
        print(f"🎉 Migration terminée ! {total_migrated} enregistrements migrés")
        return True
        
    except Exception as e:
        print(f"❌ Erreur générale migration: {e}")
        if 'postgres_session' in locals():
            postgres_session.close()
        sqlite_conn.close()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
```

### scripts/migrate_password_hashes.py (extrait - fonction analyse)
```python
def analyze_password_hashes():
    """
    Analyse des hashes de mots de passe dans la base
    Identifie les équipes avec des hashes bcrypt qui nécessitent une migration
    """
    conn = get_sqlite_connection()
    if not conn:
        return
    
    try:
        print("🔍 Analyse des hashes de mots de passe...")
        print("=" * 50)
        
        # Récupérer toutes les équipes
        cursor = conn.execute("SELECT id, name, password_hash, created_at FROM teams ORDER BY id")
        teams = cursor.fetchall()
        
        if not teams:
            print("ℹ️ Aucune équipe trouvée dans la base")
            return
        
        bcrypt_count = 0
        argon2_count = 0
        unknown_count = 0
        
        print(f"{'Équipe':<15} {'Algorithme':<10} {'Statut':<20} {'Créé le'}")
        print("-" * 65)
        
        for team in teams:
            # [... logique d'analyse des hashes ...]
            hash_info = get_password_hash_info(hash_value)
            algorithm = hash_info['algorithm']
            
            if algorithm == 'bcrypt':
                bcrypt_count += 1
                status = "🔄 À migrer"
            elif algorithm == 'argon2':
                argon2_count += 1
                status = "✅ Moderne"
            else:
                unknown_count += 1
                status = "❓ Inconnu"
        
        print(f"\n📊 Résumé: {bcrypt_count} bcrypt, {argon2_count} Argon2, {unknown_count} inconnus")
        
    except Exception as e:
        print(f"❌ Erreur lors de l'analyse: {e}")
    finally:
        conn.close()
```

## FICHIERS DE CONFIGURATION

### alembic.ini
```ini
# Configuration Alembic pour migrations PostgreSQL

[alembic]
script_location = src/database/migrations
prepend_sys_path = .
version_path_separator = os
sqlalchemy.url = postgresql://user:pass@localhost/dbname

[post_write_hooks]

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```

### requirements.txt (référence)
```txt
streamlit>=1.36.0
pandas>=2.2.0
requests>=2.31.0
folium==0.20.0
streamlit-folium>=0.21.0
sqlalchemy==2.0.23
alembic==1.13.1
psycopg2-binary>=2.9.7
passlib[argon2]==1.7.4
boto3==1.34.144
bcrypt>=4.0.0
```

## COMMITS PRÉPARÉS (PHASE 1)

### Commit 1: Infrastructure base SQLAlchemy + PostgreSQL
```
feat: infrastructure SQLAlchemy + PostgreSQL v5.0

- Connexion PostgreSQL avec QueuePool + retry logic
- Modèles SQLAlchemy (Street, Team, Note, ActivityLog, Address)
- Configuration centralisée avec secrets Streamlit
- Alembic pour migrations de schéma
- Support fallback SQLite pour dev local

Files:
+ src/config.py
+ src/database/connection.py  
+ src/database/models.py
+ src/database/migrations/env.py
+ alembic.ini
```

### Commit 2: Stockage S3/local + authentification Argon2
```
feat: stockage S3/local + auth Argon2 avec compat bcrypt

- Client S3 pour backup cloud avec fallback local
- Migration paresseuse bcrypt → Argon2 à la connexion
- API stockage unifiée (cloud.py / local.py)
- Scripts de migration SQLite → PostgreSQL
- Scripts d'analyse des hashes de mot de passe

Files:
+ src/storage/cloud.py
+ src/storage/local.py
+ src/auth/passwords.py
+ scripts/migrate_sqlite_to_postgres.py
+ scripts/migrate_password_hashes.py
```

### Commit 3: Correctifs DataFrame + adapters UI (CRITIQUE)
```
fix: correctifs DataFrame + adapters UI robustes

CORRECTIFS PHASE 1 AUDIT:
- create_map(): coercion DataFrame + itération sécurisée + valeurs par défaut
- Wrapping 6× st.dataframe() avec to_dataframe() adapter
- Adapter List[Dict] → DataFrame pour cohérence UI
- Gestion erreurs robuste (rue manquante, coordonnées invalides)

Files:
~ guignomap/app.py (create_map + 6 st.dataframe fixes)
+ src/utils/adapters.py
```

## VALIDATION & TESTS RECOMMANDÉS

### Scripts PowerShell de validation (déjà fournis)
```powershell
# Validation DataFrame (à exécuter après correctifs)
python -c "
import pandas as pd
from src.utils.adapters import to_dataframe

# Test 1: List[Dict] → DataFrame
data = [{'name': 'Rue A', 'status': 'fait'}, {'name': 'Rue B', 'status': 'a_faire'}]
df = to_dataframe(data)
assert isinstance(df, pd.DataFrame)
assert len(df) == 2
print('✅ Test 1 passed: List[Dict] → DataFrame')

# Test 2: DataFrame → DataFrame (passthrough)
df2 = to_dataframe(df)
assert df.equals(df2)
print('✅ Test 2 passed: DataFrame passthrough')

# Test 3: Données vides → DataFrame vide
empty_df = to_dataframe([], default_columns=['name', 'status'])
assert isinstance(empty_df, pd.DataFrame)
assert list(empty_df.columns) == ['name', 'status']
print('✅ Test 3 passed: Empty → DataFrame with defaults')

print('🎉 Tous les tests adapters passed')
"
```

### Tests de connexion base PostgreSQL
```powershell
python -c "
from src.database.connection import test_connection, get_engine
from src.config import get_database_url

print(f'Database URL: {get_database_url()[:20]}...')  # Masquer credentials
engine = get_engine()
print(f'Engine pool size: {engine.pool.size()}')

if test_connection():
    print('✅ Connexion PostgreSQL OK')
else:
    print('❌ Connexion PostgreSQL échouée')
"
```

## PROCÉDURES DE DÉPLOIEMENT

### 1. Environnement local (SQLite)
```powershell
# Setup venv
py -m venv .venv
.\.venv\Scripts\activate
pip install -r requirements.txt

# Test local
streamlit run guignomap/app.py
```

### 2. Déploiement Streamlit Cloud (PostgreSQL)
```yaml
# Secrets Streamlit Cloud (.streamlit/secrets.toml)
[database]
url = "postgresql://user:***@***:5432/guignomap"
pool_size = 5
max_overflow = 10

[storage]
s3_bucket = "guignomap-prod"
s3_region = "us-east-1"
s3_access_key = "AKIA***"
s3_secret_key = "***"
cdn_base_url = "https://cdn.example.com"  # optionnel
```

### 3. Migration des données (une fois en prod)
```powershell
# Exécuter depuis l'environnement local avec accès PostgreSQL prod
python scripts/migrate_sqlite_to_postgres.py
python scripts/migrate_password_hashes.py  # Analyse seulement
```

## FICHIERS MANQUANTS/AMBIGUS DÉTECTÉS

### Fichiers non trouvés lors de l'export:
- `src/ui/pages/admin_migration.py` : référencé mais absent du repo
- Révisions Alembic dans `src/database/migrations/versions/` : dossier vide

### Questions pour l'utilisateur:
1. Le fichier `admin_migration.py` doit-il être créé ou était-ce une référence incorrecte ?
2. Des révisions Alembic doivent-elles être générées (`alembic revision --autogenerate`) ?
3. Y a-t-il d'autres fichiers/configurations spécifiques au déploiement Streamlit Cloud ?

## NOTES IMPORTANTES

- **IPv6 Streamlit Cloud**: Les migrations PostgreSQL doivent être exécutées côté Cloud car l'environnement local peut avoir des problèmes IPv6
- **Politique MDP v4.1 conservée**: Minimum 4 caractères + confirmation pour maintenir l'UX existante
- **Migration paresseuse**: Les hashes bcrypt sont automatiquement migrés vers Argon2 lors de la prochaine connexion réussie
- **Fallbacks robustes**: Tous les composants ont des fallbacks SQLite/local en cas de problème PostgreSQL/S3
- **Logs détaillés**: Tous les composants loggent leurs actions pour faciliter le debug

---
**FIN DE L'EXPORT - Version v5.0 PHASE 1 complète**
**Date: 2025-09-15 14:30:00 local**
**Taille: ~15KB de code + config + documentation**
```
```