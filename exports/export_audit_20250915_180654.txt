# GuignoMap — Export d’audit COMPLET (code et config utiles)
# Date : 2025-09-15 18:06:55
# Racine : C:\Users\nick\guignomap_clone\GuignoMap
# Contenu : 100% des .py (zones pertinentes) + fichiers de config/migrations essentiels
# Exclus : backups, exports, caches, venv, .git, binaires, secrets (.streamlit/secrets.toml)

## ENVIRONNEMENT
- Python : 3.13.6 (tags/v3.13.6:4e66535, Aug  6 2025, 14:36:00) [MSC v.1944 64 bit (AMD64)]
- Exécutable : C:\Users\nick\guignomap_clone\GuignoMap\.venv\Scripts\python.exe
- Plateforme : Windows-11-10.0.26100-SP0
- streamlit : 1.49.1
- sqlalchemy : 2.0.43
- pandas : 2.3.2
- boto3 : 1.34.144
- passlib : présent

### Dépendances installées (inventaire)
alembic==1.16.5
altair==5.5.0
argon2-cffi-bindings==25.1.0
argon2-cffi==25.1.0
attrs==25.3.0
bcrypt==4.3.0
blinker==1.9.0
boto3==1.34.144
botocore==1.34.162
branca==0.8.1
cachetools==6.2.0
certifi==2025.8.3
cffi==2.0.0
charset-normalizer==3.4.3
click==8.2.1
colorama==0.4.6
folium==0.20.0
gitdb==4.0.12
GitPython==3.1.45
greenlet==3.2.4
idna==3.10
Jinja2==3.1.6
jmespath==1.0.1
jsonschema-specifications==2025.9.1
jsonschema==4.25.1
Mako==1.3.10
MarkupSafe==3.0.2
narwhals==2.5.0
numpy==2.3.3
overpy==0.7
packaging==25.0
pandas==2.3.2
passlib==1.7.4
pillow==11.3.0
pip==25.2
plotly==6.3.0
protobuf==6.32.1
psycopg2-binary==2.9.10
pyarrow==21.0.0
pycparser==2.23
pydeck==0.9.1
python-dateutil==2.9.0.post0
pytz==2025.2
referencing==0.36.2
reportlab==4.4.3
requests==2.32.5
rpds-py==0.27.1
s3transfer==0.10.4
six==1.17.0
smmap==5.0.2
SQLAlchemy==2.0.43
streamlit-folium==0.25.1
streamlit==1.49.1
tenacity==9.1.2
toml==0.10.2
tornado==6.5.2
typing_extensions==4.15.0
tzdata==2025.2
urllib3==2.5.0
watchdog==6.0.0
xlsxwriter==3.2.8
xyzservices==2025.4.0

## INDEX DES FICHIERS INCLUS
- .gitignore
- .streamlit/config.toml
- alembic.ini
- guignomap/__init__.py
- guignomap/app.py
- guignomap/backup.py
- guignomap/db.py
- guignomap/osm.py
- guignomap/reports.py
- guignomap/validators.py
- requirements.txt
- scripts/export_repo_audit.py
- scripts/export_repo_min.py
- scripts/export_repo_snapshot.py
- scripts/fix_app_types.py
- scripts/fix_specific.py
- scripts/migrate_password_hashes.py
- scripts/migrate_sqlite_to_postgres.py
- scripts/validation_dataframe.ps1
- src/auth/passwords.py
- src/config.py
- src/database/connection.py
- src/database/db_v5.py
- src/database/migrations/env.py
- src/database/models.py
- src/storage/__init__.py
- src/storage/cloud.py
- src/storage/local.py
- src/utils/__init__.py
- src/utils/adapters.py
- tests/manual/test_db_connection.py
- tests/manual/test_db_simple.py

## CONTENU DES FICHIERS

---8<--- .gitignore BEGIN ---
```txt
# ===============================================
# GITIGNORE POUR GUIGNO-MAP
# ===============================================

# ----------------------------------------
# ENVIRONNEMENTS PYTHON
# ----------------------------------------
.venv/
venv/
env/
ENV/
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
pip-log.txt
pip-delete-this-directory.txt

# ----------------------------------------
# BASES DE DONNÉES
# ----------------------------------------
*.db
*.sqlite
*.sqlite3
guigno_map.db

# ----------------------------------------
# CACHES & DONNÉES TEMPORAIRES
# ----------------------------------------
osm_cache.json
*.cache
*.tmp
.DS_Store
Thumbs.db

# ----------------------------------------
# LOGS & EXPORTS
# ----------------------------------------
*.log
export_*.txt
rapport_*.csv

# ----------------------------------------
# VS CODE & ÉDITEURS
# ----------------------------------------
.vscode/
*.swp
*.swo
*~

# ----------------------------------------
# SYSTÈME WINDOWS/LINUX
# ----------------------------------------
*.Zone.Identifier
.Trash-*
Desktop.ini

# ----------------------------------------
# DÉPENDANCES NODE (si ajoutées plus tard)
# ----------------------------------------
node_modules/
npm-debug.log*

# ----------------------------------------
# STREAMLIT
# ----------------------------------------
.streamlit/secrets.toml

# ----------------------------------------
# DONNÉES SENSIBLES
# ----------------------------------------
*.key
*.pem
.env
config.ini

# ----------------------------------------
# FICHIERS DE SAUVEGARDE
# ----------------------------------------
*.bak
*.backup
*.save
*~

# ----------------------------------------
# KEEP THESE FILES (exceptions)
# ----------------------------------------
!.streamlit/config.toml
!guignomap/assets/
!requirements.txt
!README.md
# exports snapshots
/exports/
export.txt

```
---8<--- .gitignore END ---

---8<--- .streamlit/config.toml BEGIN ---
```toml
[theme]
# Thème sombre avec les couleurs du Relais
base = "dark"
primaryColor = "#A9CF3B"              # Vert du Relais
backgroundColor = "#0F1318"           # Fond très sombre
secondaryBackgroundColor = "#1A1F26"  # Fond secondaire
textColor = "#F2F3F5"                 # Texte clair
font = "sans serif"

[client]
# Configuration minimale de la toolbar
toolbarMode = "minimal"
showErrorDetails = false

[runner]
# Optimisations de performance
magicEnabled = true
installTracer = false
fixMatplotlib = true

[server]
# Configuration serveur
headless = true
runOnSave = true
maxUploadSize = 10
enableCORS = false
enableXsrfProtection = true

[deprecation]
# Désactiver les avertissements de dépréciation
showImageFormat = false
showPyplotGlobalUse = false
```
---8<--- .streamlit/config.toml END ---

---8<--- alembic.ini BEGIN ---
```ini
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/src/database/migrations

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the python>=3.9 or backports.zoneinfo library and tzdata library.
# Any required deps can installed by adding `alembic[tz]` to the pip requirements
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
# sqlalchemy.url = driver://user:pass@localhost/dbname
# Note: URL configured programmatically via src.config in env.py


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S

```
---8<--- alembic.ini END ---

---8<--- guignomap/__init__.py BEGIN ---
```py

```
---8<--- guignomap/__init__.py END ---

---8<--- guignomap/app.py BEGIN ---
```py
"""
Guigno-Map - Application de gestion de collecte de denrées
Le Relais de Mascouche
Version 3.0 - Production
"""

from pathlib import Path
import time
from datetime import datetime
import pandas as pd
import streamlit as st
import folium
from streamlit_folium import st_folium

# Import des modules locaux
import db_v5 as db
from validators import validate_and_clean_input
from osm import build_geometry_cache, load_geometry_cache, build_addresses_cache, load_addresses_cache, CACHE_FILE
from src.utils.adapters import to_dataframe

# Configuration des chemins - Legacy pour backup seulement
DB_PATH = Path(__file__).parent / "guigno_map.db"

# --- Utilitaire de compatibilité pandas Styler ---
from typing import Callable, Any

def style_map_compat(df: pd.DataFrame, fn: Callable[[Any], str], subset: Any = None):
    """Applique un style cellule-à-cellule en utilisant Styler.map si disponible,
    sinon fallback dynamique vers applymap sans exposer l'attribut (OK pour Pylance).
    
    Args:
        df: DataFrame à styliser
        fn: Fonction qui prend une valeur cellule et retourne une string CSS
        subset: Colonnes à cibler (ex: ['status'] ou None pour toutes)
    """
    styler = df.style
    if hasattr(styler, "map"):
        # Pandas 2.4+ : utilise la nouvelle API map()
        return styler.map(fn, subset=subset)
    # Pandas < 2.4 : fallback vers applymap (sans référence statique)
    return getattr(styler, "applymap")(fn, subset=subset)

# --- Mapping des statuts pour l'affichage ---
STATUS_TO_LABEL = {"a_faire": "À faire", "en_cours": "En cours", "terminee": "Terminée"}
LABEL_TO_STATUS = {v: k for k, v in STATUS_TO_LABEL.items()}

ASSETS = Path(__file__).parent / "assets"

# Configuration Streamlit
st.set_page_config(
    page_title="Guigno-Map | Relais de Mascouche",
    page_icon="🎁",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialisation session
if "auth" not in st.session_state:
    st.session_state.auth = None

# ============================================
# COMPOSANTS UI
# ============================================

def inject_css():
    """Charge le CSS depuis le fichier externe"""
    css_file = ASSETS / "styles.css"
    if css_file.exists():
        css = css_file.read_text(encoding="utf-8")
        st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)

def render_header():
    """Header moderne avec logo Guignolée et design festif"""
    
    # Container principal avec fond festif
    st.markdown("""
    <div style="
        background: linear-gradient(135deg, #c41e3a 0%, #165b33 100%);
        border-radius: 20px;
        padding: 2rem;
        margin-bottom: 2rem;
        position: relative;
        overflow: hidden;
        box-shadow: 0 10px 30px rgba(0,0,0,0.3);
    ">
        <!-- Flocons de neige animés en CSS -->
        <div style="position: absolute; width: 100%; height: 100%; opacity: 0.1;">
            <span style="position: absolute; top: 10%; left: 10%; font-size: 2rem;">❄️</span>
            <span style="position: absolute; top: 20%; left: 80%; font-size: 1.5rem;">❄️</span>
            <span style="position: absolute; top: 60%; left: 30%; font-size: 1.8rem;">❄️</span>
        </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([2, 5, 2])
    
    with col1:
        # Logo Guignolée
        if (ASSETS / "guignolee.png").exists():
            st.image(str(ASSETS / "guignolee.png"), width=150)
    
    with col2:
        st.markdown("""
        <div style="text-align: center;">
            <h1 style="
                color: white;
                font-family: 'Manrope', sans-serif;
                font-size: 2.5rem;
                margin: 0;
                text-shadow: 3px 3px 6px rgba(0,0,0,0.5);
                letter-spacing: 2px;
            ">🎅 GUIGNOLÉE 2025 🎁</h1>
            <p style="
                color: #FFD700;
                font-size: 1.2rem;
                margin: 0.5rem 0 0 0;
                font-weight: 600;
            ">Le Relais de Mascouche - 1er décembre</p>
            <p style="
                color: rgba(255,255,255,0.9);
                font-size: 1rem;
                margin-top: 0.5rem;
            ">Système de gestion de collecte</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        # Stats en temps réel
        stats = db.extended_stats()
        progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
        
        st.markdown(f"""
        <div style="
            background: rgba(255,255,255,0.2);
            border-radius: 15px;
            padding: 1rem;
            text-align: center;
        ">
            <div style="color: #FFD700; font-size: 2rem; font-weight: bold;">
                {progress:.0f}%
            </div>
            <div style="color: white; font-size: 0.9rem;">
                Complété
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("</div>", unsafe_allow_html=True)

def render_login_card(role="benevole", conn=None):
    """Carte de connexion moderne avec design festif"""
    
    # Container de connexion stylisé
    st.markdown("""
    <div style="
        max-width: 400px;
        margin: 3rem auto;
        background: linear-gradient(135deg, rgba(255,255,255,0.1), rgba(255,255,255,0.05));
        backdrop-filter: blur(10px);
        border: 2px solid rgba(255,215,0,0.3);
        border-radius: 20px;
        padding: 2rem;
        box-shadow: 0 20px 40px rgba(0,0,0,0.3);
    ">
    """, unsafe_allow_html=True)
    
    # Icône et titre
    if role == "superviseur" or role == "gestionnaire":
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="font-size: 3rem;">👔</div>
            <h2 style="color: #FFD700; margin: 1rem 0;">Espace Gestionnaire</h2>
            <p style="color: #cbd5e1;">Gérez la collecte et les équipes</p>
        </div>
        """, unsafe_allow_html=True)
        
        with st.form("login_gestionnaire", clear_on_submit=False):
            password = st.text_input(
                "🔐 Mot de passe",
                type="password",
                placeholder="Entrez le mot de passe gestionnaire"
            )
            
            col1, col2, col3 = st.columns([1,2,1])
            with col2:
                submit = st.form_submit_button(
                    "🚀 Connexion",
                    width="stretch"
                )
            
            if submit:
                if db.verify_team("ADMIN", password):
                    st.session_state.auth = {"role": "supervisor", "team_id": "ADMIN"}
                    st.success("✅ Bienvenue dans l'espace gestionnaire!")
                    st.snow()
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("❌ Mot de passe incorrect")
    
    else:  # Bénévole
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="font-size: 3rem;">🎅</div>
            <h2 style="color: #FFD700; margin: 1rem 0;">Espace Bénévole</h2>
            <p style="color: #cbd5e1;">Accédez à vos rues assignées</p>
        </div>
        """, unsafe_allow_html=True)
        
        with st.form("login_benevole", clear_on_submit=False):
            col1, col2 = st.columns(2)
            
            with col1:
                team_id = st.text_input(
                    "👥 Identifiant d'équipe",
                    placeholder="Ex: EQ001"
                )
            
            with col2:
                password = st.text_input(
                    "🔐 Mot de passe",
                    type="password",
                    placeholder="Mot de passe équipe"
                )
            
            col1, col2, col3 = st.columns([1,2,1])
            with col2:
                submit = st.form_submit_button(
                    "🎄 Connexion",
                    width="stretch"
                )
            
            if submit:
                if db.verify_team(team_id, password):
                    st.session_state.auth = {"role": "volunteer", "team_id": team_id}
                    st.success(f"✅ Bienvenue équipe {team_id}!")
                    st.snow()
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("❌ Identifiants incorrects")
    
    st.markdown("</div>", unsafe_allow_html=True)
    
    # Aide en bas
    st.markdown("""
    <div style="text-align: center; margin-top: 2rem; color: #8b92a4;">
        <small>
        Besoin d'aide? Contactez votre gestionnaire<br>
        📞 450-474-4133
        </small>
    </div>
    """, unsafe_allow_html=True)

def render_metrics(stats):
    """Affiche les métriques principales"""
    progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Rues", stats['total'])
    
    with col2:
        st.metric("Rues Terminées", stats['done'])
    
    with col3:
        st.metric("En Cours", stats.get('partial', 0))
    
    with col4:
        st.metric("Progression", f"{progress:.1f}%")

def render_dashboard_gestionnaire(conn, geo):
    """Dashboard moderne pour gestionnaires avec KPIs visuels"""
    
    # KPIs principaux en cartes colorées
    stats = db.extended_stats()
    progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
    
    st.markdown("### 📊 Tableau de bord en temps réel")
    
    # Ligne de KPIs avec icônes festives
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #22c55e, #16a34a);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(34,197,94,0.3);
        ">
            <div style="font-size: 2.5rem;">🏘️</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{stats['total']}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Total Rues</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(59,130,246,0.3);
        ">
            <div style="font-size: 2.5rem;">✅</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{stats['done']}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Terminées</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #f59e0b, #d97706);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(245,158,11,0.3);
        ">
            <div style="font-size: 2.5rem;">🚶</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{stats.get('partial', 0)}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">En cours</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        # Nombre d'équipes actives
        teams_count = len(db.teams())
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #8b5cf6, #7c3aed);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(139,92,246,0.3);
        ">
            <div style="font-size: 2.5rem;">👥</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{teams_count}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Équipes</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col5:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #c41e3a, #165b33);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(196,30,58,0.3);
        ">
            <div style="font-size: 2.5rem;">🎯</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{progress:.0f}%</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Progression</div>
        </div>
        """, unsafe_allow_html=True)
    
    # Barre de progression visuelle
    st.markdown("### 🎄 Progression globale")
    st.progress(progress / 100)
    
    # Graphique par secteur (si disponible)
    st.markdown("### 📈 Performance par équipe")
    try:
        teams_stats = db.stats_by_team()
        if teams_stats:  # Liste non vide
            # Convertir en DataFrame pour plotly
            import pandas as pd
            teams_df = pd.DataFrame(teams_stats)
            
            # Calculer le pourcentage de progression
            teams_df['progress'] = ((teams_df['completed'] / teams_df['total_streets']) * 100).fillna(0)
            
            # Graphique en barres colorées
            import plotly.express as px
            fig = px.bar(
                teams_df, 
                x='id', 
                y='progress',
                color='progress',
                color_continuous_scale=['#ef4444', '#f59e0b', '#22c55e'],
                labels={'team': 'Équipe', 'progress': 'Progression (%)'},
                title="Performance des équipes"
            )
            fig.update_layout(
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                font_color='white'
            )
            st.plotly_chart(fig, width="stretch")
        else:
            st.info("Aucune statistique d'équipe disponible")
    except Exception as e:
        st.warning("Graphiques non disponibles (module plotly manquant)")
        # Fallback vers un tableau simple
        try:
            teams_stats = db.stats_by_team()
            if teams_stats:  # Liste non vide
                st.dataframe(to_dataframe(teams_stats), width="stretch")
        except:
            st.info("Aucune statistique d'équipe disponible")

def add_persistent_legend(m):
    """Ajoute une légende persistante pour les 4 états des rues via contrôle HTML"""
    legend_html = """
    <div id='gm-legend' class='leaflet-control-layers leaflet-control' 
         style='position: absolute; bottom: 10px; right: 10px; z-index: 1000;
                background: white; border: 2px solid rgba(0,0,0,0.2); 
                border-radius: 5px; padding: 10px; box-shadow: 0 1px 5px rgba(0,0,0,0.2);
                font-family: "Helvetica Neue", Arial, Helvetica, sans-serif; 
                font-size: 12px; line-height: 18px; color: #333;'>
        <strong style='margin-bottom: 8px; display: block;'>Légende</strong>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px solid #28a745; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>Terminée</span>
        </div>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px solid #f1c40f; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>En cours</span>
        </div>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px solid #ff4d4f; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>Assignée (à faire)</span>
        </div>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px dashed #ff4d4f; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>Non assignée</span>
        </div>
    </div>
    """
    m.get_root().html.add_child(folium.Element(legend_html))

def create_map(df, geo):
    """Crée la carte Folium centrée sur Mascouche avec toutes les rues"""
    # 1) Coercition sûre en DataFrame
    if not isinstance(df, pd.DataFrame):
        try:
            df = pd.DataFrame(df)
        except Exception:
            df = pd.DataFrame([])
    
    # Limites de Mascouche
    bounds = {
        "north": 45.78,
        "south": 45.70,
        "east": -73.55,
        "west": -73.70
    }
    center = [(bounds["north"] + bounds["south"]) / 2, 
              (bounds["east"] + bounds["west"]) / 2]
    
    # Créer la carte
    m = folium.Map(
        location=center,
        zoom_start=13,  # Zoom optimisé pour voir toute la ville
        tiles="https://{s}.tile.openstreetmap.fr/osmfr/{z}/{x}/{y}.png",
        attr='© OpenStreetMap France',
        control_scale=True,
        max_bounds=True,
        min_zoom=11,
        max_zoom=18,
        prefer_canvas=True,
        zoom_control=True,
        scrollWheelZoom=True
    )
    
    # Ajouter plusieurs couches de fond
    folium.TileLayer(
        tiles='https://{s}.tile.openstreetmap.fr/osmfr/{z}/{x}/{y}.png',
        attr='© OpenStreetMap France',
        name='OSM France (Détaillé)',
        overlay=False,
        control=True
    ).add_to(m)

    folium.TileLayer(
        tiles='https://{s}.basemaps.cartocdn.com/rastertiles/voyager/{z}/{x}/{y}.png',
        attr='© CARTO',
        name='CARTO Voyager',
        overlay=False,
        control=True
    ).add_to(m)

    folium.TileLayer(
        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}',
        attr='© Esri',
        name='Esri WorldStreetMap',
        overlay=False,
        control=True
    ).add_to(m)

    # Ajouter le contrôle des couches
    folium.LayerControl().add_to(m)
    
    # Définir les limites de la carte sur Mascouche
    m.fit_bounds([[bounds["south"], bounds["west"]], 
                  [bounds["north"], bounds["east"]]])
    
    if not geo:
        st.warning("Aucune donnée géométrique disponible")
        return m
    
    # Construire le lookup des infos DB
    street_info = {}
    if not df.empty:  # DataFrame non vide
        for idx, row in df.iterrows():
            name = str(row.get('name', '')) if pd.notna(row.get('name', '')) else ''
            status = row.get('status', 'a_faire')
            status = status if pd.notna(status) else 'a_faire'
            team = row.get('team', '')
            team = team if pd.notna(team) else ''
            notes = str(row.get('notes', '0')) if pd.notna(row.get('notes', '0')) else '0'
            
            street_info[name] = {
                'status': status,
                'team': str(team).strip() if team else '',
                'notes': notes
            }
    
    # Couleurs par statut
    status_colors = {
        'terminee': '#22c55e',  # Vert
        'en_cours': '#f59e0b',  # Orange
        'a_faire': '#ef4444'    # Rouge
    }
    
    # Compteurs pour stats
    stats = {"total": 0, "assigned": 0, "unassigned": 0}
    
    # Ajouter TOUTES les rues de la géométrie
    for name, paths in geo.items():
        stats["total"] += 1
        
        # Info depuis DB ou défaut (rouge pointillé)
        info = street_info.get(name, {
            'status': 'a_faire',
            'team': '',
            'notes': '0'
        })
        
        status = info['status']
        team = info['team']
        notes = info['notes']
        
        # Style: TOUJOURS pointillé si pas d'équipe
        has_team = bool(team)
        color = status_colors.get(status, '#ef4444')  # Rouge par défaut
        opacity = 0.9 if has_team else 0.7
        dash = None if has_team else '8,12'  # Pointillés si non assigné
        weight = 7 if has_team else 5
        
        if has_team:
            stats["assigned"] += 1
        else:
            stats["unassigned"] += 1
        
        # Tooltip informatif
        tooltip_html = f"""
        <div style='font-family: sans-serif'>
            <strong style='font-size: 14px'>{name}</strong><br>
            <span style='color: {color}'>● Statut: {status.replace('_', ' ').title()}</span><br>
            <span>📋 Équipe: {team if team else '⚠️ NON ASSIGNÉE'}</span><br>
            <span>📝 Notes: {notes}</span>
        </div>
        """
        
        # Ajouter chaque segment de la rue
        for path in paths:
            if path and len(path) >= 2:
                folium.PolyLine(
                    path,
                    color=color,
                    weight=weight,
                    opacity=opacity,
                    dash_array=dash,
                    tooltip=folium.Tooltip(tooltip_html, sticky=True)
                ).add_to(m)
    
    # Ajouter un marqueur au centre-ville
    folium.Marker(
        [45.7475, -73.6005],
        popup="Centre-ville de Mascouche",
        tooltip="Centre-ville",
        icon=folium.Icon(color='red', icon='info-sign')
    ).add_to(m)
    
    # Ajouter la légende persistante
    add_persistent_legend(m)
    
    return m


# ============================================
# UTILITAIRES EXPORT
# ============================================

def export_excel_professionnel(conn):
    """Export Excel avec mise en forme professionnelle"""
    try:
        from reports import ReportGenerator
        generator = ReportGenerator(conn)
        return generator.generate_excel()
    except ImportError:
        # Fallback si les dépendances ne sont pas installées
        return db.export_to_csv()


# ============================================
# FONCTIONNALITÉS AVANCÉES
# ============================================

def detect_mobile():
    """Détecte si l'utilisateur est sur mobile"""
    try:
        # Récupérer les paramètres de l'URL pour forcer le mode mobile
        query_params = st.experimental_get_query_params()
        if 'mobile' in query_params:
            return True
        
        # Mobile-first approach pour l'instant
        return True
    except:
        return False

def show_notification(message, type="success"):
    """Affiche une notification stylisée"""
    icons = {
        "success": "✅",
        "error": "❌",
        "warning": "⚠️",
        "info": "ℹ️"
    }
    colors = {
        "success": "#22c55e",
        "error": "#ef4444", 
        "warning": "#f59e0b",
        "info": "#3b82f6"
    }
    
    st.markdown(f"""
    <div style="
        position: fixed;
        top: 20px;
        right: 20px;
        background: {colors[type]};
        color: white;
        padding: 1rem 1.5rem;
        border-radius: 10px;
        box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        z-index: 9999;
        animation: slideIn 0.3s ease-out;
    ">
        <strong>{icons[type]} {message}</strong>
    </div>
    <style>
    @keyframes slideIn {{
        from {{ transform: translateX(100%); opacity: 0; }}
        to {{ transform: translateX(0); opacity: 1; }}
    }}
    </style>
    """, unsafe_allow_html=True)

def show_team_badges(conn, team_id):
    """Affiche les badges de réussite de l'équipe"""
    try:
        df = db.list_streets(team=team_id)
        done = len(df[df['status'] == 'terminee'])
        total = len(df)
        
        badges = []
        if done >= 1:
            badges.append("🏆 Première rue!")
        if done >= total * 0.25:
            badges.append("🥉 25% complété")
        if done >= total * 0.5:
            badges.append("🥈 50% complété")
        if done >= total * 0.75:
            badges.append("🥇 75% complété")
        if done == total:
            badges.append("🌟 CHAMPION!")
        
        if badges:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #FFD700, #FFA500);
                padding: 1rem;
                border-radius: 10px;
                text-align: center;
                margin: 1rem 0;
            ">
                <strong>Vos badges:</strong><br>
                <div style="font-size: 2rem; margin-top: 0.5rem;">
                    {' '.join(badges)}
                </div>
            </div>
            """, unsafe_allow_html=True)
    except:
        pass

def generate_sms_list(conn):
    """Génère une liste de téléphones pour SMS de groupe"""
    try:
        # Cette fonction nécessiterait une table de téléphones
        # Pour l'instant, retourne un exemple
        return "# Liste des téléphones bénévoles\n# 450-XXX-XXXX\n# 438-XXX-XXXX"
    except:
        return "Liste non disponible"

def page_export_gestionnaire(conn):
    """Section export avec formats multiples"""
    
    st.markdown("### 📊 Centre d'export des données")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div style="text-align: center; padding: 1rem; border: 2px dashed #ccc; border-radius: 10px;">
            <h4>� Rapport PDF</h4>
            <p><small>Format professionnel pour présentation</small></p>
        </div>
        """, unsafe_allow_html=True)
        
        try:
            from reports import ReportGenerator
            generator = ReportGenerator(conn)
            pdf_data = generator.generate_pdf()
            st.download_button(
                "📥 Télécharger PDF",
                pdf_data,
                "rapport_guignolee_2025.pdf",
                "application/pdf",
                width="stretch"
            )
        except ImportError:
            st.button("PDF (Installer reportlab)", disabled=True, width="stretch")
    
    with col2:
        st.markdown("""
        <div style="text-align: center; padding: 1rem; border: 2px dashed #ccc; border-radius: 10px;">
            <h4>📊 Excel détaillé</h4>
            <p><small>Avec graphiques et mise en forme</small></p>
        </div>
        """, unsafe_allow_html=True)
        
        try:
            excel_data = export_excel_professionnel(conn)
            st.download_button(
                "📥 Télécharger Excel",
                excel_data,
                "guignolee_2025.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                width="stretch"
            )
        except:
            st.button("Excel (Non disponible)", disabled=True, width="stretch")
    
    with col3:
        st.markdown("""
        <div style="text-align: center; padding: 1rem; border: 2px dashed #ccc; border-radius: 10px;">
            <h4>📱 Liste SMS</h4>
            <p><small>Téléphones des bénévoles</small></p>
        </div>
        """, unsafe_allow_html=True)
        
        sms_list = generate_sms_list(conn)
        st.download_button(
            "📥 Liste téléphones",
            sms_list,
            "telephones_benevoles.txt",
            "text/plain",
            width="stretch"
        )


# ============================================
# PAGES
# ============================================

def page_accueil(conn, geo):
    """Page d'accueil"""
    st.markdown("### 🎁 Bienvenue sur Guigno-Map!")
    st.info("Sélectionnez votre mode dans le menu de gauche pour commencer.")
    
    st.markdown("---")
    st.markdown("#### 📊 Aperçu de la collecte")
    
    stats = db.extended_stats()
    render_metrics(stats)
    
    df_all = db.list_streets()
    if df_all:  # Liste non vide
        m = create_map(df_all, geo)
        st_folium(m, height=800, width=None, returned_objects=[])

def page_accueil_v2(conn, geo):
    """Page d'accueil festive avec compte à rebours"""
    
    # Compte à rebours jusqu'au 1er décembre
    from datetime import datetime, timedelta
    target = datetime(2025, 12, 1, 8, 0, 0)
    now = datetime.now()
    diff = target - now
    
    if diff.days > 0:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #c41e3a, #165b33);
            padding: 2rem;
            border-radius: 20px;
            text-align: center;
            margin-bottom: 2rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        ">
            <h2 style="color: #FFD700; margin: 0;">⏰ Compte à rebours Guignolée</h2>
            <div style="font-size: 3rem; color: white; margin: 1rem 0;">
                {diff.days} jours {diff.seconds//3600} heures
            </div>
            <p style="color: rgba(255,255,255,0.9);">avant le grand jour!</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div style="
            background: linear-gradient(135deg, #FFD700, #FFA500);
            padding: 2rem;
            border-radius: 20px;
            text-align: center;
            margin-bottom: 2rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        ">
            <h2 style="color: #c41e3a; margin: 0;">🎉 C'EST AUJOURD'HUI!</h2>
            <div style="font-size: 2rem; color: #165b33; margin: 1rem 0;">
                Bonne Guignolée 2025!
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    # Hero section festif
    st.markdown("""
    <div style="
        text-align: center;
        padding: 3rem 1rem;
        background: linear-gradient(135deg, rgba(196,30,58,0.1), rgba(22,91,51,0.1));
        border-radius: 20px;
        margin-bottom: 2rem;
    ">
        <h1 style="font-size: 3rem; margin: 0;">🎄 Bienvenue sur Guigno-Map 🎄</h1>
        <p style="font-size: 1.3rem; color: #666; margin: 1rem 0;">
            Votre plateforme digitale pour la Guignolée 2025
        </p>
        <p style="color: #888;">
            Gérez efficacement votre collecte de denrées avec une interface moderne
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Stats visuelles améliorées
    stats = db.extended_stats()
    progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
    
    st.markdown("### 📊 État de la collecte en temps réel")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #22c55e, #16a34a);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(34,197,94,0.3);
        ">
            <div style="font-size: 3rem;">🏘️</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{stats['total']}</div>
            <div>Total Rues</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(59,130,246,0.3);
        ">
            <div style="font-size: 3rem;">✅</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{stats['done']}</div>
            <div>Complétées</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #f59e0b, #d97706);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(245,158,11,0.3);
        ">
            <div style="font-size: 3rem;">🚶</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{stats.get('partial', 0)}</div>
            <div>En Cours</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #c41e3a, #165b33);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(196,30,58,0.3);
        ">
            <div style="font-size: 3rem;">🎯</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{progress:.0f}%</div>
            <div>Progression</div>
        </div>
        """, unsafe_allow_html=True)
    
    # Barre de progression globale
    st.markdown("### 🎄 Progression globale de la collecte")
    st.progress(progress / 100)
    
    # Carte festive
    st.markdown("### 🗺️ Vue d'ensemble de Mascouche")
    df_all = db.list_streets()
    if df_all:  # Liste non vide
        m = create_map(df_all, geo)
        st_folium(m, height=750, width=None, returned_objects=[])
    
    # CSS pour réduire l'espace après la carte
    st.markdown("""
    <style>
    div[data-testid="stVerticalBlock"] > div:has(iframe) {
        margin-bottom: 0 !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Call to action
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem;
        background: linear-gradient(135deg, rgba(255,215,0,0.1), rgba(255,215,0,0.05));
        border: 2px solid rgba(255,215,0,0.3);
        border-radius: 15px;
        margin-top: 1rem;
    ">
        <h3>🎅 Prêt à participer ?</h3>
        <p>Choisissez votre rôle dans le menu de gauche pour commencer</p>
        <p style="font-size: 0.9rem; color: #666;">
            Bénévoles : Accédez à vos rues assignées<br>
            Gestionnaires : Supervisez toute la collecte
        </p>
    </div>
    """, unsafe_allow_html=True)

def page_benevole(conn, geo):
    """Interface bénévole moderne avec vue limitée"""
    
    if not st.session_state.auth or st.session_state.auth.get("role") != "volunteer":
        render_login_card("benevole", conn)
        return
    
    team_id = st.session_state.auth["team_id"]
    
    # Header d'équipe personnalisé
    st.markdown(f"""
    <div style="
        background: linear-gradient(135deg, #165b33, #c41e3a);
        padding: 1.5rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        text-align: center;
    ">
        <h2 style="color: white; margin: 0;">🎅 Équipe {team_id}</h2>
        <p style="color: #FFD700; margin: 0.5rem 0 0 0;">Bonne collecte!</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Stats de l'équipe
    df_team = db.list_streets(team=team_id)
    if not df_team:  # Liste vide
        st.warning("Aucune rue assignée. Contactez votre superviseur.")
        return
    
    done = len(df_team[df_team['status'] == 'terminee'])
    total = len(df_team)
    progress = (done / total * 100) if total > 0 else 0
    
    # Mini dashboard équipe
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("📍 Vos rues", total)
    with col2:
        st.metric("✅ Complétées", done)
    with col3:
        st.metric("🎯 Progression", f"{progress:.0f}%")
    
    # Système de badges
    show_team_badges(conn, team_id)
    
    # Barre de progression
    st.progress(progress / 100)
    
    # Tabs modernisés
    tab1, tab2, tab3 = st.tabs(["🗺️ Ma carte", "📝 Collecte", "📊 Historique"])
    
    with tab1:
        # CARTE LIMITÉE AUX RUES DE L'ÉQUIPE
        st.markdown("### Vos rues assignées")
        
        # Créer une carte avec SEULEMENT les rues de l'équipe
        m = folium.Map(
            location=[45.7475, -73.6005],
            zoom_start=14,
            tiles='https://{s}.basemaps.cartocdn.com/rastertiles/voyager/{z}/{x}/{y}.png',
            attr='© CARTO'
        )
        
        # Filtrer geo pour n'afficher QUE les rues de l'équipe
        team_streets = df_team['name'].tolist()
        
        for street_name in team_streets:
            if street_name in geo:
                status = df_team[df_team['name'] == street_name]['status'].iloc[0]
                
                # Couleurs selon statut
                colors = {
                    'terminee': '#22c55e',
                    'en_cours': '#f59e0b',
                    'a_faire': '#ef4444'
                }
                color = colors.get(status, '#ef4444')
                
                # Ajouter les segments de cette rue
                for path in geo[street_name]:
                    if path and len(path) >= 2:
                        folium.PolyLine(
                            path,
                            color=color,
                            weight=8,  # Plus épais pour mobile
                            opacity=0.9,
                            tooltip=f"{street_name} - {status.replace('_', ' ').title()}"
                        ).add_to(m)
        
        # Centrer sur les rues de l'équipe
        if team_streets and team_streets[0] in geo:
            first_street = geo[team_streets[0]][0]
            if first_street:
                m.location = first_street[0]
        
        st_folium(m, height=650, width=None, returned_objects=[])
    
    with tab2:
        st.markdown("### 📋 Checklist de collecte")
        
        # Liste interactive des rues
        for row in df_team:
            street = row['name']
            status = row['status']
            notes_count = row.get('notes', 0)
            
            # Carte de rue stylisée
            status_emoji = {'terminee': '✅', 'en_cours': '🚶', 'a_faire': '⭕'}
            status_color = {'terminee': '#22c55e', 'en_cours': '#f59e0b', 'a_faire': '#ef4444'}
            
            with st.expander(f"{status_emoji.get(status, '⭕')} **{street}** ({notes_count} notes)"):
                
                # Changement rapide de statut
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("⭕ À faire", key=f"todo_{street}", width="stretch"):
                        db.set_status(street, 'a_faire')
                        st.rerun()
                with col2:
                    if st.button("🚶 En cours", key=f"progress_{street}", width="stretch"):
                        db.set_status(street, 'en_cours')
                        st.rerun()
                with col3:
                    if st.button("✅ Terminée", key=f"done_{street}", width="stretch"):
                        db.set_status(street, 'terminee')
                        st.rerun()
                
                st.markdown("---")
                
                # Ajout de note rapide
                st.markdown("**Ajouter une note:**")
                with st.form(f"note_{street}", clear_on_submit=True):
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        num = st.text_input("N°", placeholder="123")
                    with col2:
                        note = st.text_input("Note", placeholder="Personne absente")
                    
                    if st.form_submit_button("➕ Ajouter"):
                        if num and note:
                            db.add_note_for_address(street, team_id, num, note)
                            st.success("Note ajoutée!")
                            st.rerun()
                
                # Notes existantes
                notes = db.get_street_addresses_with_notes(street)
                if notes:  # Liste non vide
                    st.markdown("**Notes existantes:**")
                    for n in notes:
                        st.markdown(f"• **{n['address_number']}** : {n['comment']}")
    
    with tab3:
        st.markdown("### 📊 Votre historique")
        try:
            notes = db.get_team_notes(team_id)
            if notes:  # Liste non vide
                st.dataframe(to_dataframe(notes), width="stretch")
            else:
                st.info("Aucune note encore")
        except:
            st.info("Historique non disponible")

def page_benevole_v2(conn, geo):
    """Interface bénévole moderne v4.1 avec vue 'Mes rues'"""
    
    # Vérifier l'authentification
    if not st.session_state.auth or st.session_state.auth.get("role") != "volunteer":
        # Afficher la page de connexion bénévole
        return page_benevole(conn, geo)
    
    # Interface bénévole connecté avec tabs
    st.header("🎅 Espace Bénévole")
    team_id = st.session_state.auth.get("team", "Équipe inconnue")
    st.markdown(f"**Équipe:** {team_id}")
    
    # Tabs pour bénévoles
    tabs = st.tabs([
        "🏘️ Mes rues",
        "🗺️ Carte de terrain", 
        "📝 Journal d'activité"
    ])
    
    with tabs[0]:
        # Nouvelle vue "Mes rues" v4.1
        page_benevole_mes_rues(conn)
    
    with tabs[1]:
        # Carte traditionnelle (réutilise l'ancienne interface)
        page_benevole(conn, geo)
    
    with tabs[2]:
        # Journal d'activité de l'équipe
        st.markdown("### 📝 Journal d'activité de votre équipe")
        try:
            # Afficher les activités récentes de l'équipe
            cursor = conn.execute("""
                SELECT action, details, created_at
                FROM activity_log
                WHERE team_id = ?
                ORDER BY created_at DESC
                LIMIT 20
            """, (team_id,))
            
            activities = cursor.fetchall()
            if activities:
                for activity in activities:
                    action, details, created_at = activity
                    st.markdown(f"**{created_at}** - {action}: {details}")
            else:
                st.info("Aucune activité enregistrée pour votre équipe")
                
        except Exception as e:
            st.info("Journal d'activité temporairement indisponible")
            st.caption(f"Erreur: {e}")

def page_gestionnaire_v2(conn, geo):
    """Interface gestionnaire moderne (ancien superviseur)"""
    st.header("👔 Tableau de Bord Gestionnaire")
    
    # Vérifier l'authentification
    if not st.session_state.auth or st.session_state.auth.get("role") != "supervisor":
        render_login_card("gestionnaire", conn)
        return
    
    # Dashboard moderne
    render_dashboard_gestionnaire(conn, geo)
    
    # Tabs
    tabs = st.tabs([
        "📊 Vue d'ensemble",
        "👥 Équipes",
        "🗺️ Assignation",
        "📥 Export",
        "🛠 Tech"
    ])
    
    with tabs[0]:
        # Carte générale
        st.markdown("### Carte générale")
        df_all = db.list_streets()
        if df_all:  # Liste non vide
            m = create_map(df_all, geo)
            st_folium(m, height=800, width=None, returned_objects=[])
        
        # Activité récente
        st.markdown("### Activité récente")
        try:
            recent = db.recent_activity(limit=10)
            if recent:  # Liste non vide
                st.dataframe(to_dataframe(recent), width="stretch")
            else:
                st.info("Aucune activité récente")
        except:
            st.info("Historique d'activité non disponible")
    
    with tabs[1]:
        # Gestion des équipes
        st.subheader("👥 Gestion des équipes", anchor=False)
        
        # === Formulaire de création d'équipe (robuste) ===
        with st.expander("➕ Créer une nouvelle équipe", expanded=False):
            with st.form("create_team_form", clear_on_submit=True):
                team_id_in = st.text_input(
                    "Identifiant d'équipe", 
                    key="new_team_id", 
                    placeholder="Ex: EQUIPE1",
                    help="Lettres et chiffres uniquement, max 20 caractères"
                )
                team_name_in = st.text_input(
                    "Nom d'équipe", 
                    key="new_team_name", 
                    placeholder="Ex: Équipe Centre",
                    help="Nom descriptif de l'équipe"
                )
                
                # Toggle pour afficher/masquer les mots de passe
                show_pw = st.checkbox("Afficher les mots de passe", value=False)
                pw_type = "default" if show_pw else "password"
                
                pwd_in = st.text_input(
                    "Mot de passe", 
                    type=pw_type, 
                    key="new_team_pwd", 
                    placeholder="Minimum 4 caractères",
                    help="Tout caractère accepté, min 4 / max 128"
                )
                pwd_conf = st.text_input(
                    "Confirmer le mot de passe", 
                    type=pw_type, 
                    key="new_team_pwd_conf", 
                    placeholder="Retapez le mot de passe",
                    help="Doit correspondre au mot de passe ci-dessus"
                )
                
                submitted = st.form_submit_button("✅ Créer l'équipe", width="stretch")

            if submitted:
                # Validation avec validators.py
                ok_id, team_id = validate_and_clean_input("team_id", team_id_in)
                ok_name, team_name = validate_and_clean_input("text", team_name_in)
                ok_pw, password = validate_and_clean_input("password", pwd_in)
                
                if not ok_id:
                    st.error("❌ Identifiant d'équipe invalide (lettres/chiffres, max 20)")
                elif not ok_name:
                    st.error("❌ Nom d'équipe invalide ou vide")
                elif not ok_pw:
                    st.error("❌ Mot de passe invalide (minimum 4 caractères)")
                elif pwd_in != pwd_conf:
                    st.error("❌ Les mots de passe ne correspondent pas")
                else:
                    # Tentative de création avec db.create_team
                    try:
                        created = db.create_team(team_id, team_name, password)
                        if created:
                            st.toast(f"✅ Équipe {team_id} créée avec succès", icon="✅")
                            st.rerun()
                        else:
                            st.error("❌ Échec de création (ID déjà existant ?)")
                    except Exception as e:
                        st.error(f"❌ Erreur lors de la création: {e}")
        
        # === Liste des équipes (sans doublon de titre) ===
        try:
            teams_df = db.get_all_teams()
            if teams_df:  # Liste non vide
                st.dataframe(to_dataframe(teams_df), width="stretch")
            else:
                st.info("Aucune équipe créée")
        except Exception as e:
            st.info("Liste des équipes non disponible")
    
    with tabs[2]:
        # Assignation v4.1
        page_assignations_v41(conn)
    
    with tabs[3]:
        # Export amélioré v4.1
        page_export_gestionnaire_v41(conn)

    with tabs[4]:
        st.markdown("### 🛠 Opérations techniques (protégées)")

        # -- PIN stocké dans secrets (config.toml -> [secrets] TECH_PIN="xxxx")
        try:
            TECH_PIN = st.secrets.get("TECH_PIN", "")
        except:
            TECH_PIN = ""  # Pas de fichier secrets.toml

        if "tech_ok" not in st.session_state:
            st.session_state.tech_ok = False

        if not st.session_state.tech_ok:
            pin = st.text_input("Entrer le PIN technique", type="password")
            if st.button("Déverrouiller"):
                if TECH_PIN and pin == TECH_PIN:
                    st.session_state.tech_ok = True
                    st.success("Accès technique déverrouillé.")
                    st.rerun()
                else:
                    st.error("PIN invalide.")
            st.stop()

        st.info("⚠️ Ces actions sont lourdes et n'affectent pas les statuts/notes. Elles régénèrent les caches OSM.")

        # --- Reconstruire le cache géométrique (lourd)
        with st.expander("🔄 Reconstruire cache OSM (géométries)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirm = st.checkbox("Je comprends les implications")
            with col2:
                safety = st.text_input('Écrire "REBUILD" pour confirmer')

            if st.button("Lancer la reconstruction"):
                if confirm and safety.strip().upper() == "REBUILD":
                    with st.spinner("Construction du cache…"):
                        build_geometry_cache()       # reconstruit le fichier osm_cache.json
                        st.cache_data.clear()        # purge cache Streamlit
                    st.success("✅ Cache OSM mis à jour (géométries).")
                    st.rerun()
                else:
                    st.warning("Confirmation incomplète.")

        # --- Reconstruire/Importer le cache des adresses
        with st.expander("📍 Mettre à jour les adresses (OSM)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirmA = st.checkbox("Je confirme")
            with col2:
                safetyA = st.text_input('Écrire "IMPORT" pour confirmer')

            if st.button("Lancer la mise à jour des adresses"):
                if confirmA and safetyA.strip().upper() == "IMPORT":
                    with st.spinner("Téléchargement des adresses OSM…"):
                        build_addresses_cache()
                        addr_cache = load_addresses_cache()
                        count = db.import_addresses_from_cache(addr_cache)
                    st.success(f"✅ {count} adresses importées depuis OSM.")
                    st.rerun()
                else:
                    st.warning("Confirmation incomplète.")

        # --- Gestion des backups
        with st.expander("💾 Gestion des backups", expanded=False):
            backup_mgr = db.get_backup_manager(DB_PATH)
            
            col1, col2 = st.columns([2, 1])
            with col1:
                if st.button("🔄 Créer un backup manuel", width="stretch"):
                    backup_file = backup_mgr.create_backup("manual")
                    if backup_file:
                        st.success(f"Backup créé : {Path(backup_file).name}")
            
            with col2:
                if st.button("📋 Voir les backups", width="stretch"):
                    backups = backup_mgr.list_backups()
                    if backups:
                        for backup in backups[:5]:  # Montrer les 5 derniers
                            st.text(f"• {backup['name']} ({backup['size']})")
                    else:
                        st.info("Aucun backup disponible")

def page_superviseur(conn, geo):
    """Interface superviseur"""
    st.header("🎯 Tableau de Bord Superviseur")
    
    # Vérifier l'authentification
    if not st.session_state.auth or st.session_state.auth.get("role") != "supervisor":
        render_login_card("superviseur", conn)
        return
    
    # Dashboard moderne
    render_dashboard_gestionnaire(conn, geo)
    
    # Tabs
    tabs = st.tabs([
        "📊 Vue d'ensemble",
        "👥 Équipes",
        "🗺️ Assignation",
        "📥 Export",
        "🛠 Tech"
    ])
    
    with tabs[0]:
        # Carte générale
        st.markdown("### Carte générale")
        df_all = db.list_streets()
        if df_all:  # Liste non vide
            m = create_map(df_all, geo)
            st_folium(m, height=800, width=None, returned_objects=[])
        
        # Activité récente
        st.markdown("### Activité récente")
        recent = db.recent_activity(limit=10)
        if recent:  # Liste non vide
            st.dataframe(to_dataframe(recent), width="stretch")
    
    with tabs[1]:
        # Gestion des équipes
        st.markdown("### Gestion des équipes")
        
        with st.expander("Créer une équipe"):
            with st.form("new_team", clear_on_submit=True):
                new_id = st.text_input("Identifiant")
                new_name = st.text_input("Équipe")
                new_pass = st.text_input("Mot de passe", type="password")
                
                if st.form_submit_button("Créer"):
                    if all([new_id, new_name, new_pass]):
                        if db.create_team(new_id, new_name, new_pass):
                            st.success(f"Équipe {new_id} créée")
                            st.rerun()
        
        # Liste des équipes
        teams_df = db.get_all_teams()
        if teams_df:  # Liste non vide
            st.dataframe(to_dataframe(teams_df), width="stretch")
    
    with tabs[2]:
        # Assignation
        st.markdown("### Assignation des rues")
        
        unassigned = db.get_unassigned_streets()
        
        if unassigned:  # Liste non vide
            with st.form("assign"):
                team = st.selectbox("Équipe", db.teams())
                streets = st.multiselect("Rues", unassigned)
                
                if st.form_submit_button("Assigner"):
                    if team and streets:
                        db.assign_streets_to_team(streets, team)
                        st.success("Rues assignées!")
                        st.rerun()
        else:
            st.success("Toutes les rues sont assignées!")
        
        # Tableau des assignations
        df_all = db.list_streets()
        if df_all:  # Liste non vide
            st.dataframe(
                df_all[['name', 'sector', 'team', 'status']],
                width="stretch"
            )
    
    with tabs[3]:
        # Export
        st.markdown("### Export des données")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.download_button(
                "📥 Export rues (CSV)",
                db.export_to_csv(),
                "rapport_rues.csv",
                "text/csv",
                width="stretch"
            )
        
        with col2:
            st.download_button(
                "📥 Export notes (CSV)",
                db.export_notes_csv(),
                "rapport_notes.csv",
                "text/csv",
                width="stretch"
            )

    with tabs[4]:
        st.markdown("### 🛠 Opérations techniques (protégées)")

        # -- PIN stocké dans secrets (config.toml -> [secrets] TECH_PIN="xxxx")  
        try:
            TECH_PIN = st.secrets.get("TECH_PIN", "")
        except:
            TECH_PIN = ""  # Pas de fichier secrets.toml

        if "tech_ok" not in st.session_state:
            st.session_state.tech_ok = False

        if not st.session_state.tech_ok:
            pin = st.text_input("Entrer le PIN technique", type="password")
            if st.button("Déverrouiller"):
                if TECH_PIN and pin == TECH_PIN:
                    st.session_state.tech_ok = True
                    st.success("Accès technique déverrouillé.")
                    st.rerun()
                else:
                    st.error("PIN invalide.")
            st.stop()

        st.info("⚠️ Ces actions sont lourdes et n'affectent pas les statuts/notes. Elles régénèrent les caches OSM.")

        # --- Reconstruire le cache géométrique (lourd)
        with st.expander("🔄 Reconstruire cache OSM (géométries)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirm = st.checkbox("Je comprends les implications")
            with col2:
                safety = st.text_input('Écrire "REBUILD" pour confirmer')

            if st.button("Lancer la reconstruction"):
                if confirm and safety.strip().upper() == "REBUILD":
                    with st.spinner("Construction du cache…"):
                        build_geometry_cache()       # reconstruit le fichier osm_cache.json
                        st.cache_data.clear()        # purge cache Streamlit
                    st.success("✅ Cache OSM mis à jour (géométries).")
                    st.rerun()
                else:
                    st.warning("Confirmation incomplète.")

        # --- Reconstruire/Importer le cache des adresses
        with st.expander("📍 Mettre à jour les adresses (OSM)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirmA = st.checkbox("Je confirme")
            with col2:
                safetyA = st.text_input('Écrire "IMPORT" pour confirmer')

            if st.button("Lancer la mise à jour des adresses"):
                if confirmA and safetyA.strip().upper() == "IMPORT":
                    with st.spinner("Téléchargement des adresses OSM…"):
                        build_addresses_cache()
                        addr_cache = load_addresses_cache()
                        count = db.import_addresses_from_cache(addr_cache)
                    st.success(f"✅ {count} adresses importées depuis OSM.")
                    st.rerun()
                else:
                    st.warning("Confirmation incomplète.")

# ============================================
# MAIN
# ============================================

# ================================================================================
# NOUVELLES FONCTIONS v4.1 - SUPERVISEUR ET BÉNÉVOLE
# ================================================================================

def page_assignations_v41(conn):
    """Panneau d'assignations v4.1 pour superviseurs"""
    
    try:
        # ===== Bloc Assignations (refactor propre) =====
        st.subheader("🗺️ Assignations par secteur", anchor=False)
        
        # Compteur de rues non assignées (bannière info)
        unassigned_count = db.get_unassigned_streets_count()
        if unassigned_count > 0:
            st.info(f"⚠️ {unassigned_count} rue(s) non assignée(s)")
        
        with st.container():
            c1, c2, c3 = st.columns([1, 1.2, 0.7], vertical_alignment="bottom")
            
            with c1:
                # Récupérer la liste des secteurs
                liste_secteurs = db.get_sectors_list()
                secteur = st.selectbox(
                    "SECTEUR À ASSIGNER",
                    options=[""] + (liste_secteurs if liste_secteurs else []),
                    index=0,
                    key="assign_sector",
                    help="Choisissez le secteur à assigner",
                    label_visibility="visible",
                )
            
            with c2:
                # Récupérer la liste des équipes
                teams = db.get_teams_list()
                liste_equipes = [f"{team[1]} ({team[0]})" for team in teams] if teams else []
                
                if liste_equipes:
                    team_display = st.selectbox(
                        "ÉQUIPE", 
                        options=[""] + liste_equipes, 
                        index=0, 
                        key="assign_team"
                    )
                    # Extraire l'ID de l'équipe
                    team = ""
                    if team_display and team_display != "":
                        team = team_display.split("(")[-1].rstrip(")")
                else:
                    st.info("Aucune équipe disponible")
                    team = None
            
            with c3:
                disabled = not (secteur and team)
                if st.button("🎯 Assigner tout le secteur", width="stretch", disabled=disabled):
                    # Appel métier : assigner toutes les rues non assignées du secteur à l'équipe
                    if secteur and team:
                        try:
                            nb = db.bulk_assign_sector(secteur, team)
                            if nb > 0:
                                st.toast(f"✅ {nb} rue(s) assignée(s) à l'équipe {team}", icon="✅")
                                st.rerun()
                            else:
                                st.toast("ℹ️ Aucune rue non assignée dans ce secteur", icon="ℹ️")
                        except Exception as e:
                            st.error(f"Erreur lors de l'assignation: {e}")
        
        # ===== Tableau d'état (uniforme, sans style spécial) =====
        st.markdown("### 📋 État des assignations")
        
        df = db.list_streets()
        if df:  # Liste non vide
            df_disp = df.assign(
                Statut=df["status"].map(STATUS_TO_LABEL).fillna("À faire")
            ).rename(columns={
                "name": "Rue", 
                "sector": "Secteur", 
                "team": "Équipe"
            })[["Rue", "Secteur", "Équipe", "Statut"]]
            
            st.dataframe(df_disp, width="stretch")  # aucun Styler, aucun CSS cellule
        else:
            st.info("Aucune rue trouvée")
            
    except Exception as e:
        st.error(f"Erreur dans le panneau d'assignations: {e}")
        st.info("Fonctionnalité temporairement indisponible")

def page_export_gestionnaire_v41(conn):
    """Page d'export v4.1 avec nouvelles fonctionnalités"""
    st.markdown("### 📥 Export des données")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Export CSV standard
        try:
            st.download_button(
                "📥 Export CSV Standard",
                db.export_to_csv(),
                "rapport_rues.csv",
                "text/csv",
                width="stretch"
            )
        except Exception as e:
            st.button("📥 CSV (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    with col2:
        # Export Excel professionnel
        try:
            from reports import ReportGenerator
            generator = ReportGenerator(conn)
            excel_data = generator.generate_excel()
            st.download_button(
                "📊 Export Excel Pro",
                excel_data,
                "guignolee_2025_rapport.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                width="stretch"
            )
        except ImportError:
            st.button("📊 Excel (Installer xlsxwriter)", disabled=True, width="stretch")
        except Exception as e:
            st.button("📊 Excel (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    with col3:
        # Export PDF professionnel
        try:
            from reports import ReportGenerator
            generator = ReportGenerator(conn)
            pdf_data = generator.generate_pdf()
            st.download_button(
                "📄 Export PDF Pro",
                pdf_data,
                "guignolee_2025_rapport.pdf",
                "application/pdf",
                width="stretch"
            )
        except ImportError:
            st.button("📄 PDF (Installer reportlab)", disabled=True, width="stretch")
        except Exception as e:
            st.button("📄 PDF (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    # Export CSV assignations (nouveau v4.1)
    st.markdown("---")
    st.markdown("### 📋 Export spécialisés v4.1")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Export CSV assignations
        try:
            assignations_data = db.get_assignations_export_data()
            if assignations_data:  # Liste non vide
                csv_data = pd.DataFrame(assignations_data).to_csv(index=False, encoding='utf-8')
                st.download_button(
                    "📋 Export CSV Assignations",
                    csv_data,
                    "assignations_secteurs.csv",
                    "text/csv",
                    width="stretch",
                    help="Colonnes: secteur, rue, équipe, statut"
                )
            else:
                st.button("📋 Assignations (Aucune donnée)", disabled=True, width="stretch")
        except Exception as e:
            st.button("📋 Assignations (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    with col2:
        # Export notes
        try:
            st.download_button(
                "📝 Export Notes",
                db.export_notes_csv(),
                "rapport_notes.csv",
                "text/csv",
                width="stretch"
            )
        except Exception as e:
            st.button("📝 Notes (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")

def page_benevole_mes_rues(conn):
    """Vue 'Mes rues' pour bénévoles v4.1"""
    
    # Récupérer l'équipe du bénévole connecté
    if not st.session_state.auth or st.session_state.auth.get("role") != "volunteer":
        st.warning("Accès réservé aux bénévoles connectés")
        return
    
    team_id = st.session_state.auth.get("team")
    if not team_id:
        st.error("Équipe non identifiée")
        return
    
    st.markdown(f"### 🏘️ Mes rues assignées - Équipe {team_id}")
    
    try:
        # Récupérer les rues de l'équipe
        team_streets = db.get_team_streets(team_id)
        
        if not team_streets:  # Liste vide
            st.info("Aucune rue assignée à votre équipe pour le moment.")
            return
        
        # Afficher les statistiques de l'équipe
        total_streets = len(team_streets)
        done_streets = len([s for s in team_streets if hasattr(s, 'status') and s.status == 'terminee'])
        in_progress = len([s for s in team_streets if hasattr(s, 'status') and s.status == 'en_cours'])
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total rues", total_streets)
        with col2:
            st.metric("Terminées", done_streets)
        with col3:
            st.metric("En cours", in_progress)
        with col4:
            progress = (done_streets / total_streets * 100) if total_streets > 0 else 0
            st.metric("Progression", f"{progress:.1f}%")
        
        st.markdown("---")
        
        # Affichage par rue avec actions
        for street in team_streets:
            if isinstance(street, str):
                street_name = street
            else:
                street_name = street.get("name", street)
            street_name = street['street_name']
            current_status = street['status']
            notes_count = street['notes_count']
            
            with st.expander(f"🏘️ {street_name} ({street['sector']}) - {current_status.replace('_', ' ').title()}", 
                           expanded=current_status == 'en_cours'):
                
                col1, col2, col3 = st.columns([2, 1, 1])
                
                with col1:
                    st.markdown(f"**Secteur:** {street['sector']}")
                    st.markdown(f"**Statut actuel:** {current_status.replace('_', ' ').title()}")
                    if notes_count > 0:
                        st.markdown(f"**Notes existantes:** {notes_count}")
                
                with col2:
                    # Bouton "En cours"
                    if st.button(
                        "🚀 En cours", 
                        key=f"progress_{street_name}",
                        disabled=current_status == 'en_cours',
                        width="stretch"
                    ):
                        if db.update_street_status(street_name, 'en_cours', team_id):
                            st.toast(f"✅ {street_name} marquée en cours", icon="🚀")
                            st.rerun()
                        else:
                            st.error("Erreur lors de la mise à jour")
                
                with col3:
                    # Bouton "Terminée"
                    if st.button(
                        "✅ Terminée", 
                        key=f"done_{street_name}",
                        disabled=current_status == 'terminee',
                        width="stretch"
                    ):
                        if db.update_street_status(street_name, 'terminee', team_id):
                            st.toast(f"🎉 {street_name} terminée!", icon="🎉")
                            st.rerun()
                        else:
                            st.error("Erreur lors de la mise à jour")
                
                # Section notes
                st.markdown("**Gestion des notes:**")
                
                # Afficher les notes existantes
                existing_notes = db.get_street_notes_for_team(street_name, team_id)
                if existing_notes:
                    st.markdown("*Notes existantes:*")
                    for note in existing_notes:
                        st.markdown(f"• **#{list(note.values())[0] if isinstance(note, dict) else note[0]}** : {list(note.values())[1] if isinstance(note, dict) else note[1]} _{list(note.values())[2] if isinstance(note, dict) else note[2]}_")
                
                # Ajouter une nouvelle note
                with st.form(f"note_form_{street_name}"):
                    col_addr, col_note = st.columns([1, 3])
                    with col_addr:
                        address_number = st.text_input(
                            "N° civique", 
                            key=f"addr_{street_name}",
                            placeholder="123A"
                        )
                    with col_note:
                        comment = st.text_area(
                            "Commentaire", 
                            key=f"comment_{street_name}",
                            placeholder="Ex: Absent, refus, don reçu...",
                            max_chars=500,
                            height=80
                        )
                    
                    if st.form_submit_button("💾 Enregistrer note"):
                        if address_number and comment:
                            if db.add_street_note(street_name, team_id, address_number, comment):
                                st.toast(f"📝 Note ajoutée pour {street_name} #{address_number}", icon="📝")
                                st.rerun()
                            else:
                                st.error("Erreur lors de l'enregistrement de la note")
                        else:
                            st.warning("Veuillez remplir le numéro et le commentaire")
                            
    except Exception as e:
        st.error(f"Erreur lors du chargement de vos rues: {e}")
        st.info("Fonctionnalité temporairement indisponible")

def main():
    """Point d'entrée principal - Version 2.0 Guignolée"""
    
    # CSS moderne
    inject_css()
    
    # Connexion DB
    # Initialisation de la base de données
    db.init_db()
    
    # Compatibilité legacy pour les backups
    if 'conn' not in st.session_state:
        import sqlite3
        conn = sqlite3.connect(DB_PATH, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        st.session_state['conn'] = conn
    st.session_state['conn'] = conn
    
    # Cache géométrique
    @st.cache_data(ttl=None)
    def get_geo(_sig):
        data = load_geometry_cache()
        return data if data else {}
    
    sig = int(CACHE_FILE.stat().st_mtime_ns) if CACHE_FILE.exists() else 0
    geo = get_geo(sig)
    
    # Header festif
    render_header()
    
    # Navigation modernisée dans la sidebar
    with st.sidebar:
        # CSS pour la sidebar sans position absolue
        st.markdown("""
        <style>
        .css-1d391kg { padding-top: 1rem !important; }
        .stSidebar > div:first-child { padding-top: 1rem !important; }
        </style>
        """, unsafe_allow_html=True)
        
        # Logo en haut de la sidebar (position normale)
        logo_path = ASSETS / "logo.png"
        if logo_path.exists():
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                st.image(str(logo_path), width=150)
            st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
        else:
            # Placeholder centré
            st.markdown("""
            <div style="
                background: linear-gradient(135deg, #c41e3a, #165b33);
                border-radius: 15px;
                padding: 2rem;
                color: white;
                text-align: center;
                margin: 1rem 0;
                box-shadow: 0 4px 15px rgba(0,0,0,0.3);
            ">
                <div style="font-size: 2.5rem;">🎁</div>
                <div style="font-weight: bold; font-size: 1.2rem;">LOGO</div>
                <small>Espace réservé</small>
            </div>
            """, unsafe_allow_html=True)
            st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
        
        # Navigation
        st.markdown("### 🎄 Navigation")
        
        # Boutons de navigation stylisés
        if st.button("🏠 Accueil", width="stretch"):
            st.session_state.page = "accueil"
            st.rerun()
        
        if st.button("🎅 Bénévole", width="stretch"):
            st.session_state.page = "benevole"
            st.rerun()
            
        if st.button("👔 Gestionnaire", width="stretch"):
            st.session_state.page = "gestionnaire"  
            st.rerun()
        
        # Déconnexion si connecté
        if st.session_state.auth:
            st.markdown("---")
            if st.button("🚪 Déconnexion", width="stretch"):
                st.session_state.auth = None
                st.rerun()
        
        # Compteur temps réel
        st.markdown("---")
        stats = db.extended_stats()
        st.markdown(f"""
        <div style="text-align: center;">
            <h4>État de la collecte</h4>
            <div style="font-size: 2rem; color: #FFD700;">
                {stats['done']}/{stats['total']}
            </div>
            <small>Rues complétées</small>
        </div>
        """, unsafe_allow_html=True)
    
    # Routing pages
    page = st.session_state.get('page', 'accueil')
    
    if page == "accueil":
        page_accueil_v2(conn, geo)
    elif page == "benevole":
        page_benevole_v2(conn, geo)
    elif page == "gestionnaire":
        page_gestionnaire_v2(conn, geo)
    
    # Footer festif
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem;
        margin-top: 3rem;
        border-top: 2px solid rgba(255,215,0,0.3);
        color: #8b92a4;
    ">
        <p>
            🎄 Guignolée 2025 - Le Relais de Mascouche 🎄<br>
            <small>Ensemble, redonnons espoir | 📞 450-474-4133</small>
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Bannière en bas de page
    if (ASSETS / "banner.png").exists():
        st.image(str(ASSETS / "banner.png"), width="stretch")

if __name__ == "__main__":
    main()

```
---8<--- guignomap/app.py END ---

---8<--- guignomap/backup.py BEGIN ---
```py
"""
Système de backup automatique pour GuignoMap
Sauvegarde la base de données et les caches
"""

import shutil
import sqlite3
from pathlib import Path
from datetime import datetime
import json
import zipfile

class BackupManager:
    def __init__(self, db_path):
        self.db_path = Path(db_path)
        self.backup_dir = self.db_path.parent / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        self.max_backups = 7  # Garder 7 jours de backups
        
    def create_backup(self, reason="manual"):
        """Crée un backup complet avec timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{timestamp}_{reason}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)
        
        try:
            # Backup de la base de données
            db_backup = backup_path / "guigno_map.db"
            shutil.copy2(self.db_path, db_backup)
            
            # Backup des caches OSM
            for cache_file in ["osm_cache.json", "osm_addresses.json"]:
                cache_path = self.db_path.parent / cache_file
                if cache_path.exists():
                    shutil.copy2(cache_path, backup_path / cache_file)
            
            # Créer un ZIP
            zip_path = self.backup_dir / f"{backup_name}.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file in backup_path.iterdir():
                    zipf.write(file, file.name)
            
            # Nettoyer le dossier temporaire
            shutil.rmtree(backup_path)
            
            # Nettoyer les vieux backups
            self._cleanup_old_backups()
            
            # Log le backup
            self._log_backup(timestamp, reason)
            
            print(f"✅ Backup créé : {zip_path.name}")
            return str(zip_path)
            
        except Exception as e:
            print(f"❌ Erreur backup : {e}")
            if backup_path.exists():
                shutil.rmtree(backup_path)
            return None
    
    def restore_backup(self, backup_file):
        """Restaure un backup spécifique"""
        backup_path = self.backup_dir / backup_file
        if not backup_path.exists():
            print(f"❌ Backup introuvable : {backup_file}")
            return False
            
        try:
            # Créer un backup de sécurité avant restauration
            self.create_backup("pre_restore")
            
            # Extraire le ZIP
            temp_dir = self.backup_dir / "temp_restore"
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                zipf.extractall(temp_dir)
            
            # Restaurer les fichiers
            for file in temp_dir.iterdir():
                target = self.db_path.parent / file.name
                shutil.copy2(file, target)
            
            # Nettoyer
            shutil.rmtree(temp_dir)
            
            print(f"✅ Backup restauré : {backup_file}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur restauration : {e}")
            return False
    
    def list_backups(self):
        """Liste tous les backups disponibles"""
        backups = []
        for file in self.backup_dir.glob("backup_*.zip"):
            stat = file.stat()
            backups.append({
                "name": file.name,
                "size": f"{stat.st_size / 1024 / 1024:.2f} MB",
                "date": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            })
        return sorted(backups, key=lambda x: x["date"], reverse=True)
    
    def _cleanup_old_backups(self):
        """Supprime les backups de plus de 7 jours"""
        backups = sorted(self.backup_dir.glob("backup_*.zip"), key=lambda x: x.stat().st_mtime)
        while len(backups) > self.max_backups:
            oldest = backups.pop(0)
            oldest.unlink()
            print(f"🗑️ Ancien backup supprimé : {oldest.name}")
    
    def _log_backup(self, timestamp, reason):
        """Log les backups dans un fichier"""
        log_file = self.backup_dir / "backup_log.json"
        log = []
        if log_file.exists():
            with open(log_file, 'r') as f:
                log = json.load(f)
        
        log.append({
            "timestamp": timestamp,
            "reason": reason,
            "date": datetime.now().isoformat()
        })
        
        # Garder seulement les 100 derniers logs
        log = log[-100:]
        
        with open(log_file, 'w') as f:
            json.dump(log, f, indent=2)

def auto_backup_before_critical(func):
    """Décorateur pour backup automatique avant opérations critiques"""
    def wrapper(*args, **kwargs):
        # Trouver la connexion DB dans les arguments
        conn = None
        for arg in args:
            if hasattr(arg, 'execute'):  # C'est une connexion SQLite
                conn = arg
                break
        
        if conn:
            try:
                # Créer un backup avant l'opération
                db_path = Path(__file__).parent / "guigno_map.db"
                backup_mgr = BackupManager(db_path)
                backup_mgr.create_backup(f"auto_{func.__name__}")
            except:
                pass  # Ne pas bloquer l'opération si le backup échoue
        
        return func(*args, **kwargs)
    return wrapper
```
---8<--- guignomap/backup.py END ---

---8<--- guignomap/db.py BEGIN ---
```py
import sqlite3
import pandas as pd
import hashlib
import bcrypt
from backup import auto_backup_before_critical, BackupManager
from validators import validate_and_clean_input, InputValidator
from datetime import datetime
import json
from pathlib import Path
import os
import secrets
import string

# Schéma amélioré de la base de données
SCHEMA = """
-- Table des rues
CREATE TABLE IF NOT EXISTS streets (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    sector TEXT,
    team TEXT,
    status TEXT NOT NULL DEFAULT 'a_faire' 
        CHECK (status IN ('a_faire', 'en_cours', 'terminee'))
);

-- Table des équipes
CREATE TABLE IF NOT EXISTS teams (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    password_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    active BOOLEAN DEFAULT 1
);

-- Table des notes/commentaires PAR ADRESSE
CREATE TABLE IF NOT EXISTS notes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    street_name TEXT NOT NULL,
    team_id TEXT NOT NULL,
    address_number TEXT,
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (street_name) REFERENCES streets(name),
    FOREIGN KEY (team_id) REFERENCES teams(id)
);

-- Table d'activité (log)
CREATE TABLE IF NOT EXISTS activity_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    team_id TEXT,
    action TEXT NOT NULL,
    details TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Table des adresses OSM
CREATE TABLE IF NOT EXISTS addresses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    street_name TEXT NOT NULL,
    house_number TEXT NOT NULL,
    latitude REAL,
    longitude REAL,
    osm_type TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (street_name) REFERENCES streets(name)
);

-- Index pour améliorer les performances
CREATE INDEX IF NOT EXISTS idx_streets_team ON streets(team);
CREATE INDEX IF NOT EXISTS idx_streets_status ON streets(status);
CREATE INDEX IF NOT EXISTS idx_notes_street ON notes(street_name);
CREATE INDEX IF NOT EXISTS idx_activity_created ON activity_log(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_addresses_street ON addresses(street_name);
CREATE INDEX IF NOT EXISTS idx_addresses_number ON addresses(house_number);
"""

def get_conn(db_path):
    """Crée une connexion à la base de données"""
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn

def init_db(conn):
    """Initialise la base de données avec le schéma et les données initiales"""
    try:
        # Créer les tables si elles n'existent pas
        conn.executescript(SCHEMA)
        conn.commit()
        
        # Créer un compte admin par défaut s'il n'existe pas
        cursor = conn.execute("SELECT COUNT(*) FROM teams WHERE id = 'ADMIN'")
        if cursor.fetchone()[0] == 0:
            pwd = os.getenv("GM_ADMIN_PWD", "RELAIS2025")  # Par défaut RELAIS2025
            create_team(conn, 'ADMIN', 'Superviseur', pwd)
        
        # AUTO-IMPORT : Si aucune rue n'existe, importer automatiquement depuis OSM
        cursor = conn.execute("SELECT COUNT(*) FROM streets")
        if cursor.fetchone()[0] == 0:
            print("🔄 Aucune rue trouvée. Import automatique depuis OpenStreetMap...")
            auto_import_streets(conn)
            
    except Exception as e:
        print(f"Erreur lors de l'initialisation de la DB: {e}")
        raise

@auto_backup_before_critical
def auto_import_streets(conn):
    """Import automatique des rues de Mascouche"""
    try:
        # Essayer d'abord avec OSM
        from osm import generate_streets_csv
        csv_data = generate_streets_csv("Mascouche")
        
        if csv_data:
            import io
            df = pd.read_csv(io.StringIO(csv_data.decode('utf-8')))
            
            if not df.empty:
                for _, row in df.iterrows():
                    conn.execute(
                        "INSERT OR IGNORE INTO streets(name, sector, team, status) VALUES (?, ?, ?, 'a_faire')",
                        (row.get("name", ""), row.get("sector", ""), row.get("team", ""))
                    )
                conn.commit()
                print(f"✅ {len(df)} rues importées automatiquement")
                log_activity(conn, None, "AUTO_IMPORT", f"Import automatique de {len(df)} rues")
                return
    except Exception as e:
        print(f"⚠️ Erreur lors de l'import OSM: {e}")
    
    # Fallback : Données de test si OSM échoue
    print("📦 Import de données de test...")
    test_streets = [
        ("Montée Masson", "Centre", ""),
        ("Chemin Sainte-Marie", "Centre", ""),
        ("Boulevard de Mascouche", "Centre", ""),
        ("Rue Dupras", "Centre", ""),
        ("Rue Saint-Pierre", "Centre", ""),
        ("Rue de l'Église", "Centre", ""),
        ("Avenue des Érables", "Nord", ""),
        ("Rue des Pins", "Nord", ""),
        ("Rue Gravel", "Sud", ""),
        ("Rue Forget", "Sud", ""),
    ]
    
    for name, sector, team in test_streets:
        conn.execute(
            "INSERT OR IGNORE INTO streets(name, sector, team, status) VALUES (?, ?, ?, 'a_faire')",
            (name, sector, team)
        )
    conn.commit()
    print(f"✅ {len(test_streets)} rues de test importées")

# ---------- Fonctions pour les équipes ----------
def hash_password(password):
    """Hash un mot de passe avec bcrypt et salt automatique"""
    salt = bcrypt.gensalt()
    hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
    return hashed.decode('utf-8')

def create_team(conn, team_id, name, password):
    """Crée une nouvelle équipe avec validation"""
    try:
        # Valider les entrées
        valid_id, clean_id = validate_and_clean_input("team_id", team_id)
        valid_name, clean_name = validate_and_clean_input("text", name)
        valid_pwd, _ = validate_and_clean_input("password", password)
        
        if not valid_id or not valid_name or not valid_pwd:
            return False
        
        conn.execute(
            "INSERT INTO teams (id, name, password_hash) VALUES (?, ?, ?)",
            (clean_id, clean_name, hash_password(password))
        )
        conn.commit()
        log_activity(conn, clean_id, "TEAM_CREATED", f"Équipe {clean_name} créée")
        return True
    except sqlite3.IntegrityError:
        return False

def verify_team(conn, team_id, password):
    """Vérifie les identifiants d'une équipe avec bcrypt"""
    cursor = conn.execute(
        "SELECT password_hash FROM teams WHERE id = ? AND active = 1",
        (team_id,)
    )
    row = cursor.fetchone()
    if row:
        try:
            # Support ancien SHA256 pour migration
            stored_hash = row[0]
            if stored_hash.startswith('$2b$') or stored_hash.startswith('$2a$'):
                # Hash bcrypt
                return bcrypt.checkpw(password.encode('utf-8'), stored_hash.encode('utf-8'))
            else:
                # Ancien SHA256, vérifier et migrer
                if stored_hash == hashlib.sha256(password.encode()).hexdigest():
                    # Migrer vers bcrypt
                    new_hash = hash_password(password)
                    conn.execute("UPDATE teams SET password_hash = ? WHERE id = ?", (new_hash, team_id))
                    conn.commit()
                    return True
                return False
        except Exception as e:
            print(f"Erreur vérification mot de passe: {e}")
            return False
    return False

def migrate_all_passwords_to_bcrypt(conn):
    """Migration manuelle des mots de passe SHA256 vers bcrypt"""
    print("⚠️ Migration des mots de passe requise")
    print("Entrez les mots de passe actuels pour migration:")
    
    cursor = conn.execute("SELECT id, name FROM teams WHERE active = 1")
    teams = cursor.fetchall()
    
    for team_id, team_name in teams:
        if team_id == 'ADMIN':
            pwd = input(f"Mot de passe actuel pour {team_name} (ADMIN): ")
            if pwd:
                new_hash = hash_password(pwd)
                conn.execute("UPDATE teams SET password_hash = ? WHERE id = ?", (new_hash, team_id))
        
    conn.commit()
    print("✅ Migration terminée")

def get_all_teams(conn):
    """Récupère toutes les équipes avec leurs statistiques"""
    query = """
    SELECT 
        t.id,
        t.name,
        t.created_at,
        COUNT(DISTINCT s.name) as streets_count,
        SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as done_count,
        CASE 
            WHEN COUNT(s.name) > 0 
            THEN (SUM(CASE WHEN s.status = 'terminee' THEN 1.0 ELSE 0 END) / COUNT(s.name)) * 100
            ELSE 0 
        END as progress
    FROM teams t
    LEFT JOIN streets s ON t.id = s.team
    WHERE t.active = 1 AND t.id != 'ADMIN'
    GROUP BY t.id, t.name, t.created_at
    ORDER BY t.id
    """
    return pd.read_sql_query(query, conn)

@auto_backup_before_critical
def delete_team(conn, team_id):
    """Désactive une équipe"""
    conn.execute("UPDATE teams SET active = 0 WHERE id = ?", (team_id,))
    conn.execute("UPDATE streets SET team = NULL WHERE team = ?", (team_id,))
    conn.commit()
    log_activity(conn, None, "TEAM_DELETED", f"Équipe {team_id} supprimée")

def teams(conn):
    """Liste des IDs d'équipes actives"""
    cursor = conn.execute(
        "SELECT id FROM teams WHERE active = 1 AND id != 'ADMIN' ORDER BY id"
    )
    return [row[0] for row in cursor.fetchall()]

# ---------- Fonctions pour les rues ----------
def list_streets(conn, team=None):
    """Liste les rues, optionnellement filtrées par équipe"""
    try:
        if team:
            query = """
                SELECT 
                    s.name, 
                    COALESCE(s.sector, '') as sector, 
                    COALESCE(s.team, '') as team, 
                    COALESCE(s.status, 'a_faire') as status,
                    COUNT(n.id) as notes,
                    COUNT(DISTINCT n.address_number) as addresses_with_notes
                FROM streets s
                LEFT JOIN notes n ON s.name = n.street_name
                WHERE s.team = ?
                GROUP BY s.name, s.sector, s.team, s.status
                ORDER BY 
                    CASE s.status 
                        WHEN 'a_faire' THEN 1 
                        WHEN 'en_cours' THEN 2 
                        WHEN 'terminee' THEN 3 
                    END, 
                    s.name
            """
            df = pd.read_sql_query(query, conn, params=(team,))
        else:
            query = """
                SELECT 
                    s.name, 
                    COALESCE(s.sector, '') as sector, 
                    COALESCE(s.team, '') as team, 
                    COALESCE(s.status, 'a_faire') as status,
                    COUNT(n.id) as notes,
                    COUNT(DISTINCT n.address_number) as addresses_with_notes
                FROM streets s
                LEFT JOIN notes n ON s.name = n.street_name
                GROUP BY s.name, s.sector, s.team, s.status
                ORDER BY 
                    s.team, 
                    CASE s.status 
                        WHEN 'a_faire' THEN 1 
                        WHEN 'en_cours' THEN 2 
                        WHEN 'terminee' THEN 3 
                    END, 
                    s.name
            """
            df = pd.read_sql_query(query, conn)
        
        # S'assurer que toutes les colonnes existent
        for col in ['name', 'sector', 'team', 'status', 'notes', 'addresses_with_notes']:
            if col not in df.columns:
                df[col] = '' if col in ['sector', 'team'] else ('a_faire' if col == 'status' else 0)
        
        return df
        
    except Exception as e:
        print(f"Erreur list_streets: {e}")
        # Retourner un DataFrame vide avec la structure attendue
        return pd.DataFrame(columns=['name', 'sector', 'team', 'status', 'notes', 'addresses_with_notes'])

def get_unassigned_streets(conn):
    """Récupère les rues non assignées"""
    query = """
        SELECT name, sector 
        FROM streets 
        WHERE team IS NULL OR team = ''
        ORDER BY sector, name
    """
    return pd.read_sql_query(query, conn)

def assign_streets_to_team(conn, street_names, team_id):
    """Assigne plusieurs rues à une équipe en une transaction"""
    try:
        for street_name in street_names:
            conn.execute(
                "UPDATE streets SET team = ? WHERE name = ?",
                (team_id, street_name)
            )
        conn.commit()
        log_activity(conn, team_id, "STREETS_ASSIGNED", f"{len(street_names)} rues assignées")
        return True
    except Exception as e:
        conn.rollback()
        print(f"Erreur lors de l'assignation: {e}")
        return False

def set_status(conn, name, status):
    """Met à jour le statut d'une rue avec validation"""
    valid_name, clean_name = validate_and_clean_input("street_name", name)
    clean_status = InputValidator.validate_status(status)
    
    if not valid_name:
        print("❌ Nom de rue invalide")
        return False
    
    conn.execute(
        "UPDATE streets SET status = ? WHERE name = ?",
        (clean_status, clean_name)
    )
    conn.commit()
    
    cursor = conn.execute("SELECT team FROM streets WHERE name = ?", (clean_name,))
    row = cursor.fetchone()
    if row:
        log_activity(conn, row[0], f"STATUS_{clean_status.upper()}", f"Rue {clean_name}")
    return True

# ---------- Fonctions pour les notes PAR ADRESSE ----------
def add_note_for_address(conn, street_name, team_id, address_number, comment):
    """Ajoute une note pour une adresse spécifique avec validation"""
    # Valider toutes les entrées
    valid_street, clean_street = validate_and_clean_input("street_name", street_name)
    valid_team, clean_team = validate_and_clean_input("team_id", team_id)
    valid_addr, clean_addr = validate_and_clean_input("address", address_number)
    valid_note, clean_note = validate_and_clean_input("note", comment)
    
    if not all([valid_street, valid_team, valid_addr, valid_note]):
        print("❌ Données invalides pour la note")
        return False
    
    conn.execute(
        """INSERT INTO notes (street_name, team_id, address_number, comment) 
           VALUES (?, ?, ?, ?)""",
        (clean_street, clean_team, clean_addr, clean_note)
    )
    
    # Met automatiquement le statut à "en_cours" si c'était "a_faire"
    conn.execute(
        """UPDATE streets 
           SET status = CASE 
               WHEN status = 'a_faire' THEN 'en_cours' 
               ELSE status 
           END
           WHERE name = ?""",
        (clean_street,)
    )
    
    conn.commit()
    log_activity(conn, clean_team, "NOTE_ADDED", f"Note ajoutée pour {clean_addr} {clean_street}")
    return True

def get_street_addresses_with_notes(conn, street_name):
    """Récupère toutes les adresses avec notes pour une rue"""
    query = """
        SELECT 
            n.address_number,
            n.comment,
            n.created_at,
            t.name as team_name
        FROM notes n
        JOIN teams t ON n.team_id = t.id
        WHERE n.street_name = ?
        ORDER BY 
            CAST(n.address_number AS INTEGER),
            n.created_at DESC
    """
    return pd.read_sql_query(query, conn, params=(street_name,))

def get_team_notes(conn, team_id):
    """Récupère toutes les notes d'une équipe"""
    query = """
        SELECT 
            street_name, 
            address_number, 
            comment, 
            created_at
        FROM notes
        WHERE team_id = ?
        ORDER BY created_at DESC
        LIMIT 50
    """
    return pd.read_sql_query(query, conn, params=(team_id,))

# ---------- Fonctions de statistiques ----------
def extended_stats(conn):
    """Statistiques étendues avec détails par adresse"""
    cursor = conn.execute("""
        SELECT 
            COUNT(DISTINCT s.name) as total,
            SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as done,
            SUM(CASE WHEN s.status = 'en_cours' THEN 1 ELSE 0 END) as partial,
            SUM(CASE WHEN s.status = 'a_faire' THEN 1 ELSE 0 END) as todo,
            COUNT(DISTINCT n.id) as total_notes,
            COUNT(DISTINCT n.address_number || n.street_name) as addresses_with_notes
        FROM streets s
        LEFT JOIN notes n ON s.name = n.street_name
    """)
    row = cursor.fetchone()
    return {
        "total": row[0] or 0,
        "done": row[1] or 0,
        "partial": row[2] or 0,
        "todo": row[3] or 0,
        "total_notes": row[4] or 0,
        "addresses_with_notes": row[5] or 0
    }

def stats_by_team(conn):
    """Statistiques par équipe"""
    query = """
        SELECT 
            s.team,
            COUNT(DISTINCT s.name) as total,
            SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as done,
            SUM(CASE WHEN s.status = 'en_cours' THEN 1 ELSE 0 END) as partial,
            COUNT(DISTINCT n.id) as notes,
            ROUND(
                (SUM(CASE WHEN s.status = 'terminee' THEN 1.0 ELSE 0 END) / COUNT(*)) * 100, 
                1
            ) as progress
        FROM streets s
        LEFT JOIN notes n ON s.name = n.street_name AND n.team_id = s.team
        WHERE s.team IS NOT NULL AND s.team != ''
        GROUP BY s.team
        ORDER BY progress DESC
    """
    return pd.read_sql_query(query, conn)

# ---------- Fonctions d'activité ----------
def recent_activity(conn, limit=10):
    """Récupère l'activité récente"""
    query = """
        SELECT 
            datetime(created_at, 'localtime') as timestamp,
            COALESCE(team_id, 'SYSTEM') as team,
            action,
            details
        FROM activity_log
        ORDER BY created_at DESC
        LIMIT ?
    """
    return pd.read_sql_query(query, conn, params=(limit,))

# ---------- Fonctions d'export ----------
def export_to_csv(conn):
    """Exporte toutes les données en CSV"""
    query = """
        SELECT 
            s.name as rue,
            s.sector as secteur,
            s.team as equipe,
            s.status as statut,
            COUNT(DISTINCT n.id) as nombre_notes,
            COUNT(DISTINCT n.address_number) as adresses_avec_notes
        FROM streets s
        LEFT JOIN notes n ON s.name = n.street_name
        GROUP BY s.name, s.sector, s.team, s.status
        ORDER BY s.team, s.name
    """
    df = pd.read_sql_query(query, conn)
    return df.to_csv(index=False).encode('utf-8')

def export_notes_csv(conn):
    """Exporte toutes les notes en CSV avec adresses"""
    query = """
        SELECT 
            n.street_name as rue,
            n.address_number as numero,
            n.team_id as equipe,
            n.comment as commentaire,
            n.created_at as date_creation
        FROM notes n
        ORDER BY n.street_name, CAST(n.address_number AS INTEGER), n.created_at DESC
    """
    df = pd.read_sql_query(query, conn)
    return df.to_csv(index=False).encode('utf-8')

# ========================================
# NOUVELLES FONCTIONS POUR LES ADRESSES
# ========================================

@auto_backup_before_critical
def import_addresses_from_cache(conn, cache):
    """
    Importe les adresses depuis le cache OSM vers la base de données
    """
    try:
        # Vider la table existante
        conn.execute("DELETE FROM addresses")
        
        imported_count = 0
        skipped_count = 0
        
        for street_name, addresses in cache.items():
            # Vérifier que la rue existe dans la DB
            cursor = conn.execute("SELECT COUNT(*) FROM streets WHERE name = ?", (street_name,))
            if cursor.fetchone()[0] == 0:
                # Si la rue n'existe pas, la créer
                conn.execute(
                    "INSERT OR IGNORE INTO streets(name, sector, team, status) VALUES (?, '', '', 'a_faire')",
                    (street_name,)
                )
                print(f"➕ Rue ajoutée: {street_name}")
            
            for addr in addresses:
                try:
                    # Validation des données
                    number = str(addr.get("number", "")).strip()
                    lat = addr.get("lat")
                    lon = addr.get("lon")
                    osm_type = addr.get("type", "unknown")
                    
                    if not number or lat is None or lon is None:
                        skipped_count += 1
                        continue
                    
                    conn.execute(
                        """INSERT INTO addresses (street_name, house_number, latitude, longitude, osm_type) 
                           VALUES (?, ?, ?, ?, ?)""",
                        (street_name, number, float(lat), float(lon), osm_type)
                    )
                    imported_count += 1
                except Exception as e:
                    print(f"⚠️ Erreur import adresse {addr}: {e}")
                    skipped_count += 1
        
        conn.commit()
        log_activity(conn, None, "ADDRESSES_IMPORTED", f"{imported_count} adresses importées, {skipped_count} ignorées")
        print(f"✅ {imported_count} adresses importées en base de données ({skipped_count} ignorées)")
        return imported_count
        
    except Exception as e:
        conn.rollback()
        print(f"❌ Erreur import adresses: {e}")
        return 0

def get_addresses_for_street(conn, street_name):
    """
    Récupère toutes les adresses d'une rue depuis la base de données
    """
    query = """
        SELECT 
            house_number,
            latitude,
            longitude,
            osm_type,
            created_at
        FROM addresses
        WHERE street_name = ?
        ORDER BY CAST(house_number AS INTEGER)
    """
    return pd.read_sql_query(query, conn, params=(street_name,))

def get_addresses_stats(conn):
    """
    Récupère les statistiques des adresses
    """
    cursor = conn.execute("""
        SELECT 
            COUNT(DISTINCT street_name) as streets_with_addresses,
            COUNT(*) as total_addresses,
            COUNT(DISTINCT CASE WHEN osm_type = 'node' THEN id END) as node_addresses,
            COUNT(DISTINCT CASE WHEN osm_type = 'way' THEN id END) as way_addresses
        FROM addresses
    """)
    row = cursor.fetchone()
    return {
        "streets_with_addresses": row[0] or 0,
        "total_addresses": row[1] or 0,
        "node_addresses": row[2] or 0,
        "way_addresses": row[3] or 0
    }

def get_backup_manager(db_path):
    """Retourne une instance du gestionnaire de backup"""
    return BackupManager(db_path)

# ================================================================================
# NOUVELLES FONCTIONS v4.1 - SUPERVISEUR ET BÉNÉVOLE
# ================================================================================

def get_unassigned_streets_count(conn):
    """Compte les rues non assignées à une équipe"""
    try:
        cursor = conn.execute("""
            SELECT COUNT(*) FROM streets 
            WHERE team IS NULL OR team = ''
        """)
        return cursor.fetchone()[0] or 0
    except Exception as e:
        print(f"Erreur get_unassigned_streets_count: {e}")
        return 0

def get_sectors_list(conn):
    """Récupère la liste des secteurs disponibles"""
    try:
        cursor = conn.execute("""
            SELECT DISTINCT sector FROM streets 
            WHERE sector IS NOT NULL AND sector != ''
            ORDER BY sector
        """)
        return [row[0] for row in cursor.fetchall()]
    except Exception as e:
        print(f"Erreur get_sectors_list: {e}")
        return []

def get_teams_list(conn):
    """Récupère la liste des équipes actives"""
    try:
        cursor = conn.execute("""
            SELECT id, name FROM teams 
            WHERE active = 1 AND id != 'ADMIN'
            ORDER BY name
        """)
        return [(row[0], row[1]) for row in cursor.fetchall()]
    except Exception as e:
        print(f"Erreur get_teams_list: {e}")
        return []

@auto_backup_before_critical
def bulk_assign_sector(conn, sector, team_id):
    """Assigne toutes les rues d'un secteur à une équipe"""
    try:
        # Valider les entrées
        valid_sector, clean_sector = validate_and_clean_input("sector", sector)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        
        if not valid_sector or not valid_team:
            raise ValueError("Secteur ou équipe invalide")
        
        # Vérifier que l'équipe existe
        cursor = conn.execute("SELECT COUNT(*) FROM teams WHERE id = ?", (clean_team,))
        if cursor.fetchone()[0] == 0:
            raise ValueError(f"Équipe {clean_team} inexistante")
        
        # Effectuer l'assignation
        cursor = conn.execute("""
            UPDATE streets 
            SET team = ? 
            WHERE sector = ? AND (team IS NULL OR team = '')
        """, (clean_team, clean_sector))
        
        affected_rows = cursor.rowcount
        conn.commit()
        
        # Log de l'activité
        log_activity(conn, clean_team, "bulk_assign", 
                    f"Assignation secteur {clean_sector}: {affected_rows} rues")
        
        return affected_rows
        
    except Exception as e:
        print(f"Erreur bulk_assign_sector: {e}")
        return 0

def get_team_streets(conn, team_id):
    """Récupère les rues assignées à une équipe"""
    try:
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        if not valid_team:
            return pd.DataFrame()
        
        query = """
            SELECT 
                s.name as street_name,
                s.sector,
                s.status,
                COUNT(n.id) as notes_count
            FROM streets s
            LEFT JOIN notes n ON s.name = n.street_name AND n.team_id = ?
            WHERE s.team = ?
            GROUP BY s.name, s.sector, s.status
            ORDER BY s.sector, s.name
        """
        return pd.read_sql_query(query, conn, params=(clean_team, clean_team))
        
    except Exception as e:
        print(f"Erreur get_team_streets: {e}")
        return pd.DataFrame()

@auto_backup_before_critical
def update_street_status(conn, street_name, new_status, team_id):
    """Met à jour le statut d'une rue"""
    try:
        # Valider les entrées
        valid_street, clean_street = validate_and_clean_input("street_name", street_name)
        valid_status, clean_status = validate_and_clean_input("status", new_status)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        
        if not all([valid_street, valid_status, valid_team]):
            raise ValueError("Paramètres invalides")
        
        # Vérifier que la rue est assignée à cette équipe
        cursor = conn.execute("""
            SELECT COUNT(*) FROM streets 
            WHERE name = ? AND team = ?
        """, (clean_street, clean_team))
        
        if cursor.fetchone()[0] == 0:
            raise ValueError(f"Rue {clean_street} non assignée à l'équipe {clean_team}")
        
        # Mettre à jour le statut
        conn.execute("""
            UPDATE streets 
            SET status = ? 
            WHERE name = ? AND team = ?
        """, (clean_status, clean_street, clean_team))
        
        conn.commit()
        
        # Log de l'activité
        log_activity(conn, clean_team, "status_update", 
                    f"Rue {clean_street}: {clean_status}")
        
        return True
        
    except Exception as e:
        print(f"Erreur update_street_status: {e}")
        return False

def get_assignations_export_data(conn):
    """Récupère les données d'assignation pour export CSV"""
    try:
        query = """
            SELECT 
                COALESCE(sector, 'Non défini') as secteur,
                name as rue,
                COALESCE(team, 'Non assignée') as equipe,
                CASE status 
                    WHEN 'a_faire' THEN 'À faire'
                    WHEN 'en_cours' THEN 'En cours'
                    WHEN 'terminee' THEN 'Terminée'
                    ELSE status 
                END as statut
            FROM streets
            ORDER BY secteur, rue
        """
        return pd.read_sql_query(query, conn)
        
    except Exception as e:
        print(f"Erreur get_assignations_export_data: {e}")
        return pd.DataFrame()

def log_activity(conn, team_id, action, details):
    """Enregistre une activité dans le log"""
    try:
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        valid_action, clean_action = validate_and_clean_input("text", action)
        valid_details, clean_details = validate_and_clean_input("note", details)
        
        if not all([valid_team, valid_action, valid_details]):
            print("Paramètres de log invalides")
            return
        
        conn.execute("""
            INSERT INTO activity_log (team_id, action, details)
            VALUES (?, ?, ?)
        """, (clean_team, clean_action, clean_details))
        
        conn.commit()
        
        # Log aussi dans un fichier texte pour backup
        log_dir = Path(__file__).parent / "logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / "activity.log"
        with open(log_file, "a", encoding="utf-8") as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"{timestamp} | {clean_team} | {clean_action} | {clean_details}\n")
            
    except Exception as e:
        print(f"Erreur log_activity: {e}")

def get_street_notes_for_team(conn, street_name, team_id):
    """Récupère les notes d'une rue pour une équipe"""
    try:
        valid_street, clean_street = validate_and_clean_input("street_name", street_name)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        
        if not all([valid_street, valid_team]):
            return []
        
        cursor = conn.execute("""
            SELECT address_number, comment, created_at
            FROM notes
            WHERE street_name = ? AND team_id = ?
            ORDER BY created_at DESC
        """, (clean_street, clean_team))
        
        return cursor.fetchall()
        
    except Exception as e:
        print(f"Erreur get_street_notes_for_team: {e}")
        return []

@auto_backup_before_critical
def add_street_note(conn, street_name, team_id, address_number, comment):
    """Ajoute une note pour une adresse spécifique"""
    try:
        # Valider les entrées
        valid_street, clean_street = validate_and_clean_input("street_name", street_name)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        valid_address, clean_address = validate_and_clean_input("address", address_number)
        valid_comment, clean_comment = validate_and_clean_input("note", comment)
        
        if not all([valid_street, valid_team, valid_address, valid_comment]):
            raise ValueError("Paramètres invalides")
        
        # Vérifier que la rue est assignée à cette équipe
        cursor = conn.execute("""
            SELECT COUNT(*) FROM streets 
            WHERE name = ? AND team = ?
        """, (clean_street, clean_team))
        
        if cursor.fetchone()[0] == 0:
            raise ValueError(f"Rue {clean_street} non assignée à l'équipe {clean_team}")
        
        # Ajouter la note
        conn.execute("""
            INSERT INTO notes (street_name, team_id, address_number, comment)
            VALUES (?, ?, ?, ?)
        """, (clean_street, clean_team, clean_address, clean_comment))
        
        conn.commit()
        
        # Log de l'activité
        log_activity(conn, clean_team, "note_added", 
                    f"Note ajoutée - {clean_street} #{clean_address}")
        
        return True
        
    except Exception as e:
        print(f"Erreur add_street_note: {e}")
        return False
```
---8<--- guignomap/db.py END ---

---8<--- guignomap/osm.py BEGIN ---
```py
"""
Module OSM pour Guigno-Map
Gère l'import et le cache des données OpenStreetMap pour Mascouche
"""

import io
import json
from pathlib import Path
import pandas as pd
import overpy

# Configuration
CACHE_FILE = Path(__file__).parent / "osm_cache.json"
ADDR_CACHE_FILE = Path(__file__).parent / "osm_addresses.json"

# Toutes les voies routières nommées de Mascouche
QUERY_STREETS_ALL = """
[out:json][timeout:300];
area["name"="Mascouche"]["boundary"="administrative"]->.a;
(
  way["highway"~"^(primary|secondary|tertiary|residential|service|unclassified|living_street|pedestrian|track|road|busway|footway|path)$"](area.a);
);
(._;>;);
out body;
"""
# Note: Récupère TOUS les types de voies incluant petites rues, allées, chemins piétonniers

# Requête pour les adresses
QUERY_ADDR_NODES = """
[out:json][timeout:180];
area["name"="Mascouche"]["boundary"="administrative"]->.a;
(
  node["addr:housenumber"]["addr:street"](area.a);
  way["addr:housenumber"]["addr:street"](area.a);
);
out tags center;
"""

def generate_streets_csv(city="Mascouche"):
    """
    Génère un CSV avec les noms des rues principales de la ville
    Filtre automatiquement les rues privées et les petites ruelles
    """
    try:
        api = overpy.Overpass()
        result = api.query(QUERY_STREETS_ALL)
        
        streets = []
        for way in result.ways:
            name = way.tags.get("name")
            if not name:
                continue
            g = getattr(way, "geometry", None)
            # garder si on a une vraie géométrie (>= 2 points)
            if isinstance(g, list) and len(g) >= 2:
                streets.append(name)

        streets = sorted(set(streets))
        
        # Assigner automatiquement des secteurs basés sur les patterns de noms
        sectors = []
        for street in streets:
            if any(word in street.lower() for word in ["montée", "chemin", "boulevard"]):
                sectors.append("Principal")
            elif any(word in street.lower() for word in ["avenue", "place", "croissant"]):
                sectors.append("Résidentiel")
            elif "rue" in street.lower():
                sectors.append("Centre")
            else:
                sectors.append("")
        
        df = pd.DataFrame({
            "name": streets,
            "sector": sectors,
            "team": [""] * len(streets)
        })
        
        buf = io.StringIO()
        df.to_csv(buf, index=False)
        print(f"✅ CSV généré avec {len(streets)} rues principales")
        return buf.getvalue().encode("utf-8")
        
    except Exception as e:
        print(f"❌ Erreur OSM: {e}")
        # Retourner des données de test en cas d'erreur
        return create_fallback_csv()

def build_geometry_cache():
    """
    Construit le cache des géométries pour TOUTES les voies de Mascouche
    Force la résolution complète des nodes
    """
    try:
        print("🔄 Récupération complète de toutes les voies de Mascouche...")
        
        # IMPORTANT: Configurer l'API pour résoudre automatiquement les nodes manquants
        api = overpy.Overpass()
        
        # Requête améliorée qui force le retour des coordonnées
        query = """
        [out:json][timeout:300];
        area["name"="Mascouche"]["boundary"="administrative"]->.a;
        (
          way["highway"]["name"](area.a);
          way["highway"]["ref"](area.a);
        );
        (._;>;);
        out body;
        """
        
        print("📡 Connexion à OpenStreetMap (cela peut prendre 30-60 secondes)...")
        result = api.query(query)
        
        geo = {}
        stats = {"total": 0, "avec_geo": 0, "sans_geo": 0}
        
        # Construire un dictionnaire des nodes pour accès rapide
        nodes_dict = {}
        if hasattr(result, 'nodes'):
            for node in result.nodes:
                if hasattr(node, 'id') and hasattr(node, 'lat') and hasattr(node, 'lon'):
                    nodes_dict[node.id] = (float(node.lat), float(node.lon))
        
        print(f"📍 {len(nodes_dict)} nodes récupérés")
        
        ways = result.ways if hasattr(result, 'ways') else []
        print(f"📊 {len(ways)} voies trouvées dans OpenStreetMap")
        
        for way in ways:
            try:
                # Récupérer le nom ou ref
                if not hasattr(way, 'tags'):
                    continue
                    
                name = way.tags.get("name")
                if not name:
                    ref = way.tags.get("ref")
                    if ref:
                        name = f"Autoroute {ref}"
                    else:
                        continue
                
                stats["total"] += 1
                coords = []
                
                # Récupérer les IDs des nodes
                if hasattr(way, 'nd_ids'):
                    # Si on a les IDs des nodes, les résoudre
                    for node_id in way.nd_ids:
                        if node_id in nodes_dict:
                            lat, lon = nodes_dict[node_id]
                            coords.append([lat, lon])
                elif hasattr(way, 'nodes'):
                    # Si on a directement les nodes
                    for node in way.nodes:
                        if hasattr(node, 'lat') and hasattr(node, 'lon'):
                            coords.append([float(node.lat), float(node.lon)])
                        elif hasattr(node, 'id') and node.id in nodes_dict:
                            lat, lon = nodes_dict[node.id]
                            coords.append([lat, lon])
                
                if len(coords) >= 2:
                    if name not in geo:
                        geo[name] = []
                    geo[name].append(coords)
                    stats["avec_geo"] += 1
                else:
                    stats["sans_geo"] += 1
                    
            except Exception as e:
                continue
        
        print(f"✅ Résultat: {stats['avec_geo']} voies avec géométrie sur {stats['total']} trouvées")
        
        # Si on a récupéré des données, sauvegarder
        if geo:
            CACHE_FILE.write_text(json.dumps(geo, indent=2), encoding="utf-8")
            print(f"💾 Cache créé: {len(geo)} voies sauvegardées dans osm_cache.json")
            
            # Importer aussi automatiquement dans la DB
            try:
                from pathlib import Path
                import sys
                sys.path.append(str(Path(__file__).parent))
                import db
                
                db_path = Path(__file__).parent / "guigno_map.db"
                conn = db.get_conn(db_path)
                
                # Ajouter les rues manquantes à la DB
                for street_name in geo.keys():
                    cursor = conn.execute("SELECT COUNT(*) FROM streets WHERE name = ?", (street_name,))
                    if cursor.fetchone()[0] == 0:
                        conn.execute(
                            "INSERT INTO streets(name, sector, team, status) VALUES (?, '', '', 'a_faire')",
                            (street_name,)
                        )
                conn.commit()
                print(f"✅ Rues importées dans la base de données")
            except Exception as e:
                print(f"⚠️ Import DB: {e}")
            
            return geo
        
        # Si aucune donnée, utiliser un fallback étendu
        print("⚠️ Aucune donnée OSM, utilisation du fallback local")
        return get_extended_fallback()
            
    except Exception as e:
        print(f"❌ Erreur: {e}")
        return get_extended_fallback()

def get_fallback_geometry():
    """Fallback avec les principales voies de Mascouche"""
    return {
        "Autoroute 25": [[[45.70, -73.65], [45.78, -73.58]]],
        "Autoroute 640": [[[45.76, -73.70], [45.76, -73.55]]],
        "Montée Masson": [[[45.730, -73.620], [45.765, -73.580]]],
        "Chemin Sainte-Marie": [[[45.735, -73.615], [45.755, -73.595]]],
        "Boulevard de Mascouche": [[[45.740, -73.610], [45.752, -73.590]]],
        "Chemin des Anglais": [[[45.74, -73.65], [45.75, -73.64]]],
        "Chemin Gascon": [[[45.75, -73.62], [45.76, -73.60]]],
        "Chemin Pincourt": [[[45.72, -73.64], [45.73, -73.63]]],
        "Chemin Newton": [[[45.73, -73.58], [45.74, -73.57]]],
        "Chemin Saint-Henri": [[[45.71, -73.61], [45.72, -73.60]]],
        "Chemin Saint-Pierre": [[[45.74, -73.59], [45.75, -73.58]]],
        "Avenue de la Gare": [[[45.745, -73.601], [45.748, -73.598]]],
        "Rue Dupras": [[[45.745, -73.602], [45.748, -73.599]]],
        "Rue Saint-Pierre": [[[45.746, -73.604], [45.749, -73.600]]],
        "Rue de l'Église": [[[45.747, -73.601], [45.750, -73.599]]],
        "Avenue des Érables": [[[45.755, -73.605], [45.758, -73.600]]],
        "Rue des Pins": [[[45.756, -73.603], [45.759, -73.598]]],
        "Rue Brien": [[[45.738, -73.605], [45.741, -73.600]]],
        "Rue Bohémier": [[[45.742, -73.607], [45.745, -73.604]]]
    }

def get_extended_fallback():
    """Fallback étendu avec les principales voies de Mascouche"""
    fallback = {
        # Autoroutes
        "Autoroute 25": [[[45.70, -73.65], [45.72, -73.63], [45.74, -73.61], [45.76, -73.59], [45.78, -73.58]]],
        "Autoroute 640": [[[45.76, -73.70], [45.76, -73.65], [45.76, -73.60], [45.76, -73.55]]],
        
        # Chemins principaux
        "Montée Masson": [[[45.730, -73.620], [45.740, -73.610], [45.750, -73.600], [45.765, -73.580]]],
        "Chemin Sainte-Marie": [[[45.735, -73.615], [45.745, -73.605], [45.755, -73.595]]],
        "Boulevard de Mascouche": [[[45.740, -73.610], [45.747, -73.600], [45.752, -73.590]]],
        "Chemin des Anglais": [[[45.74, -73.65], [45.745, -73.645], [45.75, -73.64]]],
        "Chemin Gascon": [[[45.75, -73.62], [45.755, -73.615], [45.76, -73.60]]],
        "Chemin Pincourt": [[[45.72, -73.64], [45.725, -73.635], [45.73, -73.63]]],
        "Chemin Newton": [[[45.73, -73.58], [45.735, -73.575], [45.74, -73.57]]],
        "Chemin Saint-Henri": [[[45.71, -73.61], [45.715, -73.605], [45.72, -73.60]]],
        "Chemin Saint-Pierre": [[[45.74, -73.59], [45.745, -73.585], [45.75, -73.58]]],
        
        # Avenues
        "Avenue de la Gare": [[[45.745, -73.601], [45.747, -73.599], [45.748, -73.598]]],
        "Avenue Bourque": [[[45.742, -73.603], [45.744, -73.601], [45.746, -73.599]]],
        "Avenue Crépeau": [[[45.743, -73.602], [45.745, -73.600], [45.747, -73.598]]],
        "Avenue Garden": [[[45.751, -73.606], [45.753, -73.604], [45.755, -73.602]]],
        "Avenue de l'Esplanade": [[[45.748, -73.605], [45.750, -73.603], [45.752, -73.601]]],
        
        # Rues du centre
        "Rue Dupras": [[[45.745, -73.602], [45.747, -73.600], [45.748, -73.599]]],
        "Rue Saint-Pierre": [[[45.746, -73.604], [45.748, -73.602], [45.749, -73.600]]],
        "Rue de l'Église": [[[45.747, -73.601], [45.749, -73.599], [45.750, -73.598]]],
        "Rue Brien": [[[45.738, -73.605], [45.740, -73.603], [45.741, -73.600]]],
        "Rue Bohémier": [[[45.742, -73.607], [45.744, -73.605], [45.745, -73.604]]],
        
        # Rues résidentielles
        "Rue des Pins": [[[45.756, -73.603], [45.758, -73.601], [45.759, -73.598]]],
        "Avenue des Érables": [[[45.755, -73.605], [45.757, -73.603], [45.758, -73.600]]],
        "Rue Gravel": [[[45.738, -73.605], [45.740, -73.603], [45.741, -73.600]]]
    }
    
    # Sauvegarder le fallback
    CACHE_FILE.write_text(json.dumps(fallback, indent=2), encoding="utf-8")
    print(f"💾 Fallback sauvegardé avec {len(fallback)} voies principales")
    
    return fallback

def load_geometry_cache():
    """
    Charge le cache de géométries depuis le fichier JSON
    Crée un cache de base si le fichier n'existe pas
    """
    if not CACHE_FILE.exists():
        print("⚠️ Cache non trouvé, construction en cours...")
        return build_geometry_cache()  # build_geometry_cache() gère déjà le fallback en mémoire
    
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
            print(f"✅ Cache chargé: {len(cache)} rues")
            return cache
    except Exception as e:
        print(f"❌ Erreur chargement cache: {e}")
        # Ne pas écrire de fallback sur disque ! Utiliser build_geometry_cache() qui gère le fallback en mémoire
        return build_geometry_cache()

def create_fallback_csv():
    """
    Crée un CSV de fallback avec quelques rues principales de Mascouche
    Utilisé si l'API OSM est indisponible
    """
    fallback_streets = [
        ("Montée Masson", "Principal"),
        ("Chemin Sainte-Marie", "Principal"),
        ("Boulevard de Mascouche", "Principal"),
        ("Chemin des Anglais", "Principal"),
        ("Rue Dupras", "Centre"),
        ("Rue Saint-Pierre", "Centre"),
        ("Rue de l'Église", "Centre"),
        ("Avenue des Érables", "Résidentiel"),
        ("Rue des Pins", "Résidentiel"),
        ("Avenue Garden", "Résidentiel"),
    ]
    
    df = pd.DataFrame(fallback_streets, columns=["name", "sector"])
    df["team"] = ""
    
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    print("⚠️ Mode fallback: 10 rues de test")
    return buf.getvalue().encode("utf-8")

def create_fallback_cache():
    """
    Crée un cache minimal pour tests
    """
    fallback_geo = {
        "Montée Masson": [[[45.730, -73.620], [45.750, -73.600], [45.765, -73.580]]],
        "Chemin Sainte-Marie": [[[45.735, -73.615], [45.748, -73.602], [45.755, -73.595]]],
        "Boulevard de Mascouche": [[[45.740, -73.610], [45.747, -73.600], [45.752, -73.590]]],
        "Rue Dupras": [[[45.745, -73.602], [45.748, -73.599]]],
        "Rue Saint-Pierre": [[[45.746, -73.604], [45.749, -73.600]]],
        "Rue de l'Église": [[[45.747, -73.601], [45.750, -73.599]]],
        "Avenue des Érables": [[[45.755, -73.605], [45.758, -73.600]]],
        "Rue des Pins": [[[45.756, -73.603], [45.759, -73.598]]],
        "Avenue Garden": [[[45.753, -73.606], [45.756, -73.601]]],
        "Rue Gravel": [[[45.738, -73.605], [45.741, -73.600]]]
    }
    
    CACHE_FILE.write_text(json.dumps(fallback_geo, indent=2), encoding="utf-8")
    print("⚠️ Cache fallback créé avec 10 rues")

# Fonction utilitaire pour tests
def test_osm_connection():
    """
    Teste la connexion à l'API Overpass
    """
    try:
        api = overpy.Overpass()
        # Requête minimale pour tester
        result = api.query('[out:json];node(45.7475,-73.6005,45.7476,-73.6004);out;')
        print("✅ Connexion OSM OK")
        return True
    except:
        print("❌ Connexion OSM échouée")
        return False

# ========================================
# NOUVELLES FONCTIONS POUR LES ADRESSES
# ========================================

def build_addresses_cache():
    """
    Construit le cache des adresses OSM pour Mascouche
    Récupère addr:housenumber + addr:street depuis OSM
    """
    try:
        api = overpy.Overpass()
        result = api.query(QUERY_ADDR_NODES)
        
        addresses = {}
        
        # Traiter les nodes avec adresses
        for node in result.nodes:
            house_number = node.tags.get("addr:housenumber")
            street_name = node.tags.get("addr:street")
            
            if house_number and street_name:
                if street_name not in addresses:
                    addresses[street_name] = []
                addresses[street_name].append({
                    "number": str(house_number),  # Forcer en string
                    "lat": float(node.lat),
                    "lon": float(node.lon),
                    "type": "node"
                })
        
        # Traiter les ways avec adresses
        for way in result.ways:
            num = way.tags.get("addr:housenumber")
            street = way.tags.get("addr:street")
            if not num or not street:
                continue
            
            # Récupérer le centre du way
            lat = getattr(way, "center_lat", None)
            lon = getattr(way, "center_lon", None)
            
            # Fallback si center_lat/lon non disponibles
            if lat is None or lon is None:
                nodes = getattr(way, "nodes", []) or []
                if nodes:
                    try:
                        valid_lats = []
                        valid_lons = []
                        for n in nodes:
                            if hasattr(n, 'lat') and hasattr(n, 'lon'):
                                if n.lat is not None and n.lon is not None:
                                    valid_lats.append(float(n.lat))
                                    valid_lons.append(float(n.lon))
                        if valid_lats and valid_lons:
                            lat = sum(valid_lats) / len(valid_lats)
                            lon = sum(valid_lons) / len(valid_lons)
                    except Exception as e:
                        print(f"Erreur calcul centre pour way: {e}")
                        continue
            
            if lat is not None and lon is not None:
                addresses.setdefault(street, []).append({
                    "number": str(num),
                    "lat": float(lat),
                    "lon": float(lon),
                    "type": "way"
                })
        
        # Trier les adresses par numéro pour chaque rue
        for street_name in addresses:
            try:
                # Tri numérique intelligent
                addresses[street_name].sort(
                    key=lambda x: (
                        int(''.join(filter(str.isdigit, x["number"]))) 
                        if any(c.isdigit() for c in x["number"]) 
                        else float('inf')
                    )
                )
            except:
                # Si le tri échoue, garder l'ordre original
                pass
        
        # Sauvegarder le cache
        ADDR_CACHE_FILE.write_text(json.dumps(addresses, indent=2), encoding="utf-8")
        total_addresses = sum(len(addrs) for addrs in addresses.values())
        print(f"✅ Cache adresses créé: {len(addresses)} rues, {total_addresses} adresses")
        return addresses
        
    except Exception as e:
        print(f"❌ Erreur construction cache adresses: {e}")
        # Créer un cache vide en cas d'erreur
        ADDR_CACHE_FILE.write_text(json.dumps({}), encoding="utf-8")
        return {}

def load_addresses_cache():
    """
    Charge le cache d'adresses depuis le fichier JSON
    """
    if not ADDR_CACHE_FILE.exists():
        print("⚠️ Cache adresses non trouvé")
        return {}
    
    try:
        with open(ADDR_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
            total_addresses = sum(len(addrs) for addrs in cache.values())
            print(f"✅ Cache adresses chargé: {len(cache)} rues, {total_addresses} adresses")
            return cache
    except Exception as e:
        print(f"❌ Erreur chargement cache adresses: {e}")
        return {}
```
---8<--- guignomap/osm.py END ---

---8<--- guignomap/reports.py BEGIN ---
```py
"""
Générateur de rapports Excel et PDF pour GuignoMap
"""

from pathlib import Path
from datetime import datetime
import pandas as pd
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.enums import TA_CENTER, TA_RIGHT
import xlsxwriter
from io import BytesIO

# Mapping des statuts pour l'affichage (évite imports circulaires)
STATUS_TO_LABEL = {"a_faire": "À faire", "en_cours": "En cours", "terminee": "Terminée"}

class ReportGenerator:
    def __init__(self, conn):
        self.conn = conn
        self.styles = getSampleStyleSheet()
        self._setup_custom_styles()
    
    def _setup_custom_styles(self):
        """Définit les styles personnalisés pour PDF"""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Heading1'],
            fontSize=24,
            textColor=colors.HexColor('#8B0000'),
            spaceAfter=30,
            alignment=TA_CENTER
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionTitle',
            parent=self.styles['Heading2'],
            fontSize=16,
            textColor=colors.HexColor('#8B0000'),
            spaceAfter=12,
            spaceBefore=12
        ))
    
    def generate_excel(self):
        """Génère un rapport Excel professionnel"""
        output = BytesIO()
        workbook = xlsxwriter.Workbook(output, {'remove_timezone': True})
        
        # Styles Excel
        header_format = workbook.add_format({
            'bold': True,
            'bg_color': '#8B0000',
            'font_color': 'white',
            'align': 'center',
            'valign': 'vcenter',
            'border': 1
        })
        
        cell_format = workbook.add_format({
            'align': 'center',
            'valign': 'vcenter',
            'border': 1
        })
        
        status_formats = {
            'terminee': workbook.add_format({'bg_color': '#90EE90', 'border': 1}),
            'en_cours': workbook.add_format({'bg_color': '#FFE4B5', 'border': 1}),
            'a_faire': workbook.add_format({'bg_color': '#FFB6C1', 'border': 1})
        }
        
        # Feuille 1 : Résumé
        summary_sheet = workbook.add_worksheet('Résumé Guignolée 2025')
        summary_sheet.set_column(0, 4, 20)  # A:E
        
        # Titre
        title_format = workbook.add_format({
            'bold': True,
            'font_size': 20,
            'font_color': '#8B0000',
            'align': 'center'
        })
        summary_sheet.merge_range(0, 0, 0, 4, 'GUIGNOLÉE 2025 - RELAIS DE MASCOUCHE', title_format)  # A1:E1
        summary_sheet.merge_range(1, 0, 1, 4, f'Rapport généré le {datetime.now().strftime("%d/%m/%Y à %H:%M")}', cell_format)  # A2:E2
        
        # Stats globales
        from db import extended_stats
        stats = extended_stats(self.conn)
        
        row = 4
        summary_sheet.write(row, 0, 'STATISTIQUES GLOBALES', header_format)
        summary_sheet.merge_range(row, 1, row, 4, '', header_format)  # B{row+1}:E{row+1}
        
        row += 2
        summary_data = [
            ['Total des rues', stats['total']],
            ['Rues terminées', stats['done']],
            ['Rues en cours', stats.get('partial', 0)],
            ['Rues à faire', stats.get('todo', 0)],
            ['Progression globale', f"{(stats['done']/stats['total']*100) if stats['total'] > 0 else 0:.1f}%"],
            ['Total des notes', stats.get('total_notes', 0)],
            ['Adresses avec notes', stats.get('addresses_with_notes', 0)]
        ]
        
        for label, value in summary_data:
            summary_sheet.write(row, 0, label, cell_format)
            summary_sheet.write(row, 1, value, cell_format)
            row += 1
        
        # Feuille 2 : Détail des rues
        streets_sheet = workbook.add_worksheet('Détail des rues')
        streets_sheet.set_column(0, 0, 30)
        streets_sheet.set_column(1, 4, 15)
        
        # Headers
        headers = ['Rue', 'Secteur', 'Équipe', 'Statut', 'Notes']
        for col, header in enumerate(headers):
            streets_sheet.write(0, col, header, header_format)
        
        # Données
        from db import list_streets
        df = list_streets(self.conn)
        
        for idx, row_data in enumerate(df.iterrows(), 1):
            _, row = row_data
            streets_sheet.write(idx, 0, row.get('name', ''), cell_format)
            streets_sheet.write(idx, 1, row.get('sector', ''), cell_format)
            streets_sheet.write(idx, 2, row.get('team', ''), cell_format)
            
            status = row.get('status', 'a_faire')
            format_to_use = status_formats.get(status, cell_format)
            status_label = STATUS_TO_LABEL.get(status, "À faire")
            streets_sheet.write(idx, 3, status_label, format_to_use)
            
            streets_sheet.write(idx, 4, row.get('notes', 0), cell_format)
        
        # Feuille 3 : Performance des équipes
        teams_sheet = workbook.add_worksheet('Performance équipes')
        teams_sheet.set_column(0, 5, 15)
        
        from db import stats_by_team
        teams_df = stats_by_team(self.conn)
        
        if not teams_df.empty:
            headers = ['Équipe', 'Total rues', 'Terminées', 'En cours', 'Notes', 'Progression %']
            for col, header in enumerate(headers):
                teams_sheet.write(0, col, header, header_format)
            
            for idx, row_data in enumerate(teams_df.iterrows(), 1):
                _, row = row_data
                teams_sheet.write(idx, 0, row.get('team', ''), cell_format)
                teams_sheet.write(idx, 1, row.get('total', 0), cell_format)
                teams_sheet.write(idx, 2, row.get('done', 0), cell_format)
                teams_sheet.write(idx, 3, row.get('partial', 0), cell_format)
                teams_sheet.write(idx, 4, row.get('notes', 0), cell_format)
                teams_sheet.write(idx, 5, f"{row.get('progress', 0):.1f}%", cell_format)
        
        workbook.close()
        output.seek(0)
        return output.getvalue()
    
    def generate_pdf(self):
        """Génère un rapport PDF professionnel"""
        output = BytesIO()
        doc = SimpleDocTemplate(output, pagesize=A4)
        story = []
        
        # Page de titre
        story.append(Paragraph("GUIGNOLÉE 2025", self.styles['CustomTitle']))
        story.append(Paragraph("Le Relais de Mascouche", self.styles['Title']))
        story.append(Spacer(1, 0.2*inch))
        story.append(Paragraph(f"Rapport généré le {datetime.now().strftime('%d/%m/%Y à %H:%M')}", self.styles['Normal']))
        story.append(PageBreak())
        
        # Résumé
        story.append(Paragraph("Résumé de la collecte", self.styles['SectionTitle']))
        
        from db import extended_stats
        stats = extended_stats(self.conn)
        
        summary_data = [
            ['Statistique', 'Valeur'],
            ['Total des rues', str(stats['total'])],
            ['Rues terminées', str(stats['done'])],
            ['Rues en cours', str(stats.get('partial', 0))],
            ['Rues à faire', str(stats.get('todo', 0))],
            ['Progression', f"{(stats['done']/stats['total']*100) if stats['total'] > 0 else 0:.1f}%"],
            ['Total notes', str(stats.get('total_notes', 0))]
        ]
        
        summary_table = Table(summary_data, colWidths=[3*inch, 2*inch])
        summary_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#8B0000')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 14),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(summary_table)
        story.append(PageBreak())
        
        # Performance des équipes
        story.append(Paragraph("Performance des équipes", self.styles['SectionTitle']))
        
        from db import stats_by_team
        teams_df = stats_by_team(self.conn)
        
        if not teams_df.empty:
            teams_data = [['Équipe', 'Total', 'Terminées', 'En cours', 'Progression']]
            for _, row in teams_df.iterrows():
                teams_data.append([
                    row.get('team', ''),
                    str(row.get('total', 0)),
                    str(row.get('done', 0)),
                    str(row.get('partial', 0)),
                    f"{row.get('progress', 0):.1f}%"
                ])
            
            teams_table = Table(teams_data, colWidths=[2*inch, 1*inch, 1*inch, 1*inch, 1.5*inch])
            teams_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#8B0000')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lightgrey),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(teams_table)
        
        doc.build(story)
        output.seek(0)
        return output.getvalue()
```
---8<--- guignomap/reports.py END ---

---8<--- guignomap/validators.py BEGIN ---
```py
"""
Validateurs et sanitizers pour GuignoMap
Protection contre injections et validation des formats
"""

import re
import html
from typing import Optional, Tuple

class InputValidator:
    """Classe de validation et sanitization des entrées"""
    
    @staticmethod
    def sanitize_text(text: str, max_length: int = 255) -> str:
        """Nettoie et limite un texte"""
        if not text:
            return ""
        # Supprimer les caractères de contrôle
        text = "".join(char for char in text if ord(char) >= 32 or char == '\n')
        # Échapper le HTML
        text = html.escape(text)
        # Limiter la longueur
        return text[:max_length].strip()
    
    @staticmethod
    def sanitize_street_name(name: str) -> str:
        """Valide et nettoie un nom de rue"""
        if not name:
            return ""
        # Garder seulement lettres, chiffres, espaces, tirets, apostrophes, accents
        name = re.sub(r'[^a-zA-ZÀ-ÿ0-9\s\-\'\.]', '', name)
        return name[:100].strip()
    
    @staticmethod
    def sanitize_team_id(team_id: str) -> str:
        """Valide un ID d'équipe"""
        if not team_id:
            return ""
        # Format: LETTRES + CHIFFRES seulement, max 20 caractères
        team_id = re.sub(r'[^A-Z0-9]', '', team_id.upper())
        return team_id[:20]
    
    @staticmethod
    def sanitize_address_number(number: str) -> str:
        """Valide un numéro civique"""
        if not number:
            return ""
        # Garder chiffres et lettres (ex: 123A)
        number = re.sub(r'[^0-9A-Za-z\-]', '', number)
        return number[:10]
    
    @staticmethod
    def validate_password(password: str) -> Tuple[bool, str]:
        """Valide la force d'un mot de passe - minimum 4 caractères"""
        if password is None:
            return False, "Mot de passe requis"
        if len(password) < 4:
            return False, "Minimum 4 caractères"
        if len(password) > 128:
            return False, "Maximum 128 caractères"
        return True, "OK"
    
    @staticmethod
    def validate_sector(sector: str) -> str:
        """Valide un secteur"""
        valid_sectors = ['Principal', 'Centre', 'Nord', 'Sud', 'Est', 'Ouest', 'Résidentiel', '']
        if sector not in valid_sectors:
            return ''
        return sector
    
    @staticmethod
    def validate_status(status: str) -> str:
        """Valide un statut de rue"""
        valid_statuses = ['a_faire', 'en_cours', 'terminee']
        if status not in valid_statuses:
            return 'a_faire'
        return status
    
    @staticmethod
    def sanitize_note(note: str) -> str:
        """Nettoie une note/commentaire"""
        if not note:
            return ""
        # Supprimer caractères dangereux mais garder ponctuation basique
        note = re.sub(r'[<>\"\'`;]', '', note)
        return note[:500].strip()
    
    @staticmethod
    def is_sql_safe(text: str) -> bool:
        """Vérifie qu'un texte ne contient pas de patterns SQL dangereux"""
        if not text:
            return True
        dangerous_patterns = [
            r'\bDROP\b', r'\bDELETE\b', r'\bINSERT\b', r'\bUPDATE\b',
            r'\bEXEC\b', r'\bEXECUTE\b', r'--', r'/\*', r'\*/', r';'
        ]
        text_upper = text.upper()
        for pattern in dangerous_patterns:
            if re.search(pattern, text_upper):
                return False
        return True

def validate_and_clean_input(input_type: str, value: str) -> Tuple[bool, str]:
    """Fonction principale de validation"""
    validator = InputValidator()
    
    if input_type == "team_id":
        clean = validator.sanitize_team_id(value)
        return bool(clean), clean
    
    elif input_type == "street_name":
        clean = validator.sanitize_street_name(value)
        if not validator.is_sql_safe(clean):
            return False, ""
        return bool(clean), clean
    
    elif input_type == "address":
        clean = validator.sanitize_address_number(value)
        return bool(clean), clean
    
    elif input_type == "note":
        clean = validator.sanitize_note(value)
        if not validator.is_sql_safe(clean):
            return False, ""
        return bool(clean), clean
    
    elif input_type == "sector":
        clean = validator.validate_sector(value)
        return True, clean
    
    elif input_type == "status":
        clean = validator.validate_status(value)
        return True, clean
    
    elif input_type == "password":
        valid, msg = validator.validate_password(value)
        return valid, value if valid else ""
    
    else:
        clean = validator.sanitize_text(value)
        return bool(clean), clean
```
---8<--- guignomap/validators.py END ---

---8<--- requirements.txt BEGIN ---
```txt
streamlit>=1.36.0
pandas>=2.2.0
folium==0.20.0
streamlit-folium>=0.21.0
overpy==0.7
bcrypt>=4.0.0
plotly>=5.18.0
xlsxwriter==3.2.8
reportlab==4.4.3

# Database - PostgreSQL
psycopg2-binary>=2.9.7
sqlalchemy==2.0.23
alembic==1.13.1

# Authentication - Argon2
passlib[argon2]==1.7.4

# Storage - S3/Cloud
boto3==1.34.144
```
---8<--- requirements.txt END ---

---8<--- scripts/export_repo_audit.py BEGIN ---
```py
# coding: utf-8
"""
Export d’audit GuignoMap → exports/export_audit_YYYYMMDD_HHMMSS.txt

Objectifs:
- Inclure 100% du contenu de TOUS les fichiers .py (src/, tests/, scripts/, guignomap/…)
- Inclure fichiers importants: config (toml/yaml/ini/json/sql), alembic.ini, migrations, requirements*, pyproject.toml, Dockerfile, .gitignore, README.md
- Exclure: backups, exports, caches, pycache, venv, .git, binaires, secrets (.streamlit/secrets.toml)
- Ajouter une section ENV avec: version Python, chemin exécutable, plateforme, LISTE COMPLETE des paquets installés
Sortie: UTF-8 (LF), sans BOM
"""

from __future__ import annotations
from pathlib import Path
from datetime import datetime
import sys, platform, subprocess

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "exports"
OUTDIR.mkdir(parents=True, exist_ok=True)
OUTFILE = OUTDIR / f"export_audit_{datetime.now():%Y%m%d_%H%M%S}.txt"

# Dossiers à exclure totalement
EXCLUDE_DIRS = {
    ".git", ".github", ".venv", "venv", "env", ".vscode", "__pycache__", ".pytest_cache", ".mypy_cache",
    "node_modules", "storage_local", "backups", "exports", "logs", ".idea", ".DS_Store"
}

# Fichiers exactement exclus (secrets etc.)
EXCLUDE_FILES = {
    "secrets.toml",  # ne jamais sortir les secrets
    ".python-version"
}

# Extensions autorisées pour les fichiers non-.py, considérés “importants à auditer”
ALLOW_NONPY_EXTS = {
    ".toml", ".ini", ".cfg", ".conf", ".yml", ".yaml", ".json", ".sql",
    ".md", ".txt", ".ps1", ".bat", ".sh", ".dockerignore"
}

# Fichiers/chemins “importants” acceptés même sans extension (ou spécifiques)
ALLOW_SPECIAL_PATHS = {
    "alembic.ini",
    "Dockerfile",
    ".gitignore",
    ".env.example",
    ".streamlit/config.toml",
    "requirements.txt",
    "requirements-dev.txt",
    "pyproject.toml",
}

# Préfixes utiles (whitelist de zones pertinentes)
ALLOW_PREFIXES = [
    "src/",
    "tests/",
    "scripts/",
    "guignomap/",
    "src/database/migrations/",
]

# Ne PAS limiter la taille des .py (on les veut ENTIEREMENT)
MAX_NONPY_SIZE = 200 * 1024  # 200KB pour les non-.py (pour éviter blobs inutiles)

def is_excluded_path(p: Path) -> bool:
    for part in p.parts:
        if part in EXCLUDE_DIRS:
            return True
    return False

def is_allowed_file(p: Path) -> bool:
    if p.name in EXCLUDE_FILES:
        return False

    rel = p.relative_to(ROOT).as_posix()

    # .py → toujours inclus s'il est dans une zone pertinente
    if p.suffix.lower() == ".py":
        if any(rel == pre or rel.startswith(pre) for pre in ALLOW_PREFIXES):
            return True
        return False

    # Fichiers spéciaux explicitement autorisés
    if rel in ALLOW_SPECIAL_PATHS:
        return True

    # Fichiers non-.py: extension autorisée ET dans une zone pertinente
    if p.suffix.lower() in ALLOW_NONPY_EXTS:
        if any(rel == pre or rel.startswith(pre) for pre in ALLOW_PREFIXES):
            return True

    return False

def read_text_utf8(p: Path) -> str:
    try:
        txt = p.read_text(encoding="utf-8", errors="replace")
    except Exception as e:
        txt = f"<<ERREUR LECTURE {p}: {e}>>"
    return txt.replace("\r\n", "\n").replace("\r", "\n")

def collect_files() -> list[Path]:
    files: list[Path] = []
    for p in ROOT.rglob("*"):
        if not p.is_file():
            continue
        if is_excluded_path(p):
            continue
        if not is_allowed_file(p):
            continue

        # Taille: .py = aucune limite; autres = borne raisonnable
        if p.suffix.lower() != ".py":
            try:
                if p.stat().st_size > MAX_NONPY_SIZE:
                    continue
            except Exception:
                continue

        files.append(p)

    files.sort(key=lambda x: x.relative_to(ROOT).as_posix().lower())
    return files

def get_installed_packages() -> list[str]:
    """Retourne une liste 'Nom==Version' triée, en évitant les attributs privés."""
    # 1) Tentative avec importlib.metadata (propre)
    try:
        from importlib.metadata import distributions
        pkgs = []
        for dist in distributions():
            meta = getattr(dist, "metadata", None)
            # meta est un email.message.Message (supporte .get)
            name = None
            if meta:
                name = meta.get("Name")
            ver = getattr(dist, "version", None)
            if name and ver:
                pkgs.append(f"{name}=={ver}")
        if pkgs:
            return sorted(pkgs, key=lambda s: s.lower())
    except Exception:
        pass

    # 2) Fallback avec pkg_resources (setuptools)
    try:
        import pkg_resources  # type: ignore
        pkgs = [f"{d.project_name}=={d.version}" for d in pkg_resources.working_set]
        if pkgs:
            return sorted(set(pkgs), key=lambda s: s.lower())
    except Exception:
        pass

    # 3) Dernier recours: pip freeze (synchronisé avec l’environnement courant)
    try:
        out = subprocess.check_output([sys.executable, "-m", "pip", "freeze"], text=True, encoding="utf-8")
        pkgs = [line.strip() for line in out.splitlines() if line.strip()]
        if pkgs:
            return sorted(pkgs, key=lambda s: s.lower())
    except Exception as e:
        return [f"<<ERREUR INVENTAIRE DEPENDANCES: {e}>>"]

    return []

def env_section() -> str:
    lines: list[str] = []
    lines.append("## ENVIRONNEMENT")
    lines.append(f"- Python : {sys.version.splitlines()[0]}")
    lines.append(f"- Exécutable : {sys.executable}")
    lines.append(f"- Plateforme : {platform.platform()}")

    # versions utiles si présentes
    try:
        import streamlit as _st
        lines.append(f"- streamlit : {_st.__version__}")
    except Exception:
        pass
    try:
        import sqlalchemy as _sa
        lines.append(f"- sqlalchemy : {_sa.__version__}")
    except Exception:
        pass
    try:
        import pandas as _pd
        lines.append(f"- pandas : {_pd.__version__}")
    except Exception:
        pass
    try:
        import boto3 as _b3
        lines.append(f"- boto3 : {_b3.__version__}")
    except Exception:
        pass
    try:
        import passlib as _pl  # noqa
        lines.append("- passlib : présent")
    except Exception:
        pass

    lines.append("")
    lines.append("### Dépendances installées (inventaire)")
    pkgs = get_installed_packages() or ["<<Aucune dépendance détectée>>"]
    lines.extend(pkgs)
    lines.append("")
    return "\n".join(lines)

def main() -> int:
    files = collect_files()

    header = [
        "# GuignoMap — Export d’audit COMPLET (code et config utiles)",
        f"# Date : {datetime.now():%Y-%m-%d %H:%M:%S}",
        f"# Racine : {ROOT}",
        "# Contenu : 100% des .py (zones pertinentes) + fichiers de config/migrations essentiels",
        "# Exclus : backups, exports, caches, venv, .git, binaires, secrets (.streamlit/secrets.toml)",
        ""
    ]

    with OUTFILE.open("w", encoding="utf-8", newline="\n") as out:
        out.write("\n".join(header) + "\n")
        out.write(env_section() + "\n")

        out.write("## INDEX DES FICHIERS INCLUS\n")
        for p in files:
            out.write(f"- {p.relative_to(ROOT).as_posix()}\n")

        out.write("\n## CONTENU DES FICHIERS\n")
        for p in files:
            rel = p.relative_to(ROOT).as_posix()
            out.write(f"\n---8<--- {rel} BEGIN ---\n")
            out.write("```" + (p.suffix.lower().lstrip(".") or "txt") + "\n")
            out.write(read_text_utf8(p))
            out.write("\n```\n")
            out.write(f"---8<--- {rel} END ---\n")

        out.write("\n## NOTE\n- Secrets exclus par conception (ex: .streamlit/secrets.toml)\n")
        out.write("- Tous les .py des zones pertinentes sont inclus en intégralité.\n")

    print(f"✅ Export d’audit écrit : {OUTFILE}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

```
---8<--- scripts/export_repo_audit.py END ---

---8<--- scripts/export_repo_min.py BEGIN ---
```py

```
---8<--- scripts/export_repo_min.py END ---

---8<--- scripts/export_repo_snapshot.py BEGIN ---
```py
# coding: utf-8
"""
Export complet du code GuignoMap → exports/export_full_YYYYMMDD_HHMMSS.txt
- UTF-8 (sans BOM), normalise les fins de lignes.
- Inclut le contenu des fichiers code/texte utiles.
- Exclut .git, .venv, caches, binaires, gros fichiers.
- Ne lit jamais .streamlit/secrets.toml (sécurité).
"""

from __future__ import annotations
import sys, os, io, time
from pathlib import Path
from datetime import datetime

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "exports"
OUTDIR.mkdir(parents=True, exist_ok=True)
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTFILE = OUTDIR / f"export_full_{ts}.txt"

# Dossiers à exclure
EXCLUDE_DIRS = {
    ".git", ".github", ".venv", ".vscode", "__pycache__", ".mypy_cache", ".pytest_cache",
    "node_modules", "dist", "build", "storage_local", ".idea"
}

# Fichiers à exclure (par nom exact)
EXCLUDE_FILES = {
    "secrets.toml",  # ne jamais exposer des secrets
}

# Extensions à inclure (code/texte)
INCLUDE_EXTS = {
    ".py", ".txt", ".md", ".ps1", ".bat", ".toml", ".ini", ".cfg",
    ".yml", ".yaml", ".json", ".sql"
}

# Extensions à exclure d’office (binaires/pondéreux)
BINARY_EXTS = {
    ".db", ".sqlite", ".sqlite3", ".pkl", ".zip", ".7z", ".rar", ".exe", ".dll",
    ".png", ".jpg", ".jpeg", ".gif", ".ico", ".pdf", ".parquet"
}

# Taille max (Ko) par fichier pour éviter les énormes blobs
SIZE_LIMIT_KB = 300  # ajuste si besoin

def should_skip(path: Path) -> bool:
    # Exclure par dossier
    for part in path.parts:
        if part in EXCLUDE_DIRS:
            return True
    # Exclure par extension binaire
    if path.suffix.lower() in BINARY_EXTS:
        return True
    # Exclure fichier secrets explicites
    if path.name in EXCLUDE_FILES:
        return True
    # Filtre d’extensions
    if path.suffix and path.suffix.lower() not in INCLUDE_EXTS:
        return True
    # Limite de taille
    try:
        if path.stat().st_size > SIZE_LIMIT_KB * 1024:
            return True
    except Exception:
        return True
    return False

def read_text_safely(path: Path) -> str:
    # Toujours lire en UTF-8 avec remplacement pour éviter les caractères corrompus
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            txt = f.read()
    except Exception as e:
        txt = f"<<ERREUR LECTURE {path}: {e}>>"
    # Normaliser fins de lignes
    return txt.replace("\r\n", "\n").replace("\r", "\n")

def main() -> int:
    files: list[Path] = []
    for p in ROOT.rglob("*"):
        if not p.is_file():
            continue
        if should_skip(p):
            continue
        files.append(p)

    files.sort(key=lambda x: str(x).lower())

    header = f"""# GuignoMap - Export de code COMPLET
# Date : {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
# Racine : {ROOT}
# Fichiers inclus : {len(files)}
# Encodage : UTF-8 (sans BOM)
# Règles : exclusions .git/.venv/binaires/gros fichiers, NO secrets.toml
"""

    with open(OUTFILE, "w", encoding="utf-8", newline="\n") as out:
        out.write(header + "\n")
        out.write("## INDEX\n")
        for p in files:
            rel = p.relative_to(ROOT)
            out.write(f"- {rel.as_posix()}\n")

        out.write("\n## CONTENU DES FICHIERS\n")
        for p in files:
            rel = p.relative_to(ROOT)
            out.write("\n")
            out.write(f"---8<--- {rel.as_posix()} BEGIN ---\n")
            out.write("```" + f"{p.suffix.lower().lstrip('.') or 'txt'}" + "\n")
            out.write(read_text_safely(p))
            out.write("\n```\n")
            out.write(f"---8<--- {rel.as_posix()} END ---\n")

        out.write("\n## STATISTIQUES\n")
        total_bytes = sum((p.stat().st_size for p in files), 0)
        out.write(f"- Total fichiers exportés : {len(files)}\n")
        out.write(f"- Poids cumulé (approx) : {total_bytes/1024:.1f} Ko\n")
        out.write(f"- Limite par fichier : {SIZE_LIMIT_KB} Ko\n")

    print(f"✅ Export écrit : {OUTFILE}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

```
---8<--- scripts/export_repo_snapshot.py END ---

---8<--- scripts/fix_app_types.py BEGIN ---
```py
#!/usr/bin/env python3
"""
Script de migration automatique des types DataFrame vers List[Dict] dans app.py
Corrige les incompatibilités entre l'ancien db.py et le nouveau db_v5.py
"""

import re
from pathlib import Path

def fix_app_py():
    """Corrige automatiquement les incompatibilités de types dans app.py"""
    
    app_path = Path("guignomap/app.py")
    content = app_path.read_text(encoding='utf-8')
    
    # Corrections par regex
    corrections = [
        # 1. .empty sur listes -> len() == 0
        (r'if not (\w+)\.empty:', r'if \1:  # Liste non vide'),
        (r'if (\w+)\.empty:', r'if not \1:  # Liste vide'),
        
        # 2. .iterrows() -> enumerate() ou iteration directe
        (r'for _, (\w+) in (\w+)\.iterrows\(\):', r'for \1 in \2:'),
        (r'for (\w+), (\w+) in (\w+)\.iterrows\(\):', r'for \1, \2 in enumerate(\3):'),
        
        # 3. DataFrame['column'] -> list access pour unassigned
        (r"unassigned\['name'\]\.tolist\(\)", r"unassigned"),
        
        # 4. to_csv() sur listes -> DataFrame conversion
        (r'(\w+)\.to_csv\(([^)]+)\)', r'pd.DataFrame(\1).to_csv(\2)'),
        
        # 5. team_streets filtering (fonction get_team_streets retourne List[str])
        (r"team_streets\[team_streets\['status'\] == '(\w+)'\]", r"[s for s in team_streets if hasattr(s, 'status') and s.status == '\1']"),
        
        # 6. Accès par index sur dictionnaires (notes)
        (r"note\[(\d+)\]", r"list(note.values())[\1] if isinstance(note, dict) else note[\1]"),
    ]
    
    for pattern, replacement in corrections:
        content = re.sub(pattern, replacement, content)
    
    # Corrections manuelles spécifiques
    
    # Fix pour get_team_streets qui doit retourner les données complètes, pas juste les noms
    content = content.replace(
        'def get_team_streets(team_id: str) -> List[str]:',
        'def get_team_streets(team_id: str) -> List[Dict[str, Any]]:'
    )
    
    # Fix pour l'utilisation de team_streets dans l'interface équipe
    content = re.sub(
        r'if team_streets\.empty:',
        'if not team_streets:',
        content
    )
    
    content = re.sub(
        r'done_streets = len\(team_streets\[team_streets\[\'status\'\] == \'terminee\'\]\)',
        'done_streets = len([s for s in team_streets if isinstance(s, dict) and s.get("status") == "terminee"])',
        content
    )
    
    content = re.sub(
        r'in_progress = len\(team_streets\[team_streets\[\'status\'\] == \'en_cours\'\]\)',
        'in_progress = len([s for s in team_streets if isinstance(s, dict) and s.get("status") == "en_cours"])',
        content
    )
    
    # Fix pour l'iteration sur team_streets
    content = re.sub(
        r'for street in team_streets:',
        'for street in team_streets:\n            if isinstance(street, str):\n                street_name = street\n            else:\n                street_name = street.get("name", street)',
        content
    )
    
    # Fix pour les notes dans l'affichage
    content = re.sub(
        r'st\.markdown\(f"• \*\*#{note\[0\]}\*\* : {note\[1\]} _{note\[2\]}_"\)',
        'st.markdown(f"• **#{note.get(\'address_number\', \'?\')}** : {note.get(\'comment\', \'\')} _{note.get(\'created_at\', \'\')}_ ")',
        content
    )
    
    # Sauvegarder
    app_path.write_text(content, encoding='utf-8')
    print("✅ app.py corrigé automatiquement")

if __name__ == "__main__":
    fix_app_py()
```
---8<--- scripts/fix_app_types.py END ---

---8<--- scripts/fix_specific.py BEGIN ---
```py
#!/usr/bin/env python3
"""
Script de correction fine pour les types de retour dans app.py
"""

import re
from pathlib import Path

def fix_specific_issues():
    """Corrige les problèmes spécifiques identifiés"""
    
    app_path = Path("guignomap/app.py")
    content = app_path.read_text(encoding='utf-8')
    
    # 1. Revert les corrections sur DataFrames (list_streets retourne bien un DataFrame)
    content = re.sub(
        r'if df_all:  # Liste non vide',
        'if not df_all.empty:',
        content
    )
    
    content = re.sub(
        r'if not df_team:  # Liste vide',
        'if df_team.empty:',
        content
    )
    
    # 2. Fix team_streets access patterns
    content = re.sub(
        r'done_streets = len\(\[s for s in team_streets if hasattr\(s, \'status\'\) and s\.status == \'terminee\'\]\)',
        'done_streets = len([s for s in team_streets if s.get("status") == "terminee"])',
        content
    )
    
    content = re.sub(
        r'in_progress = len\(\[s for s in team_streets if hasattr\(s, \'status\'\) and s\.status == \'en_cours\'\]\)',
        'in_progress = len([s for s in team_streets if s.get("status") == "en_cours"])',
        content
    )
    
    # 3. Fix row access in iterrows (certains endroits ont encore des DataFrames)
    content = re.sub(
        r"for row in df_team:\s*street = row\['name'\]\s*status = row\['status'\]\s*notes_count = row\.get\('notes', 0\)",
        """for _, row in df_team.iterrows():
            street = row['name']
            status = row['status'] 
            notes_count = row.get('notes', 0)""",
        content, flags=re.MULTILINE
    )
    
    app_path.write_text(content, encoding='utf-8')
    print("✅ Corrections spécifiques appliquées")

if __name__ == "__main__":
    fix_specific_issues()
```
---8<--- scripts/fix_specific.py END ---

---8<--- scripts/migrate_password_hashes.py BEGIN ---
```py
"""
Script de migration des hashes de mots de passe bcrypt → Argon2
Migration paresseuse : les anciens hashes bcrypt sont migrés lors de la prochaine connexion
"""
import sys
import sqlite3
from pathlib import Path
from datetime import datetime

# Ajouter le répertoire parent au PYTHONPATH pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from src.auth.passwords import get_password_hash_info, is_bcrypt_hash, is_argon2_hash


def get_sqlite_connection():
    """Connexion à la base SQLite existante"""
    sqlite_path = Path(__file__).parent.parent / "guignomap" / "guigno_map.db"
    if not sqlite_path.exists():
        print(f"❌ Base SQLite non trouvée: {sqlite_path}")
        return None
    
    conn = sqlite3.connect(str(sqlite_path))
    conn.row_factory = sqlite3.Row
    return conn


def analyze_password_hashes():
    """
    Analyse des hashes de mots de passe dans la base
    Identifie les équipes avec des hashes bcrypt qui nécessitent une migration
    """
    conn = get_sqlite_connection()
    if not conn:
        return
    
    try:
        print("🔍 Analyse des hashes de mots de passe...")
        print("=" * 50)
        
        # Récupérer toutes les équipes
        cursor = conn.execute("SELECT id, name, password_hash, created_at FROM teams ORDER BY id")
        teams = cursor.fetchall()
        
        if not teams:
            print("ℹ️ Aucune équipe trouvée dans la base")
            return
        
        bcrypt_count = 0
        argon2_count = 0
        unknown_count = 0
        
        print(f"{'Équipe':<15} {'Algorithme':<10} {'Statut':<20} {'Créé le'}")
        print("-" * 65)
        
        for team in teams:
            team_id = team['id']
            name = team['name']
            hash_value = team['password_hash']
            created_at = team['created_at']
            
            # Analyser le hash
            hash_info = get_password_hash_info(hash_value)
            algorithm = hash_info['algorithm']
            needs_update = hash_info['needs_update']
            
            if algorithm == 'bcrypt':
                bcrypt_count += 1
                status = "🔄 À migrer"
            elif algorithm == 'argon2':
                argon2_count += 1
                status = "✅ Moderne" if not needs_update else "🔄 À mettre à jour"
            else:
                unknown_count += 1
                status = "❓ Inconnu"
            
            print(f"{team_id:<15} {algorithm:<10} {status:<20} {created_at or 'N/A'}")
        
        print("-" * 65)
        print(f"\n📊 Résumé de l'analyse:")
        print(f"   • Hashes bcrypt (à migrer) : {bcrypt_count}")
        print(f"   • Hashes Argon2 (modernes) : {argon2_count}")
        print(f"   • Hashes inconnus          : {unknown_count}")
        print(f"   • Total équipes            : {len(teams)}")
        
        if bcrypt_count > 0:
            print(f"\n💡 Migration nécessaire:")
            print(f"   Les {bcrypt_count} équipe(s) avec bcrypt seront automatiquement")
            print(f"   migrées vers Argon2 lors de leur prochaine connexion réussie.")
            print(f"   Aucune action manuelle n'est requise.")
        else:
            print(f"\n🎉 Toutes les équipes utilisent déjà Argon2 !")
        
    except Exception as e:
        print(f"❌ Erreur lors de l'analyse: {e}")
    finally:
        conn.close()


def generate_migration_report():
    """
    Génère un rapport détaillé de migration
    """
    conn = get_sqlite_connection()
    if not conn:
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = Path(__file__).parent.parent / f"password_migration_report_{timestamp}.txt"
    
    try:
        with open(report_path, 'w', encoding='utf-8') as report:
            report.write(f"Rapport de migration des mots de passe - {datetime.now()}\n")
            report.write("=" * 70 + "\n\n")
            
            # Récupérer toutes les équipes
            cursor = conn.execute("SELECT id, name, password_hash, created_at FROM teams ORDER BY id")
            teams = cursor.fetchall()
            
            teams_to_migrate = []
            
            for team in teams:
                team_id = team['id']
                name = team['name']
                hash_value = team['password_hash']
                created_at = team['created_at']
                
                hash_info = get_password_hash_info(hash_value)
                
                report.write(f"Équipe: {team_id} ({name})\n")
                report.write(f"  Créée: {created_at or 'Date inconnue'}\n")
                report.write(f"  Algorithme: {hash_info['algorithm']}\n")
                report.write(f"  Nécessite mise à jour: {hash_info['needs_update']}\n")
                
                if 'passlib_scheme' in hash_info:
                    report.write(f"  Schéma passlib: {hash_info['passlib_scheme']}\n")
                
                if hash_info['algorithm'] == 'bcrypt':
                    teams_to_migrate.append(team_id)
                    report.write(f"  🔄 MIGRATION REQUISE lors de la prochaine connexion\n")
                elif hash_info['algorithm'] == 'argon2':
                    report.write(f"  ✅ Hash moderne\n")
                else:
                    report.write(f"  ⚠️ Hash de type inconnu\n")
                
                report.write("\n")
            
            report.write(f"RÉSUMÉ DE MIGRATION\n")
            report.write("=" * 30 + "\n")
            report.write(f"Équipes à migrer: {len(teams_to_migrate)}\n")
            if teams_to_migrate:
                report.write(f"IDs concernés: {', '.join(teams_to_migrate)}\n")
            report.write(f"Total équipes: {len(teams)}\n\n")
            
            report.write("PROCÉDURE DE MIGRATION\n")
            report.write("=" * 30 + "\n")
            report.write("1. La migration est automatique et transparente\n")
            report.write("2. Elle se déclenche lors de la prochaine connexion réussie\n")
            report.write("3. L'ancien hash bcrypt est remplacé par un nouveau hash Argon2\n")
            report.write("4. Le mot de passe de l'utilisateur reste inchangé\n")
            report.write("5. Aucune action manuelle n'est requise\n\n")
            
            report.write("POLITIQUE DE MOT DE PASSE\n")
            report.write("=" * 30 + "\n")
            report.write("• Minimum 4 caractères (politique UI v4.1 conservée)\n")
            report.write("• Confirmation requise lors de la création (UI)\n")
            report.write("• Algorithme Argon2 pour nouveaux comptes\n")
            report.write("• Compatibilité bcrypt maintenue\n")
        
        print(f"📄 Rapport généré: {report_path}")
        
    except Exception as e:
        print(f"❌ Erreur génération rapport: {e}")
    finally:
        conn.close()


def test_migration_functions():
    """
    Test des fonctions de migration avec des données d'exemple
    """
    print("🧪 Test des fonctions de migration...")
    
    try:
        from src.auth.passwords import create_test_hashes, verify_password, migrate_password_if_needed
        
        # Créer des hashes de test
        test_password = "test123"
        hashes = create_test_hashes(test_password)
        
        print(f"\n🔑 Mot de passe de test: {test_password}")
        print(f"Hash Argon2: {hashes['argon2'][:50]}...")
        print(f"Hash bcrypt: {hashes['bcrypt_legacy'][:50]}...")
        
        # Tester la vérification
        print(f"\n✅ Tests de vérification:")
        
        # Test Argon2
        ok, needs_rehash = verify_password(test_password, hashes['argon2'])
        print(f"Argon2: OK={ok}, Rehash={needs_rehash}")
        
        # Test bcrypt
        ok, needs_rehash = verify_password(test_password, hashes['bcrypt_legacy'])
        print(f"bcrypt: OK={ok}, Rehash={needs_rehash}")
        
        # Test migration
        print(f"\n🔄 Test de migration:")
        migrated, new_hash = migrate_password_if_needed(test_password, hashes['bcrypt_legacy'])
        print(f"Migration effectuée: {migrated}")
        if migrated:
            print(f"Nouveau hash: {new_hash[:50]}...")
        
        print(f"✅ Tests terminés avec succès")
        
    except Exception as e:
        print(f"❌ Erreur durant les tests: {e}")


def main():
    """Point d'entrée principal du script"""
    print("🔐 Script de migration des mots de passe GuignoMap v5.0")
    print("bcrypt → Argon2 avec migration paresseuse")
    print("=" * 60)
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "analyze":
            analyze_password_hashes()
        elif command == "report":
            generate_migration_report()
        elif command == "test":
            test_migration_functions()
        else:
            print(f"❌ Commande inconnue: {command}")
            print_usage()
    else:
        # Par défaut, faire l'analyse
        analyze_password_hashes()


def print_usage():
    """Affiche l'aide d'utilisation"""
    print("\nUtilisation:")
    print("  python scripts/migrate_password_hashes.py [commande]")
    print("\nCommandes disponibles:")
    print("  analyze  - Analyser les hashes actuels (défaut)")
    print("  report   - Générer un rapport détaillé")
    print("  test     - Tester les fonctions de migration")
    print("\nExemples:")
    print("  python scripts/migrate_password_hashes.py analyze")
    print("  python scripts/migrate_password_hashes.py report")


if __name__ == "__main__":
    main()
```
---8<--- scripts/migrate_password_hashes.py END ---

---8<--- scripts/migrate_sqlite_to_postgres.py BEGIN ---
```py
"""
Script de migration SQLite → PostgreSQL pour GuignoMap v5.0
Copie toutes les données existantes de SQLite vers PostgreSQL
"""
import sqlite3
import sys
import os
from pathlib import Path

# Ajouter le répertoire parent au PYTHONPATH pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from src.database.connection import get_engine, execute_transaction
from src.database.models import Base, Street, Team, Note, ActivityLog, Address
from sqlalchemy.orm import sessionmaker
from datetime import datetime
import pandas as pd


def get_sqlite_connection():
    """Connexion en lecture seule à la base SQLite existante"""
    sqlite_path = Path(__file__).parent.parent / "guignomap" / "guigno_map.db"
    if not sqlite_path.exists():
        print(f"❌ Base SQLite non trouvée: {sqlite_path}")
        return None
    
    conn = sqlite3.connect(str(sqlite_path))
    conn.row_factory = sqlite3.Row
    return conn


def create_postgres_tables():
    """Créer les tables PostgreSQL via Alembic/SQLAlchemy"""
    try:
        engine = get_engine()
        Base.metadata.create_all(engine)
        print("✅ Tables PostgreSQL créées")
        return True
    except Exception as e:
        print(f"❌ Erreur création tables PostgreSQL: {e}")
        return False


def copy_teams(sqlite_conn, postgres_session):
    """Copier les équipes SQLite → PostgreSQL"""
    try:
        # Lire depuis SQLite
        teams_data = pd.read_sql_query("""
            SELECT id, name, password_hash, created_at, active 
            FROM teams 
            ORDER BY created_at
        """, sqlite_conn)
        
        if teams_data.empty:
            print("ℹ️ Aucune équipe à migrer")
            return 0
        
        # Insérer dans PostgreSQL
        count = 0
        for _, row in teams_data.iterrows():
            team = Team(
                id=row['id'],
                name=row['name'],
                password_hash=row['password_hash'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow(),
                active=bool(row['active'])
            )
            postgres_session.merge(team)  # merge pour éviter les doublons
            count += 1
        
        postgres_session.commit()
        print(f"✅ {count} équipes migrées")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"❌ Erreur migration équipes: {e}")
        return 0


def copy_streets(sqlite_conn, postgres_session):
    """Copier les rues SQLite → PostgreSQL"""
    try:
        # Lire depuis SQLite
        streets_data = pd.read_sql_query("""
            SELECT id, name, sector, team, status 
            FROM streets 
            ORDER BY id
        """, sqlite_conn)
        
        if streets_data.empty:
            print("ℹ️ Aucune rue à migrer")
            return 0
        
        # Insérer dans PostgreSQL
        count = 0
        for _, row in streets_data.iterrows():
            street = Street(
                id=row['id'] if row['id'] else None,
                name=row['name'],
                sector=row['sector'],
                team=row['team'],
                status=row['status'] or 'a_faire'
            )
            postgres_session.merge(street)
            count += 1
        
        postgres_session.commit()
        print(f"✅ {count} rues migrées")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"❌ Erreur migration rues: {e}")
        return 0


def copy_notes(sqlite_conn, postgres_session):
    """Copier les notes SQLite → PostgreSQL"""
    try:
        # Lire depuis SQLite
        notes_data = pd.read_sql_query("""
            SELECT id, street_name, team_id, address_number, comment, created_at 
            FROM notes 
            ORDER BY created_at
        """, sqlite_conn)
        
        if notes_data.empty:
            print("ℹ️ Aucune note à migrer")
            return 0
        
        # Insérer dans PostgreSQL
        count = 0
        for _, row in notes_data.iterrows():
            note = Note(
                id=row['id'] if row['id'] else None,
                street_name=row['street_name'],
                team_id=row['team_id'],
                address_number=row['address_number'],
                comment=row['comment'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow()
            )
            postgres_session.merge(note)
            count += 1
        
        postgres_session.commit()
        print(f"✅ {count} notes migrées")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"❌ Erreur migration notes: {e}")
        return 0


def copy_activity_logs(sqlite_conn, postgres_session):
    """Copier les logs d'activité SQLite → PostgreSQL"""
    try:
        # Vérifier si la table existe
        cursor = sqlite_conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='activity_log'
        """)
        if not cursor.fetchone():
            print("ℹ️ Table activity_log non présente dans SQLite")
            return 0
        
        # Lire depuis SQLite
        logs_data = pd.read_sql_query("""
            SELECT id, team_id, action, details, created_at 
            FROM activity_log 
            ORDER BY created_at
        """, sqlite_conn)
        
        if logs_data.empty:
            print("ℹ️ Aucun log d'activité à migrer")
            return 0
        
        # Insérer dans PostgreSQL
        count = 0
        for _, row in logs_data.iterrows():
            log = ActivityLog(
                id=row['id'] if row['id'] else None,
                team_id=row['team_id'],
                action=row['action'],
                details=row['details'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow()
            )
            postgres_session.merge(log)
            count += 1
        
        postgres_session.commit()
        print(f"✅ {count} logs d'activité migrés")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"❌ Erreur migration logs: {e}")
        return 0


def copy_addresses(sqlite_conn, postgres_session):
    """Copier les adresses OSM SQLite → PostgreSQL"""
    try:
        # Vérifier si la table existe
        cursor = sqlite_conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='addresses'
        """)
        if not cursor.fetchone():
            print("ℹ️ Table addresses non présente dans SQLite")
            return 0
        
        # Lire depuis SQLite
        addresses_data = pd.read_sql_query("""
            SELECT id, street_name, house_number, latitude, longitude, osm_type, created_at 
            FROM addresses 
            ORDER BY created_at
        """, sqlite_conn)
        
        if addresses_data.empty:
            print("ℹ️ Aucune adresse à migrer")
            return 0
        
        # Insérer dans PostgreSQL
        count = 0
        for _, row in addresses_data.iterrows():
            address = Address(
                id=row['id'] if row['id'] else None,
                street_name=row['street_name'],
                house_number=row['house_number'],
                latitude=row['latitude'],
                longitude=row['longitude'],
                osm_type=row['osm_type'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow()
            )
            postgres_session.merge(address)
            count += 1
        
        postgres_session.commit()
        print(f"✅ {count} adresses migrées")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"❌ Erreur migration adresses: {e}")
        return 0


def main():
    """Migration complète SQLite → PostgreSQL"""
    print("🔄 Début migration SQLite → PostgreSQL...")
    
    # Connexions
    sqlite_conn = get_sqlite_connection()
    if not sqlite_conn:
        return False
    
    try:
        # Créer les tables PostgreSQL
        if not create_postgres_tables():
            return False
        
        # Session PostgreSQL
        engine = get_engine()
        Session = sessionmaker(bind=engine)
        postgres_session = Session()
        
        # Migration par table
        total_migrated = 0
        total_migrated += copy_teams(sqlite_conn, postgres_session)
        total_migrated += copy_streets(sqlite_conn, postgres_session)
        total_migrated += copy_notes(sqlite_conn, postgres_session)
        total_migrated += copy_activity_logs(sqlite_conn, postgres_session)
        total_migrated += copy_addresses(sqlite_conn, postgres_session)
        
        postgres_session.close()
        sqlite_conn.close()
        
        print(f"🎉 Migration terminée ! {total_migrated} enregistrements migrés")
        return True
        
    except Exception as e:
        print(f"❌ Erreur générale migration: {e}")
        if 'postgres_session' in locals():
            postgres_session.close()
        sqlite_conn.close()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
```
---8<--- scripts/migrate_sqlite_to_postgres.py END ---

---8<--- scripts/validation_dataframe.ps1 BEGIN ---
```ps1
# =============================================================================
# SCRIPTS POWERSHELL - VALIDATION DATAFRAME
# =============================================================================

Write-Host "🔍 AUDIT DATAFRAME - SCRIPTS DE VALIDATION" -ForegroundColor Green
Write-Host "=========================================" -ForegroundColor Green

Write-Host "`n1️⃣ PATTERNS DATAFRAME PROBLÉMATIQUES" -ForegroundColor Yellow
Write-Host "Recherche: .columns, .iterrows, .empty, .loc[], .iloc[]" -ForegroundColor Gray
Select-String -Path .\guignomap\app.py -Pattern '\.columns|\.iterrows|\.empty|\.loc\[|\.iloc\[' -CaseSensitive | 
  Select-Object LineNumber, Line | Format-Table -Auto

Write-Host "`n2️⃣ ASSIGNATIONS DE FONCTIONS DB" -ForegroundColor Yellow  
Write-Host "Recherche: variables = db.fonction()" -ForegroundColor Gray
Select-String -Path .\guignomap\app.py -Pattern '\s*\w+\s*=\s*db\.\w+\(' -CaseSensitive |
  Select-Object LineNumber, Line | Format-Table -Auto

Write-Host "`n3️⃣ UTILISATION ST.DATAFRAME" -ForegroundColor Yellow
Write-Host "Recherche: st.dataframe() pour vérifier les types" -ForegroundColor Gray  
Select-String -Path .\guignomap\app.py -Pattern 'st\.dataframe\(' -CaseSensitive |
  Select-Object LineNumber, Line | Format-Table -Auto

Write-Host "`n4️⃣ FONCTIONS DB QUI RETOURNENT DES DONNÉES" -ForegroundColor Yellow
Write-Host "Recherche: Fonctions db_v5 qui peuvent retourner listes vs DataFrames" -ForegroundColor Gray
Select-String -Path .\guignomap\db_v5.py -Pattern 'def (list_|get_|stats_|teams|recent_)' -CaseSensitive |
  Select-Object LineNumber, Line | Format-Table -Auto

Write-Host "`n5️⃣ IMPORTS PANDAS DANS APP.PY" -ForegroundColor Yellow  
Write-Host "Recherche: import pandas et pd.DataFrame" -ForegroundColor Gray
Select-String -Path .\guignomap\app.py -Pattern 'import pandas|pd\.DataFrame' -CaseSensitive |
  Select-Object LineNumber, Line | Format-Table -Auto

Write-Host "`n✅ RAPPORT COMPLET GÉNÉRÉ DANS: AUDIT_DATAFRAME.md" -ForegroundColor Green
```
---8<--- scripts/validation_dataframe.ps1 END ---

---8<--- src/auth/passwords.py BEGIN ---
```py
"""
Gestion des mots de passe avec Argon2 pour GuignoMap v5.0
Migration compatible depuis bcrypt + politique UI inchangée (min 4 + confirmation)
"""
from passlib.context import CryptContext
from typing import Tuple
import bcrypt


# Configuration passlib avec Argon2 comme algorithme principal
# Garde bcrypt pour la compatibilité ascendante (lecture uniquement)
pwd_context = CryptContext(
    schemes=["argon2", "bcrypt"],
    deprecated="auto",  # Marque bcrypt comme obsolète
    argon2__rounds=2,   # Paramètres Argon2 pour performance/sécurité équilibrée
    argon2__memory_cost=65536,  # 64 MB
    argon2__parallelism=1,
    argon2__hash_len=32
)


def hash_password(password: str) -> str:
    """
    Hash un mot de passe avec Argon2
    
    Args:
        password: Mot de passe en texte clair
        
    Returns:
        Hash Argon2 du mot de passe
    """
    return pwd_context.hash(password)


def verify_password(password: str, hashed: str) -> Tuple[bool, bool]:
    """
    Vérifie un mot de passe contre son hash
    Supporte la migration automatique bcrypt → Argon2
    
    Args:
        password: Mot de passe en texte clair
        hashed: Hash stocké (bcrypt ou Argon2)
        
    Returns:
        Tuple (verification_ok, needs_rehash)
        - verification_ok: True si le mot de passe est correct
        - needs_rehash: True si le hash doit être mis à jour (migration paresseuse)
    """
    try:
        # Vérification avec passlib (supporte bcrypt et Argon2)
        verification_ok = pwd_context.verify(password, hashed)
        
        if verification_ok:
            # Vérifier si une mise à jour du hash est nécessaire
            needs_rehash = pwd_context.needs_update(hashed)
            return True, needs_rehash
        else:
            return False, False
            
    except Exception as e:
        print(f"Erreur vérification mot de passe: {e}")
        return False, False


def is_bcrypt_hash(hashed: str) -> bool:
    """
    Détermine si un hash est au format bcrypt
    
    Args:
        hashed: Hash à vérifier
        
    Returns:
        True si c'est un hash bcrypt
    """
    return hashed.startswith('$2b$') or hashed.startswith('$2a$') or hashed.startswith('$2y$')


def is_argon2_hash(hashed: str) -> bool:
    """
    Détermine si un hash est au format Argon2
    
    Args:
        hashed: Hash à vérifier
        
    Returns:
        True si c'est un hash Argon2
    """
    return hashed.startswith('$argon2')


def migrate_password_if_needed(password: str, old_hash: str) -> Tuple[bool, str]:
    """
    Migration paresseuse d'un mot de passe bcrypt vers Argon2
    Appelé lors d'une connexion réussie
    
    Args:
        password: Mot de passe en texte clair (fourni lors de la connexion)
        old_hash: Hash actuel stocké
        
    Returns:
        Tuple (migrated, new_hash)
        - migrated: True si une migration a eu lieu
        - new_hash: Nouveau hash Argon2 (ou old_hash si pas de migration)
    """
    verification_ok, needs_rehash = verify_password(password, old_hash)
    
    if verification_ok and needs_rehash:
        # Migration nécessaire : re-hasher avec Argon2
        new_hash = hash_password(password)
        print(f"🔄 Migration hash bcrypt → Argon2")
        return True, new_hash
    
    return False, old_hash


def validate_password_policy(password: str) -> Tuple[bool, str]:
    """
    Validation de la politique de mot de passe
    IMPORTANT: Garder la politique UI v4.1 (min 4 caractères + confirmation)
    
    Args:
        password: Mot de passe à valider
        
    Returns:
        Tuple (valid, error_message)
    """
    if not password:
        return False, "Le mot de passe est requis"
    
    if len(password) < 4:
        return False, "Le mot de passe doit contenir au moins 4 caractères"
    
    # Note: La confirmation est gérée côté UI, pas ici
    return True, ""


def get_password_hash_info(hashed: str) -> dict:
    """
    Informations sur un hash de mot de passe
    Utile pour diagnostics et migration
    
    Args:
        hashed: Hash à analyser
        
    Returns:
        Dictionnaire avec les informations du hash
    """
    info = {
        'algorithm': 'unknown',
        'needs_update': False,
        'is_bcrypt': is_bcrypt_hash(hashed),
        'is_argon2': is_argon2_hash(hashed)
    }
    
    try:
        if is_bcrypt_hash(hashed):
            info['algorithm'] = 'bcrypt'
            info['needs_update'] = True  # Tous les bcrypt doivent migrer
        elif is_argon2_hash(hashed):
            info['algorithm'] = 'argon2'
            info['needs_update'] = pwd_context.needs_update(hashed)
        
        # Informations supplémentaires via passlib
        hash_info = pwd_context.identify(hashed)
        if hash_info:
            info['passlib_scheme'] = hash_info
            
    except Exception as e:
        info['error'] = str(e)
    
    return info


# Fonctions de compatibilité avec l'ancien système bcrypt
def legacy_verify_bcrypt(password: str, bcrypt_hash: str) -> bool:
    """
    Vérification directe bcrypt pour rétrocompatibilité
    Utilisé uniquement si passlib échoue
    
    Args:
        password: Mot de passe en texte clair
        bcrypt_hash: Hash bcrypt à vérifier
        
    Returns:
        True si le mot de passe correspond
    """
    try:
        return bcrypt.checkpw(password.encode('utf-8'), bcrypt_hash.encode('utf-8'))
    except Exception as e:
        print(f"Erreur vérification bcrypt legacy: {e}")
        return False


def create_test_hashes(password: str = "test123") -> dict:
    """
    Utilitaire pour créer des hashes de test
    Aide au développement et aux tests
    
    Args:
        password: Mot de passe de test
        
    Returns:
        Dictionnaire avec les différents hashes
    """
    return {
        'password': password,
        'argon2': hash_password(password),
        'bcrypt_legacy': bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
    }
```
---8<--- src/auth/passwords.py END ---

---8<--- src/config.py BEGIN ---
```py
"""
Configuration centralisée pour GuignoMap v5.0
Accès aux secrets Streamlit et paramètres applicatifs
"""
import streamlit as st
import os


def get_database_url():
    """Récupère l'URL de la base de données depuis les secrets"""
    try:
        return st.secrets["database"]["url"]
    except (KeyError, AttributeError):
        # Fallback pour développement local ou tests
        return os.getenv("DATABASE_URL", "sqlite:///guigno_map.db")


def get_database_pool_config():
    """Configuration du pool de connexions PostgreSQL"""
    try:
        return {
            "pool_size": st.secrets["database"].get("pool_size", 5),
            "max_overflow": st.secrets["database"].get("max_overflow", 10)
        }
    except (KeyError, AttributeError):
        return {"pool_size": 5, "max_overflow": 10}


def get_s3_config():
    """Configuration S3 pour le stockage cloud"""
    try:
        return {
            "bucket": st.secrets["storage"]["s3_bucket"],
            "region": st.secrets["storage"]["s3_region"],
            "access_key": st.secrets["storage"]["s3_access_key"],
            "secret_key": st.secrets["storage"]["s3_secret_key"]
        }
    except (KeyError, AttributeError):
        return {
            "bucket": os.getenv("S3_BUCKET", "guignomap-dev"),
            "region": os.getenv("S3_REGION", "us-east-1"),
            "access_key": os.getenv("S3_ACCESS_KEY", ""),
            "secret_key": os.getenv("S3_SECRET_KEY", "")
        }


def get_cdn_base_url():
    """URL de base CDN pour les assets (optionnel)"""
    try:
        return st.secrets["storage"].get("cdn_base_url", "")
    except (KeyError, AttributeError):
        return os.getenv("CDN_BASE_URL", "")
```
---8<--- src/config.py END ---

---8<--- src/database/connection.py BEGIN ---
```py
"""
Connexion PostgreSQL avec SQLAlchemy pour GuignoMap v5.0
Engine + QueuePool + cache Streamlit + retry logic
"""
import time
import functools
import streamlit as st
from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError
from src.config import get_database_url, get_database_pool_config


def db_retry(max_retries=3, backoff_factor=1):
    """
    Décorateur retry exponentiel pour opérations DB critiques
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except SQLAlchemyError as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        wait_time = backoff_factor * (2 ** attempt)
                        print(f"Retry DB operation {func.__name__} in {wait_time}s (attempt {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"Max retries reached for {func.__name__}")
                        break
            raise last_exception
        return wrapper
    return decorator


@st.cache_resource
def get_engine():
    """
    Engine PostgreSQL avec cache Streamlit et configuration pool
    Conformément au plan v5.0
    """
    database_url = get_database_url()
    pool_config = get_database_pool_config()
    
    # Configuration PostgreSQL avec QueuePool
    engine = create_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=pool_config["pool_size"],
        max_overflow=pool_config["max_overflow"],
        pool_pre_ping=True,
        pool_recycle=300,
        echo=False  # Set to True for SQL debugging
    )
    
    return engine


def get_session():
    """Fabrique de session SQLAlchemy"""
    engine = get_engine()
    Session = sessionmaker(bind=engine)
    return Session()


@db_retry(max_retries=3)
def test_connection():
    """Test de connexion à la base PostgreSQL"""
    try:
        engine = get_engine()
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1 as test"))
            return result.fetchone()[0] == 1
    except Exception as e:
        print(f"Erreur test connexion DB: {e}")
        return False


@db_retry(max_retries=3)
def execute_query(query, params=None):
    """
    Exécution de requête avec retry automatique
    Pour transition progressive vers SQLAlchemy
    """
    engine = get_engine()
    with engine.connect() as conn:
        if params:
            return conn.execute(text(query), params)
        else:
            return conn.execute(text(query))


@db_retry(max_retries=3)  
def execute_transaction(queries_and_params):
    """
    Exécution de transaction multi-requêtes avec retry
    queries_and_params: liste de tuples (query, params)
    """
    engine = get_engine()
    with engine.begin() as conn:
        results = []
        for query, params in queries_and_params:
            if params:
                result = conn.execute(text(query), params)
            else:
                result = conn.execute(text(query))
            results.append(result)
        return results
```
---8<--- src/database/connection.py END ---

---8<--- src/database/db_v5.py BEGIN ---
```py
"""
GuignoMap v5.0 - Database operation            admin_exists = session.execute(
                text("SELECT COUNT(*) FROM teams WHERE id = 'ADMIN'")
            ).scalar() or 0
            
            if admin_exists > 0:h SQLAlchemy
Migration from raw sqlite3 to SQLAlchemy + PostgreSQL support
"""
import os
import pandas as pd
import hashlib
import bcrypt
from datetime import datetime
import json
from pathlib import Path
import secrets
import string
from typing import Optional, List, Dict, Any

from sqlalchemy import text, and_, or_
from src.database.connection import get_session, db_retry
from src.database.models import Street, Team, Note, ActivityLog, Address
from backup import auto_backup_before_critical, BackupManager
from validators import validate_and_clean_input, InputValidator


# =============================================================================
# CONFIGURATION & CONSTANTES
# =============================================================================

# Schéma de migration - utilisé pour vérifier les tables existantes
REQUIRED_TABLES = ['streets', 'teams', 'notes', 'activity_log', 'addresses']


# =============================================================================
# FONCTIONS DE CONNEXION ET INITIALISATION
# =============================================================================

@db_retry(max_retries=3)
def init_db():
    """Initialise la base de données avec les données initiales"""
    try:
        with get_session() as session:
            # Vérifier si admin existe
            admin_exists = session.execute(
                text("SELECT COUNT(*) FROM teams WHERE id = 'ADMIN'")
            ).scalar()
            
            if admin_exists == 0:
                pwd = os.getenv("GM_ADMIN_PWD", "RELAIS2025")
                create_team('ADMIN', 'Superviseur', pwd)
            
            # Auto-import des rues si vide
            streets_count = session.execute(
                text("SELECT COUNT(*) FROM streets")
            ).scalar()
            
            if streets_count == 0:
                print("🔄 Aucune rue trouvée. Import automatique depuis OpenStreetMap...")
                auto_import_streets()
                
    except Exception as e:
        print(f"❌ Erreur init_db: {e}")
        raise


def auto_import_streets():
    """Import automatique des rues depuis OSM cache"""
    try:
        from osm import load_geometry_cache
        
        with get_session() as session:
            cache = load_geometry_cache()
            if not cache:
                print("⚠️ Aucun cache OSM trouvé. Utilisez 'Construire carte' dans l'admin.")
                return
            
            imported = 0
            for street_name in cache.keys():
                if street_name and street_name.strip():
                    # Vérifier si existe déjà
                    exists = session.execute(
                        text("SELECT COUNT(*) FROM streets WHERE name = :name"),
                        {"name": street_name.strip()}
                    ).scalar()
                    
                    if exists == 0:
                        session.execute(text("""
                            INSERT INTO streets (name, status) 
                            VALUES (:name, 'a_faire')
                        """), {"name": street_name.strip()})
                        imported += 1
            
            session.commit()
            print(f"✅ {imported} rues importées depuis OSM")
            
    except Exception as e:
        print(f"❌ Erreur auto_import_streets: {e}")


# =============================================================================
# GESTION DES ÉQUIPES ET AUTHENTIFICATION
# =============================================================================

def hash_password(password: str) -> str:
    """Hash un mot de passe avec bcrypt"""
    return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')


@db_retry(max_retries=2)
def create_team(team_id: str, name: str, password: str) -> bool:
    """Crée une nouvelle équipe"""
    try:
        with get_session() as session:
            # Vérifier si l'équipe existe déjà
            exists = session.execute(
                text("SELECT COUNT(*) FROM teams WHERE id = :id"),
                {"id": team_id}
            ).scalar() or 0
            
            if exists > 0:
                return False
            
            password_hash = hash_password(password)
            session.execute(text("""
                INSERT INTO teams (id, name, password_hash, created_at, active)
                VALUES (:id, :name, :hash, CURRENT_TIMESTAMP, 1)
            """), {
                "id": team_id,
                "name": name, 
                "hash": password_hash
            })
            session.commit()
            
            # Log de l'activité
            log_activity(session, team_id, 'create_team', f"Équipe '{name}' créée")
            
            return True
            
    except Exception as e:
        print(f"❌ Erreur create_team: {e}")
        return False


@db_retry(max_retries=2)
def verify_team(team_id: str, password: str) -> bool:
    """Vérifie les identifiants d'une équipe"""
    try:
        with get_session() as session:
            result = session.execute(
                text("SELECT password_hash FROM teams WHERE id = :id AND active = 1"),
                {"id": team_id}
            ).first()
            
            if not result:
                return False
            
            stored_hash = result[0]
            
            # Support bcrypt (nouveau) et MD5 legacy (migration)
            if stored_hash.startswith('$2b$'):
                # bcrypt
                return bcrypt.checkpw(password.encode('utf-8'), stored_hash.encode('utf-8'))
            else:
                # MD5 legacy - migrer automatiquement
                if hashlib.md5(password.encode()).hexdigest() == stored_hash:
                    # Migrer vers bcrypt
                    new_hash = hash_password(password)
                    session.execute(
                        text("UPDATE teams SET password_hash = :hash WHERE id = :id"),
                        {"hash": new_hash, "id": team_id}
                    )
                    session.commit()
                    return True
                return False
                
    except Exception as e:
        print(f"❌ Erreur verify_team: {e}")
        return False


def get_all_teams() -> List[Dict[str, Any]]:
    """Récupère toutes les équipes actives"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT id, name, created_at, 
                       (SELECT COUNT(*) FROM streets WHERE team = teams.id) as assigned_streets
                FROM teams 
                WHERE active = 1 
                ORDER BY name
            """))
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"❌ Erreur get_all_teams: {e}")
        return []


def teams() -> List[str]:
    """Récupère la liste des IDs d'équipes actives"""
    try:
        with get_session() as session:
            result = session.execute(
                text("SELECT id FROM teams WHERE active = 1 ORDER BY name")
            )
            return [row[0] for row in result]
            
    except Exception as e:
        print(f"❌ Erreur teams: {e}")
        return []


@auto_backup_before_critical
def delete_team(team_id: str) -> bool:
    """Supprime une équipe (soft delete)"""
    try:
        with get_session() as session:
            session.execute(
                text("UPDATE teams SET active = 0 WHERE id = :id"),
                {"id": team_id}
            )
            session.commit()
            return True
            
    except Exception as e:
        print(f"❌ Erreur delete_team: {e}")
        return False


# =============================================================================
# GESTION DES RUES ET STATUTS  
# =============================================================================

def list_streets(team: Optional[str] = None) -> pd.DataFrame:
    """Liste les rues avec filtrage optionnel par équipe"""
    try:
        with get_session() as session:
            if team:
                query = text("""
                    SELECT id, name, sector, team, status
                    FROM streets 
                    WHERE team = :team
                    ORDER BY name
                """)
                result = session.execute(query, {"team": team})
            else:
                query = text("""
                    SELECT id, name, sector, team, status
                    FROM streets 
                    ORDER BY name
                """)
                result = session.execute(query)
            
            # Convertir en DataFrame
            rows = [dict(row._mapping) for row in result]
            return pd.DataFrame(rows) if rows else pd.DataFrame(columns=['id', 'name', 'sector', 'team', 'status'])
            
    except Exception as e:
        print(f"❌ Erreur list_streets: {e}")
        return pd.DataFrame(columns=['id', 'name', 'sector', 'team', 'status'])


def get_unassigned_streets() -> List[str]:
    """Récupère les rues non assignées à une équipe"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT name FROM streets 
                WHERE team IS NULL OR team = ''
                ORDER BY name
            """))
            return [row[0] for row in result]
            
    except Exception as e:
        print(f"❌ Erreur get_unassigned_streets: {e}")
        return []


@auto_backup_before_critical
def assign_streets_to_team(street_names: List[str], team_id: str) -> int:
    """Assigne plusieurs rues à une équipe"""
    try:
        with get_session() as session:
            count = 0
            for street_name in street_names:
                # Vérifier si la rue existe et n'est pas assignée
                existing = session.execute(text("""
                    SELECT COUNT(*) FROM streets 
                    WHERE name = :name AND (team IS NULL OR team = '')
                """), {"name": street_name}).scalar() or 0
                
                if existing > 0:
                    session.execute(text("""
                        UPDATE streets 
                        SET team = :team 
                        WHERE name = :name AND (team IS NULL OR team = '')
                    """), {"team": team_id, "name": street_name})
                    count += 1
            
            session.commit()
            
            # Log de l'activité
            if count > 0:
                log_activity(session, team_id, 'assign_streets', 
                           f"{count} rues assignées à l'équipe")
            
            return count
            
    except Exception as e:
        print(f"❌ Erreur assign_streets_to_team: {e}")
        return 0


@auto_backup_before_critical
def set_status(name: str, status: str) -> bool:
    """Met à jour le statut d'une rue"""
    try:
        # Validation du statut
        valid_statuses = ['a_faire', 'en_cours', 'terminee']
        if status not in valid_statuses:
            return False
        
        with get_session() as session:
            # Vérifier si la rue existe
            exists = session.execute(
                text("SELECT COUNT(*) FROM streets WHERE name = :name"),
                {"name": name}
            ).scalar() or 0
            
            if exists > 0:
                session.execute(text("""
                    UPDATE streets 
                    SET status = :status 
                    WHERE name = :name
                """), {"status": status, "name": name})
                
                session.commit()
                
                # Log de l'activité
                team = session.execute(
                    text("SELECT team FROM streets WHERE name = :name"),
                    {"name": name}
                ).scalar()
                
                if team:
                    log_activity(session, team, 'status_change', 
                               f"Rue '{name}' -> {status}")
                
                return True
            return False
            
    except Exception as e:
        print(f"❌ Erreur set_status: {e}")
        return False


# =============================================================================
# GESTION DES NOTES ET ADRESSES
# =============================================================================

@auto_backup_before_critical
def add_note_for_address(street_name: str, team_id: str, address_number: str, comment: str) -> bool:
    """Ajoute une note pour une adresse spécifique"""
    try:
        # Validation et nettoyage
        _, comment = validate_and_clean_input("comment", comment)
        _, address_number = validate_and_clean_input("address_number", address_number)
        
        with get_session() as session:
            session.execute(text("""
                INSERT INTO notes (street_name, team_id, address_number, comment, created_at)
                VALUES (:street, :team, :addr, :comment, CURRENT_TIMESTAMP)
            """), {
                "street": street_name,
                "team": team_id,
                "addr": address_number,
                "comment": comment
            })
            session.commit()
            
            # Log de l'activité
            log_activity(session, team_id, 'add_note', 
                       f"Note ajoutée: {street_name} #{address_number}")
            
            return True
            
    except Exception as e:
        print(f"❌ Erreur add_note_for_address: {e}")
        return False


def get_street_addresses_with_notes(street_name: str) -> List[Dict[str, Any]]:
    """Récupère les adresses avec notes pour une rue"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT address_number, comment, team_id, created_at
                FROM notes 
                WHERE street_name = :street
                ORDER BY CAST(address_number AS INTEGER), created_at DESC
            """), {"street": street_name})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"❌ Erreur get_street_addresses_with_notes: {e}")
        return []


def get_team_notes(team_id: str) -> List[Dict[str, Any]]:
    """Récupère toutes les notes d'une équipe"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT street_name, address_number, comment, created_at
                FROM notes 
                WHERE team_id = :team
                ORDER BY created_at DESC
            """), {"team": team_id})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"❌ Erreur get_team_notes: {e}")
        return []


# =============================================================================
# STATISTIQUES ET RAPPORTS
# =============================================================================

def extended_stats() -> Dict[str, Any]:
    """Statistiques étendues de l'application"""
    try:
        with get_session() as session:
            # Stats de base
            total_streets = session.execute(text("SELECT COUNT(*) FROM streets")).scalar() or 0
            assigned_streets = session.execute(text("SELECT COUNT(*) FROM streets WHERE team IS NOT NULL AND team != ''")).scalar() or 0
            completed_streets = session.execute(text("SELECT COUNT(*) FROM streets WHERE status = 'terminee'")).scalar() or 0
            in_progress_streets = session.execute(text("SELECT COUNT(*) FROM streets WHERE status = 'en_cours'")).scalar() or 0
            
            # Stats par statut
            status_counts = session.execute(text("""
                SELECT status, COUNT(*) as count
                FROM streets 
                GROUP BY status
            """))
            status_data = {row[0]: row[1] for row in status_counts}
            
            return {
                'total_streets': total_streets,
                'assigned_streets': assigned_streets,
                'unassigned_streets': total_streets - assigned_streets,
                'completed_streets': completed_streets,
                'in_progress_streets': in_progress_streets,
                'todo_streets': total_streets - completed_streets - in_progress_streets,
                'completion_rate': (completed_streets / total_streets * 100) if total_streets > 0 else 0,
                'status_breakdown': status_data
            }
            
    except Exception as e:
        print(f"❌ Erreur extended_stats: {e}")
        return {}


def stats_by_team() -> List[Dict[str, Any]]:
    """Statistiques par équipe"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT 
                    t.id,
                    t.name,
                    COUNT(s.id) as total_streets,
                    SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as completed,
                    SUM(CASE WHEN s.status = 'en_cours' THEN 1 ELSE 0 END) as in_progress,
                    SUM(CASE WHEN s.status = 'a_faire' THEN 1 ELSE 0 END) as todo
                FROM teams t
                LEFT JOIN streets s ON s.team = t.id
                WHERE t.active = 1
                GROUP BY t.id, t.name
                ORDER BY t.name
            """))
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"❌ Erreur stats_by_team: {e}")
        return []


def recent_activity(limit: int = 10) -> List[Dict[str, Any]]:
    """Activité récente dans l'application"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT team_id, action, details, created_at
                FROM activity_log 
                ORDER BY created_at DESC 
                LIMIT :limit
            """), {"limit": limit})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"❌ Erreur recent_activity: {e}")
        return []


def export_to_csv() -> str:
    """Exporte les données vers CSV"""
    try:
        from datetime import datetime
        
        df = list_streets()
        if df.empty:
            return ""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        export_dir = Path(__file__).parent.parent / "exports"
        export_dir.mkdir(exist_ok=True)
        
        filename = f"guignomap_export_{timestamp}.csv"
        filepath = export_dir / filename
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        return str(filepath)
        
    except Exception as e:
        print(f"❌ Erreur export_to_csv: {e}")
        return ""


# =============================================================================
# LOG D'ACTIVITÉ
# =============================================================================

def log_activity(session, team_id: str, action: str, details: str):
    """Log une activité dans la base de données"""
    try:
        session.execute(text("""
            INSERT INTO activity_log (team_id, action, details, created_at)
            VALUES (:team, :action, :details, CURRENT_TIMESTAMP)
        """), {
            "team": team_id,
            "action": action,
            "details": details
        })
        # Note: commit fait par la fonction appelante
        
    except Exception as e:
        print(f"❌ Erreur log_activity: {e}")


# =============================================================================
# FONCTIONS MANQUANTES POUR COMPATIBILITÉ APP.PY
# =============================================================================

def get_team_streets(team_id: str) -> List[Dict[str, Any]]:
    """Récupère les rues assignées à une équipe avec tous les détails"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT id, name, sector, team, status
                FROM streets 
                WHERE team = :team 
                ORDER BY name
            """), {"team": team_id})
            return [dict(row._mapping) for row in result]
    except Exception as e:
        print(f"❌ Erreur get_team_streets: {e}")
        return []


def get_unassigned_streets_count() -> int:
    """Compte les rues non assignées"""
    try:
        with get_session() as session:
            count = session.execute(text("""
                SELECT COUNT(*) FROM streets 
                WHERE team IS NULL OR team = ''
            """)).scalar() or 0
            return count
    except Exception as e:
        print(f"❌ Erreur get_unassigned_streets_count: {e}")
        return 0


def get_sectors_list() -> List[str]:
    """Récupère la liste des secteurs"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT DISTINCT sector FROM streets 
                WHERE sector IS NOT NULL AND sector != ''
                ORDER BY sector
            """))
            return [row[0] for row in result]
    except Exception as e:
        print(f"❌ Erreur get_sectors_list: {e}")
        return []


def get_teams_list() -> List[str]:
    """Récupère la liste des équipes (alias pour teams())"""
    return teams()


def bulk_assign_sector(sector: str, team_id: str) -> int:
    """Assigne toutes les rues d'un secteur à une équipe"""
    try:
        with get_session() as session:
            # Récupérer les rues non assignées du secteur
            result = session.execute(text("""
                SELECT name FROM streets 
                WHERE sector = :sector AND (team IS NULL OR team = '')
            """), {"sector": sector})
            
            street_names = [row[0] for row in result]
            
            if street_names:
                return assign_streets_to_team(street_names, team_id)
            return 0
            
    except Exception as e:
        print(f"❌ Erreur bulk_assign_sector: {e}")
        return 0


def get_assignations_export_data() -> List[Dict[str, Any]]:
    """Données pour export des assignations"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT s.name as street_name, s.sector, s.team, s.status,
                       t.name as team_name
                FROM streets s
                LEFT JOIN teams t ON s.team = t.id
                ORDER BY s.name
            """))
            return [dict(row._mapping) for row in result]
    except Exception as e:
        print(f"❌ Erreur get_assignations_export_data: {e}")
        return []


def export_notes_csv() -> str:
    """Exporte les notes vers CSV"""
    try:
        from datetime import datetime
        
        with get_session() as session:
            result = session.execute(text("""
                SELECT n.street_name, n.team_id, n.address_number, 
                       n.comment, n.created_at,
                       t.name as team_name
                FROM notes n
                LEFT JOIN teams t ON n.team_id = t.id
                ORDER BY n.created_at DESC
            """))
            
            if not result:
                return ""
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            export_dir = Path(__file__).parent.parent / "exports"
            export_dir.mkdir(exist_ok=True)
            
            filename = f"notes_export_{timestamp}.csv"
            filepath = export_dir / filename
            
            import pandas as pd
            df = pd.DataFrame([dict(row._mapping) for row in result])
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            return str(filepath)
            
    except Exception as e:
        print(f"❌ Erreur export_notes_csv: {e}")
        return ""


def import_addresses_from_cache(addr_cache: Dict) -> int:
    """Importe les adresses depuis le cache OSM"""
    try:
        with get_session() as session:
            imported = 0
            
            for street_name, addresses in addr_cache.items():
                if isinstance(addresses, list):
                    for addr in addresses:
                        # Insérer l'adresse si elle n'existe pas
                        exists = session.execute(text("""
                            SELECT COUNT(*) FROM addresses 
                            WHERE street_name = :street AND house_number = :num
                        """), {"street": street_name, "num": str(addr)}).scalar() or 0
                        
                        if exists == 0:
                            session.execute(text("""
                                INSERT INTO addresses (street_name, house_number)
                                VALUES (:street, :num)
                            """), {"street": street_name, "num": str(addr)})
                            imported += 1
            
            session.commit()
            return imported
            
    except Exception as e:
        print(f"❌ Erreur import_addresses_from_cache: {e}")
        return 0


def update_street_status(street_name: str, status: str, team_id: str) -> bool:
    """Met à jour le statut d'une rue (alias pour set_status)"""
    return set_status(street_name, status)


def get_street_notes_for_team(street_name: str, team_id: str) -> List[Dict[str, Any]]:
    """Récupère les notes d'une rue pour une équipe spécifique"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT address_number, comment, created_at
                FROM notes 
                WHERE street_name = :street AND team_id = :team
                ORDER BY created_at DESC
            """), {"street": street_name, "team": team_id})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"❌ Erreur get_street_notes_for_team: {e}")
        return []


def add_street_note(street_name: str, team_id: str, address_number: str, comment: str) -> bool:
    """Ajoute une note pour une rue (alias pour add_note_for_address)"""
    return add_note_for_address(street_name, team_id, address_number, comment)


# =============================================================================
# COMPATIBILITÉ LEGACY
# =============================================================================

def get_backup_manager(db_path=None):
    """Compatibilité avec backup.py - retourne le BackupManager"""
    # Pour l'instant, utilise encore l'ancien système de backup
    # TODO: Migrer le backup vers SQLAlchemy dans Phase 2
    if db_path is None:
        db_path = Path(__file__).parent / "guigno_map.db"
    return BackupManager(db_path)


# =============================================================================
# MIGRATION PASSWORD LEGACY
# =============================================================================

def migrate_all_passwords_to_bcrypt():
    """Migre tous les mots de passe MD5 vers bcrypt"""
    try:
        with get_session() as session:
            # Récupérer toutes les équipes avec hash MD5
            result = session.execute(text("""
                SELECT id, password_hash 
                FROM teams 
                WHERE password_hash NOT LIKE '$2b$%' AND active = 1
            """))
            
            migrated = 0
            for row in result:
                team_id, old_hash = row
                print(f"⚠️ Équipe {team_id} a un hash MD5 legacy")
                print("La migration automatique se fera lors de la prochaine connexion")
                # Note: la migration se fait automatiquement dans verify_team()
                
            print(f"✅ {migrated} mots de passe à migrer détectés")
            
    except Exception as e:
        print(f"❌ Erreur migrate_all_passwords_to_bcrypt: {e}")
```
---8<--- src/database/db_v5.py END ---

---8<--- src/database/migrations/env.py BEGIN ---
```py
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# Import our models and config
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from src.database.models import Base
from src.config import get_database_url

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = get_database_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Use our custom database URL instead of config
    configuration = config.get_section(config.config_ini_section, {})
    configuration["sqlalchemy.url"] = get_database_url()
    
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

```
---8<--- src/database/migrations/env.py END ---

---8<--- src/database/models.py BEGIN ---
```py
"""
Modèles SQLAlchemy pour GuignoMap v5.0
Basés sur le schéma SQLite existant pour compatibilité
"""
from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, ForeignKey, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()


class Street(Base):
    __tablename__ = 'streets'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(Text, nullable=False, unique=True)
    sector = Column(Text)
    team = Column(Text)
    status = Column(Text, nullable=False, default='a_faire')
    
    # Relations
    notes = relationship("Note", back_populates="street", cascade="all, delete-orphan")
    addresses = relationship("Address", back_populates="street", cascade="all, delete-orphan")


class Team(Base):
    __tablename__ = 'teams'
    
    id = Column(Text, primary_key=True)
    name = Column(Text, nullable=False)
    password_hash = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    active = Column(Boolean, default=True)


class Note(Base):
    __tablename__ = 'notes'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    team_id = Column(Text, ForeignKey('teams.id'), nullable=False)
    address_number = Column(Text)
    comment = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="notes")


class ActivityLog(Base):
    __tablename__ = 'activity_log'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    team_id = Column(Text)
    action = Column(Text, nullable=False)
    details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)


class Address(Base):
    __tablename__ = 'addresses'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    house_number = Column(Text, nullable=False)
    latitude = Column(Float)
    longitude = Column(Float)
    osm_type = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="addresses")
```
---8<--- src/database/models.py END ---

---8<--- src/storage/__init__.py BEGIN ---
```py
"""
Adapter de stockage pour GuignoMap v5.0
Sélection automatique entre cloud S3 et local selon configuration
"""
import os
from typing import Optional, Dict, Any
from pathlib import Path

try:
    from src.storage.cloud import (
        upload_osm_cache as cloud_upload_osm_cache,
        download_osm_cache as cloud_download_osm_cache,
        upload_backup_to_cloud as cloud_upload_backup,
        list_cloud_backups as cloud_list_backups,
        download_backup_from_cloud as cloud_download_backup
    )
    CLOUD_AVAILABLE = True
except ImportError as e:
    print(f"⚠️ Stockage cloud non disponible: {e}")
    CLOUD_AVAILABLE = False

from src.storage.local import (
    upload_osm_cache as local_upload_osm_cache,
    download_osm_cache as local_download_osm_cache,
    upload_backup_to_cloud as local_upload_backup,
    list_cloud_backups as local_list_backups,
    download_backup_from_cloud as local_download_backup
)


def is_cloud_storage_enabled() -> bool:
    """
    Détermine si le stockage cloud est activé
    Vérifie la présence des secrets S3 et la disponibilité des libs
    """
    if not CLOUD_AVAILABLE:
        return False
    
    try:
        from src.config import get_s3_config
        config = get_s3_config()
        
        # Vérifier que les clés essentielles sont présentes et non vides
        required_keys = ['bucket', 'access_key', 'secret_key']
        for key in required_keys:
            if not config.get(key) or config[key] in ['', 'xxx']:
                return False
        
        return True
    except Exception:
        return False


def get_storage_backend() -> str:
    """Retourne 'cloud' ou 'local' selon la configuration"""
    return 'cloud' if is_cloud_storage_enabled() else 'local'


# API unifiée pour le stockage
def upload_osm_cache(cache_data: Dict[Any, Any]) -> bool:
    """Upload du cache OSM (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("📡 Upload cache OSM vers S3...")
        return cloud_upload_osm_cache(cache_data)
    else:
        print("💾 Sauvegarde cache OSM en local...")
        return local_upload_osm_cache(cache_data)


def download_osm_cache() -> Optional[Dict[Any, Any]]:
    """Download du cache OSM (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("📡 Téléchargement cache OSM depuis S3...")
        return cloud_download_osm_cache()
    else:
        print("💾 Lecture cache OSM local...")
        return local_download_osm_cache()


def upload_backup(backup_path: Path) -> bool:
    """Upload d'un backup (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("📡 Upload backup vers S3...")
        return cloud_upload_backup(backup_path)
    else:
        print("💾 Copie backup en local...")
        return local_upload_backup(backup_path)


def list_backups() -> list:
    """Liste des backups disponibles (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("📡 Liste backups S3...")
        return cloud_list_backups()
    else:
        print("💾 Liste backups locaux...")
        return local_list_backups()


def download_backup(backup_key: str, local_path: Path) -> bool:
    """Download d'un backup (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("📡 Téléchargement backup depuis S3...")
        return cloud_download_backup(backup_key, local_path)
    else:
        print("💾 Copie backup depuis local...")
        return local_download_backup(backup_key, local_path)


def get_storage_info() -> Dict[str, Any]:
    """Informations sur le backend de stockage actuel"""
    backend = get_storage_backend()
    info = {
        'backend': backend,
        'cloud_available': CLOUD_AVAILABLE,
        'cloud_enabled': is_cloud_storage_enabled()
    }
    
    if backend == 'cloud':
        try:
            from src.config import get_s3_config
            config = get_s3_config()
            info.update({
                'bucket': config.get('bucket', ''),
                'region': config.get('region', ''),
                'cdn_enabled': bool(config.get('cdn_base_url', ''))
            })
        except:
            pass
    
    return info
```
---8<--- src/storage/__init__.py END ---

---8<--- src/storage/cloud.py BEGIN ---
```py
"""
Stockage cloud S3 pour GuignoMap v5.0
Client boto3 pour osm_cache.json et backups
"""
import boto3
import json
import io
import os
import streamlit as st
from typing import Optional, Dict, Any, BinaryIO
from pathlib import Path
from datetime import datetime
from src.config import get_s3_config, get_cdn_base_url


class S3StorageClient:
    """Client S3 pour gérer osm_cache.json et backups"""
    
    def __init__(self):
        self.config = get_s3_config()
        self.cdn_base_url = get_cdn_base_url()
        self._client = None
    
    @property
    def client(self):
        """Client S3 avec lazy loading et cache Streamlit"""
        if self._client is None:
            try:
                self._client = boto3.client(
                    's3',
                    region_name=self.config['region'],
                    aws_access_key_id=self.config['access_key'],
                    aws_secret_access_key=self.config['secret_key']
                )
            except Exception as e:
                print(f"Erreur initialisation client S3: {e}")
                raise
        return self._client
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """
        Upload d'un fichier JSON vers S3
        """
        try:
            json_content = json.dumps(data, ensure_ascii=False, indent=2)
            json_bytes = json_content.encode('utf-8')
            
            extra_args = {
                'ContentType': 'application/json',
                'ContentEncoding': 'utf-8'
            }
            
            if metadata:
                extra_args['Metadata'] = metadata
            
            self.client.put_object(
                Bucket=self.config['bucket'],
                Key=key,
                Body=json_bytes,
                **extra_args
            )
            
            print(f"✅ JSON uploadé vers S3: {key}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur upload JSON S3 {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """
        Download d'un fichier JSON depuis S3
        """
        try:
            response = self.client.get_object(
                Bucket=self.config['bucket'],
                Key=key
            )
            
            content = response['Body'].read().decode('utf-8')
            data = json.loads(content)
            
            print(f"✅ JSON téléchargé depuis S3: {key}")
            return data
            
        except self.client.exceptions.NoSuchKey:
            print(f"ℹ️ Fichier JSON S3 non trouvé: {key}")
            return None
        except Exception as e:
            print(f"❌ Erreur download JSON S3 {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """
        Upload d'un fichier backup vers S3
        """
        try:
            if not backup_file_path.exists():
                print(f"❌ Fichier backup non trouvé: {backup_file_path}")
                return False
            
            # Générer la clé S3 si non fournie
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                s3_key = f"backups/{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            
            # Upload avec streaming pour gros fichiers
            with open(backup_file_path, 'rb') as f:
                self.client.upload_fileobj(
                    f,
                    self.config['bucket'],
                    s3_key,
                    ExtraArgs={
                        'ContentType': 'application/zip',
                        'Metadata': {
                            'original_filename': backup_file_path.name,
                            'upload_timestamp': datetime.utcnow().isoformat()
                        }
                    }
                )
            
            print(f"✅ Backup uploadé vers S3: {s3_key}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur upload backup S3: {e}")
            return False
    
    def download_backup(self, s3_key: str, local_path: Path) -> bool:
        """
        Download d'un backup depuis S3
        """
        try:
            # Créer le répertoire parent si nécessaire
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Download avec streaming
            with open(local_path, 'wb') as f:
                self.client.download_fileobj(
                    self.config['bucket'],
                    s3_key,
                    f
                )
            
            print(f"✅ Backup téléchargé depuis S3: {s3_key} → {local_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur download backup S3 {s3_key}: {e}")
            return False
    
    def list_backups(self, prefix: str = "backups/") -> list:
        """
        Liste des backups disponibles sur S3
        """
        try:
            response = self.client.list_objects_v2(
                Bucket=self.config['bucket'],
                Prefix=prefix
            )
            
            backups = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    backups.append({
                        'key': obj['Key'],
                        'size': obj['Size'],
                        'last_modified': obj['LastModified'],
                        'filename': Path(obj['Key']).name
                    })
                
                # Trier par date de modification (plus récent en premier)
                backups.sort(key=lambda x: x['last_modified'], reverse=True)
            
            return backups
            
        except Exception as e:
            print(f"❌ Erreur liste backups S3: {e}")
            return []
    
    def delete_file(self, key: str) -> bool:
        """
        Suppression d'un fichier sur S3
        """
        try:
            self.client.delete_object(
                Bucket=self.config['bucket'],
                Key=key
            )
            print(f"✅ Fichier supprimé de S3: {key}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur suppression S3 {key}: {e}")
            return False
    
    def file_exists(self, key: str) -> bool:
        """
        Vérifier si un fichier existe sur S3
        """
        try:
            self.client.head_object(
                Bucket=self.config['bucket'],
                Key=key
            )
            return True
        except self.client.exceptions.NoSuchKey:
            return False
        except Exception as e:
            print(f"❌ Erreur vérification existence S3 {key}: {e}")
            return False
    
    def get_public_url(self, key: str, expires_in: int = 3600) -> Optional[str]:
        """
        Générer URL publique signée pour un fichier S3
        """
        try:
            # Si CDN configuré, utiliser l'URL CDN
            if self.cdn_base_url:
                return f"{self.cdn_base_url.rstrip('/')}/{key}"
            
            # Sinon, générer URL signée S3
            url = self.client.generate_presigned_url(
                'get_object',
                Params={'Bucket': self.config['bucket'], 'Key': key},
                ExpiresIn=expires_in
            )
            return url
            
        except Exception as e:
            print(f"❌ Erreur génération URL publique S3 {key}: {e}")
            return None


# Instance globale pour cache Streamlit
@st.cache_resource
def get_s3_client() -> S3StorageClient:
    """Factory avec cache Streamlit pour client S3"""
    return S3StorageClient()


# API simplifiée pour les fonctions métier
def upload_osm_cache(cache_data: Dict[Any, Any]) -> bool:
    """Upload du cache OSM vers S3"""
    client = get_s3_client()
    return client.upload_json_file(
        "osm_cache.json", 
        cache_data,
        metadata={
            'type': 'osm_cache',
            'updated_at': datetime.utcnow().isoformat()
        }
    )


def download_osm_cache() -> Optional[Dict[Any, Any]]:
    """Download du cache OSM depuis S3"""
    client = get_s3_client()
    return client.download_json_file("osm_cache.json")


def upload_backup_to_cloud(backup_path: Path) -> bool:
    """Upload d'un backup vers S3"""
    client = get_s3_client()
    return client.upload_backup(backup_path)


def list_cloud_backups() -> list:
    """Liste des backups cloud disponibles"""
    client = get_s3_client()
    return client.list_backups()


def download_backup_from_cloud(s3_key: str, local_path: Path) -> bool:
    """Download d'un backup depuis S3"""
    client = get_s3_client()
    return client.download_backup(s3_key, local_path)
```
---8<--- src/storage/cloud.py END ---

---8<--- src/storage/local.py BEGIN ---
```py
"""
Stockage local pour GuignoMap v5.0  
Fallback avec API identique à cloud.py
"""
import json
import shutil
import os
from typing import Optional, Dict, Any
from pathlib import Path
from datetime import datetime


class LocalStorageClient:
    """Client stockage local avec API identique au client S3"""
    
    def __init__(self, base_path: Optional[Path] = None):
        # Répertoire de base pour le stockage local
        if base_path is None:
            base_path = Path(__file__).parent.parent.parent / "storage_local"
        
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # Sous-répertoires
        self.backups_dir = self.base_path / "backups"
        self.backups_dir.mkdir(exist_ok=True)
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """
        Sauvegarde d'un fichier JSON en local
        """
        try:
            file_path = self.base_path / key
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder les données JSON
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # Sauvegarder les métadonnées si fournies
            if metadata:
                metadata_path = file_path.with_suffix('.metadata.json')
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"✅ JSON sauvé localement: {file_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur sauvegarde JSON local {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """
        Lecture d'un fichier JSON local
        """
        try:
            file_path = self.base_path / key
            
            if not file_path.exists():
                print(f"ℹ️ Fichier JSON local non trouvé: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"✅ JSON lu localement: {file_path}")
            return data
            
        except Exception as e:
            print(f"❌ Erreur lecture JSON local {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """
        Copie d'un fichier backup vers le répertoire local
        """
        try:
            if not backup_file_path.exists():
                print(f"❌ Fichier backup non trouvé: {backup_file_path}")
                return False
            
            # Générer le nom de destination si non fourni
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                dest_name = f"{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            else:
                # Extraire le nom du fichier de la clé S3
                dest_name = Path(s3_key).name
            
            dest_path = self.backups_dir / dest_name
            
            # Copier le fichier
            shutil.copy2(backup_file_path, dest_path)
            
            # Créer un fichier de métadonnées
            metadata = {
                'original_filename': backup_file_path.name,
                'original_path': str(backup_file_path),
                'upload_timestamp': datetime.utcnow().isoformat(),
                'size': backup_file_path.stat().st_size
            }
            
            metadata_path = dest_path.with_suffix(dest_path.suffix + '.metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"✅ Backup copié localement: {dest_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur copie backup local: {e}")
            return False
    
    def download_backup(self, s3_key: str, local_path: Path) -> bool:
        """
        Copie d'un backup depuis le stockage local
        """
        try:
            # Trouver le fichier source
            source_name = Path(s3_key).name
            source_path = self.backups_dir / source_name
            
            if not source_path.exists():
                print(f"❌ Backup local non trouvé: {source_path}")
                return False
            
            # Créer le répertoire de destination
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Copier le fichier
            shutil.copy2(source_path, local_path)
            
            print(f"✅ Backup copié depuis local: {source_path} → {local_path}")
            return True
            
        except Exception as e:
            print(f"❌ Erreur copie backup depuis local {s3_key}: {e}")
            return False
    
    def list_backups(self, prefix: str = "backups/") -> list:
        """
        Liste des backups disponibles en local
        """
        try:
            backups = []
            
            # Lister tous les fichiers (sauf métadonnées)
            for backup_file in self.backups_dir.glob("*"):
                if backup_file.is_file() and not backup_file.name.endswith('.metadata.json'):
                    # Lire les métadonnées si disponibles
                    metadata_path = backup_file.with_suffix(backup_file.suffix + '.metadata.json')
                    metadata = {}
                    if metadata_path.exists():
                        try:
                            with open(metadata_path, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                        except:
                            pass
                    
                    stat = backup_file.stat()
                    backups.append({
                        'key': f"backups/{backup_file.name}",
                        'size': stat.st_size,
                        'last_modified': datetime.fromtimestamp(stat.st_mtime),
                        'filename': backup_file.name,
                        'metadata': metadata
                    })
            
            # Trier par date de modification (plus récent en premier)
            backups.sort(key=lambda x: x['last_modified'], reverse=True)
            return backups
            
        except Exception as e:
            print(f"❌ Erreur liste backups locaux: {e}")
            return []
    
    def delete_file(self, key: str) -> bool:
        """
        Suppression d'un fichier local
        """
        try:
            file_path = self.base_path / key
            
            if file_path.exists():
                file_path.unlink()
                
                # Supprimer les métadonnées si elles existent
                metadata_path = file_path.with_suffix('.metadata.json')
                if metadata_path.exists():
                    metadata_path.unlink()
                
                print(f"✅ Fichier supprimé localement: {file_path}")
                return True
            else:
                print(f"ℹ️ Fichier local non trouvé: {file_path}")
                return False
            
        except Exception as e:
            print(f"❌ Erreur suppression fichier local {key}: {e}")
            return False
    
    def file_exists(self, key: str) -> bool:
        """
        Vérifier si un fichier existe en local
        """
        file_path = self.base_path / key
        return file_path.exists()
    
    def get_public_url(self, key: str, expires_in: int = 3600) -> Optional[str]:
        """
        Générer un chemin local pour un fichier (pas d'URL publique)
        """
        file_path = self.base_path / key
        if file_path.exists():
            return f"file://{file_path.absolute()}"
        return None


# Instance globale pour le stockage local
_local_client = None

def get_local_client() -> LocalStorageClient:
    """Factory pour client de stockage local"""
    global _local_client
    if _local_client is None:
        _local_client = LocalStorageClient()
    return _local_client


# API simplifiée pour les fonctions métier (identique à cloud.py)
def upload_osm_cache(cache_data: Dict[Any, Any]) -> bool:
    """Upload du cache OSM en local"""
    client = get_local_client()
    return client.upload_json_file(
        "osm_cache.json", 
        cache_data,
        metadata={
            'type': 'osm_cache',
            'updated_at': datetime.utcnow().isoformat()
        }
    )


def download_osm_cache() -> Optional[Dict[Any, Any]]:
    """Download du cache OSM depuis local"""
    client = get_local_client()
    return client.download_json_file("osm_cache.json")


def upload_backup_to_cloud(backup_path: Path) -> bool:
    """Upload d'un backup en local"""
    client = get_local_client()
    return client.upload_backup(backup_path)


def list_cloud_backups() -> list:
    """Liste des backups locaux disponibles"""
    client = get_local_client()
    return client.list_backups()


def download_backup_from_cloud(s3_key: str, local_path: Path) -> bool:
    """Download d'un backup depuis local"""
    client = get_local_client()
    return client.download_backup(s3_key, local_path)
```
---8<--- src/storage/local.py END ---

---8<--- src/utils/__init__.py BEGIN ---
```py
# Utils module for GuignoMap
```
---8<--- src/utils/__init__.py END ---

---8<--- src/utils/adapters.py BEGIN ---
```py
import pandas as pd
from typing import Any, Iterable

def to_dataframe(records: Any) -> pd.DataFrame:
    if isinstance(records, pd.DataFrame):
        return records
    # RowMapping unique
    try:
        if hasattr(records, "keys") and hasattr(records, "__getitem__"):
            return pd.DataFrame([dict(records)])
    except Exception:
        pass
    # Séquences (list[dict]/list[Row]/list[ORM])
    if isinstance(records, Iterable):
        items = list(records)
        if items and not isinstance(items[0], dict):
            dicts = []
            for r in items:
                if hasattr(r, "__dict__"):
                    d = {k:v for k,v in r.__dict__.items() if not k.startswith("_sa_")}
                    dicts.append(d)
                else:
                    try: dicts.append(dict(r))
                    except Exception: dicts.append({"value": r})
            return pd.DataFrame(dicts)
        return pd.DataFrame(items)
    return pd.DataFrame([])
```
---8<--- src/utils/adapters.py END ---

---8<--- tests/manual/test_db_connection.py BEGIN ---
```py
import os
import sys
import socket
import psycopg2

def test_connection_with_ip():
    """Test connection with direct IPv6 address"""
    # Configuration manuelle avec l'adresse IPv6 résolue
    ipv6_host = "2600:1f11:4e2:e202:6514:7431:494f:c00f"
    
    connection_string = f"postgresql://postgres:4everSab!2304@[{ipv6_host}]:5432/postgres"
    
    print(f"Test de connexion avec IPv6 directe: {ipv6_host}")
    
    try:
        conn = psycopg2.connect(connection_string)
        print("✅ Connexion IPv6 réussie!")
        
        with conn.cursor() as cursor:
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            print(f"Version PostgreSQL: {version}")
        
        conn.close()
        return True
        
    except Exception as e:
        print(f"❌ Erreur de connexion IPv6: {e}")
        return False

def test_connection_with_hostname():
    """Test connection with hostname via custom DNS"""
    import socket
    
    # Forcer IPv4 si possible
    try:
        socket.setdefaulttimeout(10)
        original_getaddrinfo = socket.getaddrinfo
        
        def custom_getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):
            if host == "db.kdxqspmfycnwzzrmhzpa.supabase.co":
                # Retourner directement l'IPv6 connue
                return [(socket.AF_INET6, socket.SOCK_STREAM, 6, '', 
                        ('2600:1f11:4e2:e202:6514:7431:494f:c00f', port, 0, 0))]
            return original_getaddrinfo(host, port, family, type, proto, flags)
        
        socket.getaddrinfo = custom_getaddrinfo
        
        connection_string = "postgresql://postgres:4everSab!2304@db.kdxqspmfycnwzzrmhzpa.supabase.co:5432/postgres"
        
        print("Test de connexion avec hostname (DNS custom)")
        conn = psycopg2.connect(connection_string)
        print("✅ Connexion hostname réussie!")
        
        with conn.cursor() as cursor:
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            print(f"Version PostgreSQL: {version}")
        
        conn.close()
        socket.getaddrinfo = original_getaddrinfo
        return True
        
    except Exception as e:
        print(f"❌ Erreur de connexion hostname: {e}")
        socket.getaddrinfo = original_getaddrinfo
        return False

if __name__ == "__main__":
    print("=== Test de connectivité Supabase PostgreSQL ===")
    
    print("\n1. Test avec adresse IPv6 directe:")
    ipv6_success = test_connection_with_ip()
    
    print("\n2. Test avec hostname (DNS custom):")
    hostname_success = test_connection_with_hostname()
    
    if ipv6_success or hostname_success:
        print("\n✅ Au moins une méthode de connexion fonctionne!")
    else:
        print("\n❌ Aucune méthode de connexion ne fonctionne")
        print("Problème potentiel: connectivité IPv6 ou firewall")
```
---8<--- tests/manual/test_db_connection.py END ---

---8<--- tests/manual/test_db_simple.py BEGIN ---
```py
#!/usr/bin/env python3
"""Test simple de connexion base de données"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_db_simple():
    try:
        from src.database.connection import get_engine, test_connection
        
        print("=== Test de connexion base de données ===")
        
        # Test avec la fonction dédiée
        print("1. Test avec test_connection():")
        test_connection()
        
        # Test manuel avec engine
        print("\n2. Test manuel avec engine:")
        engine = get_engine()
        print(f"✅ Engine créé: {engine.url}")
        
        # Tester la connexion
        with engine.connect() as conn:
            result = conn.execute("SELECT 1 as test")
            test_value = result.fetchone()[0]
            print(f"✅ Connexion réussie! Test query result: {test_value}")
        
        print("✅ Connexion base de données fonctionnelle!")
        return True
        
    except Exception as e:
        print(f"❌ Erreur de connexion: {e}")
        return False

if __name__ == "__main__":
    success = test_db_simple()
    sys.exit(0 if success else 1)
```
---8<--- tests/manual/test_db_simple.py END ---

## NOTE
- Secrets exclus par conception (ex: .streamlit/secrets.toml)
- Tous les .py des zones pertinentes sont inclus en intégralité.
