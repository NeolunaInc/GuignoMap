# GuignoMap - Export Scripts Python
# Généré le 2025-09-19 16:47:20  
# Fichiers inclus : scripts/, legacy/scripts/
# Exclusions : .venv/, __pycache__/
#
# ============================================================================
# SCRIPTS PRINCIPAUX (scripts/)
# ============================================================================
# Total : 28 fichiers Python
#
# Catégories:
# - Import/Export: import_city_excel.py, export_repo_*.py
# - Validation: verify_*.py, validate_*.py, check_addresses.py
# - Maintenance: fix_*.py, migrate_*.py, normalize_encoding.py
# - Tests: run_all_tests.py, smoke_*.py, test_helpers.py
# - Setup: setup_nouvelle_ville.py, seed_address_demo.py
# - Utilitaires: generate_tree_clean.py, show_hash_stats.py

# ===== scripts/check_addresses.py =====
#!/usr/bin/env python3
"""
Script de diagnostic pour analyser l'Ã©tat de la base de donnÃ©es d'adresses
"""
import sqlite3
import sys
from pathlib import Path
import pandas as pd

# Ajouter le rÃ©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from guignomap.database import get_conn, stats_addresses, get_unassigned_addresses, get_team_addresses


def analyze_database():
    """Analyse complÃ¨te de la base de donnÃ©es"""
    print("=" * 60)
    print("ðŸ” DIAGNOSTIC BASE DE DONNÃ‰ES - GuignoMap")
    print("=" * 60)
    
    # 1. Tables et index
    print("\nðŸ“‹ STRUCTURE DE LA BASE")
    with get_conn() as conn:
        cursor = conn.cursor()
        
        # Lister les tables
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = [row[0] for row in cursor.fetchall()]
        print(f"   Tables: {', '.join(tables)}")
        
        # Lister les index
        cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name NOT LIKE 'sqlite_%' ORDER BY name")
        indexes = [row[0] for row in cursor.fetchall()]
        print(f"   Index personnalisÃ©s: {len(indexes)}")
        for idx in indexes[:5]:  # Afficher les 5 premiers
            print(f"     - {idx}")
        if len(indexes) > 5:
            print(f"     ... et {len(indexes) - 5} autres")
    
    # 2. Statistiques globales
    print("\nðŸ“Š STATISTIQUES GLOBALES")
    stats = stats_addresses()
    print(f"   Total adresses: {stats['total']:,}")
    print(f"   Non assignÃ©es: {stats['unassigned']:,}")
    print(f"   AssignÃ©es: {stats['assigned']:,}")
    print(f"   GÃ©ocodÃ©es: {stats['percent_geocoded']}%")
    print(f"   Ã‰quipes actives: {len(stats['per_team'])}")
    
    # 3. Top 10 non assignÃ©es
    print("\nðŸ  TOP 10 ADRESSES NON ASSIGNÃ‰ES")
    unassigned = get_unassigned_addresses(limit=10)
    if len(unassigned) > 0:
        for _, row in unassigned.iterrows():
            sector_info = f" [{row['sector']}]" if pd.notna(row['sector']) else ""
            print(f"   {row['id']:>6} - {row['street_name']} {row['house_number']}{sector_info}")
    else:
        print("   âœ… Aucune adresse non assignÃ©e")
    
    # 4. Top 10 Ã©quipes par nombre d'adresses
    print("\nðŸ‘¥ TOP 10 Ã‰QUIPES PAR NOMBRE D'ADRESSES")
    sorted_teams = sorted(stats['per_team'].items(), key=lambda x: x[1], reverse=True)
    for i, (team, count) in enumerate(sorted_teams[:10], 1):
        percent = round((count / stats['total'] * 100), 1) if stats['total'] > 0 else 0
        print(f"   {i:>2}. {team:<20} : {count:>6,} adresses ({percent:>5.1f}%)")
        
        # Montrer 3 exemples d'adresses pour cette Ã©quipe
        team_addresses = get_team_addresses(team, limit=3)
        for _, addr in team_addresses.iterrows():
            sector_info = f" [{addr['sector']}]" if pd.notna(addr['sector']) else ""
            print(f"       â”œâ”€ {addr['street_name']} {addr['house_number']}{sector_info}")
    
    # 5. DÃ©tection de problÃ¨mes potentiels
    print("\nâš ï¸  DÃ‰TECTION DE PROBLÃˆMES")
    with get_conn() as conn:
        cursor = conn.cursor()
        
        # Adresses sans nom de rue
        cursor.execute("SELECT COUNT(*) FROM addresses WHERE street_name IS NULL OR street_name = ''")
        no_street = cursor.fetchone()[0]
        if no_street > 0:
            print(f"   âŒ {no_street} adresses sans nom de rue")
        
        # Adresses sans numÃ©ro
        cursor.execute("SELECT COUNT(*) FROM addresses WHERE house_number IS NULL OR house_number = ''")
        no_number = cursor.fetchone()[0]
        if no_number > 0:
            print(f"   âŒ {no_number} adresses sans numÃ©ro")
        
        # ClÃ©s en doublon
        cursor.execute("""
            SELECT addr_key, COUNT(*) as count 
            FROM addresses 
            GROUP BY addr_key 
            HAVING count > 1
        """)
        duplicates = cursor.fetchall()
        if duplicates:
            print(f"   âŒ {len(duplicates)} clÃ©s addr_key en doublon")
            for key, count in duplicates[:3]:
                print(f"       â”œâ”€ '{key}' : {count} occurrences")
        
        # Ã‰quipes orphelines (assignÃ©es mais pas dans la table teams)
        cursor.execute("""
            SELECT DISTINCT assigned_to 
            FROM addresses 
            WHERE assigned_to IS NOT NULL 
              AND assigned_to != ''
              AND assigned_to NOT IN (SELECT name FROM teams)
        """)
        orphan_teams = [row[0] for row in cursor.fetchall()]
        if orphan_teams:
            print(f"   âš ï¸  {len(orphan_teams)} Ã©quipes assignÃ©es mais non dÃ©clarÃ©es:")
            for team in orphan_teams[:5]:
                print(f"       â”œâ”€ '{team}'")
        
        if no_street == 0 and no_number == 0 and not duplicates and not orphan_teams:
            print("   âœ… Aucun problÃ¨me dÃ©tectÃ©")
    
    print("\n" + "=" * 60)
    print("âœ… Diagnostic terminÃ©")


if __name__ == "__main__":
    try:
        analyze_database()
    except Exception as e:
        print(f"âŒ Erreur lors du diagnostic: {e}")
        sys.exit(1)

# ===== scripts/debug_excel.py =====
import pandas as pd

df = pd.read_excel('imports/mascouche_adresses.xlsx')
print('Sample data from Excel:')
for i in range(5):
    print(f'Ligne {i}:')
    print(f'  nomrue: {repr(df.iloc[i]["nomrue"])}')
    print(f'  OdoParti: {repr(df.iloc[i]["OdoParti"])}')
    print(f'  OdoSpeci: {repr(df.iloc[i]["OdoSpeci"])}')
    print()

# ===== scripts/enrich_addresses_from_osm.py =====
# scripts/enrich_addresses_from_osm.py
import json
import sqlite3
import unicodedata
import re
from pathlib import Path

DB_PATH = Path("guignomap/guigno_map.db")
OSM_ADDRESSES_PATH = Path("guignomap/osm_addresses.json")

def _normalize_text(s):
    """Normalise un texte pour crÃ©er une clÃ© d'adresse unique (identique au script d'import)"""
    if s is None or s == "":
        return ""
    
    # Convertir en string et strip
    text = str(s).strip()
    
    # Enlever les accents (NFD normalization puis supprimer les diacritiques)
    text = unicodedata.normalize('NFD', text)
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    
    # Minuscules
    text = text.lower()
    
    # Compresser les espaces multiples
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def load_osm_addresses():
    """Charge et indexe les adresses OSM par addr_key"""
    if not OSM_ADDRESSES_PATH.exists():
        print(f"âŒ Fichier OSM introuvable: {OSM_ADDRESSES_PATH}")
        return {}
    
    print(f"ðŸ“– Lecture du fichier OSM: {OSM_ADDRESSES_PATH}")
    
    with open(OSM_ADDRESSES_PATH, 'r', encoding='utf-8') as f:
        osm_data = json.load(f)
    
    # Indexer par addr_key
    osm_index = {}
    total_entries = 0
    
    for street_name, addresses in osm_data.items():
        for addr in addresses:
            house_number = str(addr.get("number", "")).strip()
            lat = addr.get("lat")
            lon = addr.get("lon") 
            osm_type = addr.get("type", "unknown")
            
            if house_number and lat is not None and lon is not None:
                # CrÃ©er la mÃªme addr_key qu'utilisÃ©e dans l'import
                # Pour OSM, nous n'avons que street_name et house_number, pas de postal_code
                addr_key = f"{_normalize_text(street_name)}|{_normalize_text(house_number)}|"
                
                # Si plusieurs entrÃ©es OSM pour la mÃªme addr_key, garder la premiÃ¨re
                if addr_key not in osm_index:
                    osm_index[addr_key] = {
                        "latitude": float(lat),
                        "longitude": float(lon),
                        "osm_type": osm_type,
                        "street_name": street_name,
                        "house_number": house_number
                    }
                
                total_entries += 1
    
    unique_keys = len(osm_index)
    print(f"ðŸ“Š {total_entries} entrÃ©es OSM lues, {unique_keys} clÃ©s d'adresse uniques indexÃ©es")
    
    return osm_index

def enrich_addresses_from_osm():
    """Enrichit les adresses en base avec les donnÃ©es OSM par jointure sur addr_key"""
    print("=== Enrichissement des adresses depuis OSM ===")
    
    # 1. Charger et indexer les donnÃ©es OSM
    osm_index = load_osm_addresses()
    
    if not osm_index:
        print("âŒ Aucune donnÃ©e OSM disponible pour l'enrichissement")
        return
    
    # 2. VÃ©rifier la base de donnÃ©es
    if not DB_PATH.exists():
        print(f"âŒ Base de donnÃ©es introuvable: {DB_PATH}")
        return
    
    with sqlite3.connect(DB_PATH) as conn:
        # VÃ©rifier que la table addresses existe
        tables = {row[0] for row in conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()}
        if "addresses" not in tables:
            print("âŒ Table 'addresses' introuvable dans la base")
            return
        
        # VÃ©rifier les colonnes nÃ©cessaires
        columns = {row[1] for row in conn.execute("PRAGMA table_info(addresses)").fetchall()}
        required_cols = {"addr_key", "latitude", "longitude", "osm_type"}
        
        if not required_cols.issubset(columns):
            missing = required_cols - columns
            print(f"âŒ Colonnes manquantes dans addresses: {missing}")
            return
        
        print("âœ… Table addresses prÃªte pour l'enrichissement")
        
        # 3. Compter les adresses totales et celles dÃ©jÃ  gÃ©ocodÃ©es
        total_addresses = conn.execute("SELECT COUNT(*) FROM addresses").fetchone()[0]
        already_geocoded = conn.execute("""
            SELECT COUNT(*) FROM addresses 
            WHERE latitude IS NOT NULL AND longitude IS NOT NULL
        """).fetchone()[0]
        
        print(f"ðŸ“Š {total_addresses} adresses en base, {already_geocoded} dÃ©jÃ  gÃ©ocodÃ©es")
        
        # 4. Enrichissement par jointure sur addr_key
        cursor = conn.cursor()
        updated_count = 0
        matched_count = 0
        
        # RÃ©cupÃ©rer toutes les adresses non gÃ©ocodÃ©es
        non_geocoded = cursor.execute("""
            SELECT id, addr_key, street_name, house_number
            FROM addresses 
            WHERE latitude IS NULL OR longitude IS NULL
        """).fetchall()
        
        print(f"ðŸ” {len(non_geocoded)} adresse(s) Ã  enrichir")
        
        # Debug: afficher quelques exemples de clÃ©s
        print("\nðŸ” DEBUG - StratÃ©gies de correspondance:")
        
        # StratÃ©gie 1: correspondance exacte par addr_key
        exact_matches = 0
        for addr_id, addr_key, street_name, house_number in non_geocoded[:100]:  # Test sur 100 premiers
            if addr_key in osm_index:
                exact_matches += 1
        
        print(f"  Correspondances exactes (addr_key): {exact_matches}/{min(100, len(non_geocoded))}")
        
        # StratÃ©gie 2: correspondance par nom de rue normalisÃ© + numÃ©ro
        street_matches = 0
        for addr_id, addr_key, street_name, house_number in non_geocoded[:100]:
            # Nettoyer le street_name (supprimer les "nan" parasites)
            clean_street = street_name.replace("nan ", "").replace(" nan", "").strip()
            if clean_street:
                test_key = f"{_normalize_text(clean_street)}|{_normalize_text(house_number)}|"
                if test_key in osm_index:
                    street_matches += 1
        
        print(f"  Correspondances par rue nettoyÃ©e: {street_matches}/{min(100, len(non_geocoded))}")
        
        # StratÃ©gie 3: correspondance partielle (mÃªme numÃ©ro, rue similaire)
        partial_matches = 0
        osm_streets = {}
        for addr_key, data in osm_index.items():
            normalized_street = _normalize_text(data["street_name"])
            house_num = _normalize_text(data["house_number"])
            key = f"{normalized_street}|{house_num}"
            osm_streets[key] = data
        
        for addr_id, addr_key, street_name, house_number in non_geocoded[:100]:
            clean_street = street_name.replace("nan ", "").replace(" nan", "").strip()
            if clean_street:
                search_key = f"{_normalize_text(clean_street)}|{_normalize_text(house_number)}"
                if search_key in osm_streets:
                    partial_matches += 1
        
        print(f"  Correspondances partielles: {partial_matches}/{min(100, len(non_geocoded))}")
        
        # Appliquer la meilleure stratÃ©gie trouvÃ©e
        print(f"\nðŸ”§ Application de l'enrichissement...")
        
        for addr_id, addr_key, street_name, house_number in non_geocoded:
            osm_data = None
            
            # StratÃ©gie 1: exacte
            if addr_key in osm_index:
                osm_data = osm_index[addr_key]
                matched_count += 1
            else:
                # StratÃ©gie 2: rue nettoyÃ©e
                clean_street = street_name.replace("nan ", "").replace(" nan", "").strip()
                if clean_street:
                    test_key = f"{_normalize_text(clean_street)}|{_normalize_text(house_number)}|"
                    if test_key in osm_index:
                        osm_data = osm_index[test_key]
                        matched_count += 1
                    else:
                        # StratÃ©gie 3: partielle
                        search_key = f"{_normalize_text(clean_street)}|{_normalize_text(house_number)}"
                        if search_key in osm_streets:
                            osm_data = osm_streets[search_key]
                            matched_count += 1
            
            if osm_data:
                try:
                    cursor.execute("""
                        UPDATE addresses 
                        SET latitude = ?, longitude = ?, osm_type = ?
                        WHERE id = ?
                    """, (
                        osm_data["latitude"],
                        osm_data["longitude"], 
                        osm_data["osm_type"],
                        addr_id
                    ))
                    
                    if cursor.rowcount > 0:
                        updated_count += 1
                    
                except Exception as e:
                    print(f"âŒ Erreur mise Ã  jour ID {addr_id}: {e}")
        
        conn.commit()
        
        # 5. Statistiques finales
        final_geocoded = conn.execute("""
            SELECT COUNT(*) FROM addresses 
            WHERE latitude IS NOT NULL AND longitude IS NOT NULL
        """).fetchone()[0]
        
        newly_geocoded = final_geocoded - already_geocoded
        
        print(f"\n=== RÃ‰SULTATS ===")
        print(f"ðŸ“Š Correspondances trouvÃ©es: {matched_count}")
        print(f"ðŸ“Š Mises Ã  jour rÃ©ussies: {updated_count}")
        print(f"ðŸ“Š Nouvellement gÃ©ocodÃ©es: {newly_geocoded}")
        print(f"ðŸ“Š Total gÃ©ocodÃ©: {final_geocoded}/{total_addresses} ({final_geocoded/total_addresses*100:.1f}%)")
        
        if updated_count > 0:
            print("âœ… Enrichissement terminÃ© avec succÃ¨s!")
        else:
            print("âš ï¸ Aucune mise Ã  jour effectuÃ©e")
        
        # 6. Quelques exemples d'adresses enrichies
        if updated_count > 0:
            print(f"\nðŸ“ Exemples d'adresses enrichies (5 premiers):")
            examples = cursor.execute("""
                SELECT street_name, house_number, latitude, longitude, osm_type
                FROM addresses 
                WHERE latitude IS NOT NULL AND longitude IS NOT NULL
                ORDER BY id DESC
                LIMIT 5
            """).fetchall()
            
            for street, number, lat, lon, osm_type in examples:
                print(f"  - {street} {number}: ({lat:.6f}, {lon:.6f}) [{osm_type}]")

if __name__ == "__main__":
    enrich_addresses_from_osm()

# ===== scripts/export_repo_audit.py =====
# coding: utf-8
"""
Export dâ€™audit GuignoMap â†’ exports/export_audit_YYYYMMDD_HHMMSS.txt

Objectifs:
- Inclure 100% du contenu de TOUS les fichiers .py (src/, tests/, scripts/, guignomap/â€¦)
- Inclure fichiers importants: config (toml/yaml/ini/json/sql), alembic.ini, migrations, requirements*, pyproject.toml, Dockerfile, .gitignore, README.md
- Exclure: backups, exports, caches, pycache, venv, .git, binaires, secrets (.streamlit/secrets.toml)
- Ajouter une section ENV avec: version Python, chemin exÃ©cutable, plateforme, LISTE COMPLETE des paquets installÃ©s
Sortie: UTF-8 (LF), sans BOM
"""

from __future__ import annotations
from pathlib import Path
from datetime import datetime
import sys, platform, subprocess

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "exports"
OUTDIR.mkdir(parents=True, exist_ok=True)
OUTFILE = OUTDIR / f"export_audit_{datetime.now():%Y%m%d_%H%M%S}.txt"

# Dossiers Ã  exclure totalement
EXCLUDE_DIRS = {
    ".git", ".github", ".venv", "venv", "env", ".vscode", "__pycache__", ".pytest_cache", ".mypy_cache",
    "node_modules", "storage_local", "backups", "exports", "logs", ".idea", ".DS_Store"
}

# Fichiers exactement exclus (secrets etc.)
EXCLUDE_FILES = {
    "secrets.toml",  # ne jamais sortir les secrets
    ".python-version"
}

# Extensions autorisÃ©es pour les fichiers non-.py, considÃ©rÃ©s â€œimportants Ã  auditerâ€
ALLOW_NONPY_EXTS = {
    ".toml", ".ini", ".cfg", ".conf", ".yml", ".yaml", ".json", ".sql",
    ".md", ".txt", ".ps1", ".bat", ".sh", ".dockerignore"
}

# Fichiers/chemins â€œimportantsâ€ acceptÃ©s mÃªme sans extension (ou spÃ©cifiques)
ALLOW_SPECIAL_PATHS = {
    "alembic.ini",
    "Dockerfile",
    ".gitignore",
    ".env.example",
    ".streamlit/config.toml",
    "requirements.txt",
    "requirements-dev.txt",
    "pyproject.toml",
}

# PrÃ©fixes utiles (whitelist de zones pertinentes)
ALLOW_PREFIXES = [
    "src/",
    "tests/",
    "scripts/",
    "guignomap/",
    "src/database/migrations/",
]

# Ne PAS limiter la taille des .py (on les veut ENTIEREMENT)
MAX_NONPY_SIZE = 200 * 1024  # 200KB pour les non-.py (pour Ã©viter blobs inutiles)

def is_excluded_path(p: Path) -> bool:
    for part in p.parts:
        if part in EXCLUDE_DIRS:
            return True
    return False

def is_allowed_file(p: Path) -> bool:
    if p.name in EXCLUDE_FILES:
        return False

    rel = p.relative_to(ROOT).as_posix()

    # .py â†’ toujours inclus s'il est dans une zone pertinente
    if p.suffix.lower() == ".py":
        if any(rel == pre or rel.startswith(pre) for pre in ALLOW_PREFIXES):
            return True
        return False

    # Fichiers spÃ©ciaux explicitement autorisÃ©s
    if rel in ALLOW_SPECIAL_PATHS:
        return True

    # Fichiers non-.py: extension autorisÃ©e ET dans une zone pertinente
    if p.suffix.lower() in ALLOW_NONPY_EXTS:
        if any(rel == pre or rel.startswith(pre) for pre in ALLOW_PREFIXES):
            return True

    return False

def read_text_utf8(p: Path) -> str:
    try:
        txt = p.read_text(encoding="utf-8", errors="replace")
    except Exception as e:
        txt = f"<<ERREUR LECTURE {p}: {e}>>"
    return txt.replace("\r\n", "\n").replace("\r", "\n")

def collect_files() -> list[Path]:
    files: list[Path] = []
    for p in ROOT.rglob("*"):
        if not p.is_file():
            continue
        if is_excluded_path(p):
            continue
        if not is_allowed_file(p):
            continue

        # Taille: .py = aucune limite; autres = borne raisonnable
        if p.suffix.lower() != ".py":
            try:
                if p.stat().st_size > MAX_NONPY_SIZE:
                    continue
            except Exception:
                continue

        files.append(p)

    files.sort(key=lambda x: x.relative_to(ROOT).as_posix().lower())
    return files

def get_installed_packages() -> list[str]:
    """Retourne une liste 'Nom==Version' triÃ©e, en Ã©vitant les attributs privÃ©s."""
    # 1) Tentative avec importlib.metadata (propre)
    try:
        from importlib.metadata import distributions
        pkgs = []
        for dist in distributions():
            meta = getattr(dist, "metadata", None)
            # meta est un email.message.Message (supporte .get)
            name = None
            if meta:
                name = meta.get("Name")
            ver = getattr(dist, "version", None)
            if name and ver:
                pkgs.append(f"{name}=={ver}")
        if pkgs:
            return sorted(pkgs, key=lambda s: s.lower())
    except Exception:
        pass

    # 2) Fallback avec pkg_resources (setuptools)
    try:
        import pkg_resources  # type: ignore
        pkgs = [f"{d.project_name}=={d.version}" for d in pkg_resources.working_set]
        if pkgs:
            return sorted(set(pkgs), key=lambda s: s.lower())
    except Exception:
        pass

    # 3) Dernier recours: pip freeze (synchronisÃ© avec lâ€™environnement courant)
    try:
        out = subprocess.check_output([sys.executable, "-m", "pip", "freeze"], text=True, encoding="utf-8")
        pkgs = [line.strip() for line in out.splitlines() if line.strip()]
        if pkgs:
            return sorted(pkgs, key=lambda s: s.lower())
    except Exception as e:
        return [f"<<ERREUR INVENTAIRE DEPENDANCES: {e}>>"]

    return []

def env_section() -> str:
    lines: list[str] = []
    lines.append("## ENVIRONNEMENT")
    lines.append(f"- Python : {sys.version.splitlines()[0]}")
    lines.append(f"- ExÃ©cutable : {sys.executable}")
    lines.append(f"- Plateforme : {platform.platform()}")

    # versions utiles si prÃ©sentes
    try:
        import streamlit as _st
        lines.append(f"- streamlit : {_st.__version__}")
    except Exception:
        pass
    # sqlalchemy removed - using pure SQLite now
    try:
        import pandas as _pd
        lines.append(f"- pandas : {_pd.__version__}")
    except Exception:
        pass
    try:
        import boto3 as _b3
        lines.append(f"- boto3 : {_b3.__version__}")
    except Exception:
        pass
    try:
        import passlib as _pl  # noqa
        lines.append("- passlib : prÃ©sent")
    except Exception:
        pass

    lines.append("")
    lines.append("### DÃ©pendances installÃ©es (inventaire)")
    pkgs = get_installed_packages() or ["<<Aucune dÃ©pendance dÃ©tectÃ©e>>"]
    lines.extend(pkgs)
    lines.append("")
    return "\n".join(lines)

def main() -> int:
    files = collect_files()

    header = [
        "# GuignoMap â€” Export dâ€™audit COMPLET (code et config utiles)",
        f"# Date : {datetime.now():%Y-%m-%d %H:%M:%S}",
        f"# Racine : {ROOT}",
        "# Contenu : 100% des .py (zones pertinentes) + fichiers de config/migrations essentiels",
        "# Exclus : backups, exports, caches, venv, .git, binaires, secrets (.streamlit/secrets.toml)",
        ""
    ]

    with OUTFILE.open("w", encoding="utf-8", newline="\n") as out:
        out.write("\n".join(header) + "\n")
        out.write(env_section() + "\n")

        out.write("## INDEX DES FICHIERS INCLUS\n")
        for p in files:
            out.write(f"- {p.relative_to(ROOT).as_posix()}\n")

        out.write("\n## CONTENU DES FICHIERS\n")
        for p in files:
            rel = p.relative_to(ROOT).as_posix()
            out.write(f"\n---8<--- {rel} BEGIN ---\n")
            out.write("```" + (p.suffix.lower().lstrip(".") or "txt") + "\n")
            out.write(read_text_utf8(p))
            out.write("\n```\n")
            out.write(f"---8<--- {rel} END ---\n")

        out.write("\n## NOTE\n- Secrets exclus par conception (ex: .streamlit/secrets.toml)\n")
        out.write("- Tous les .py des zones pertinentes sont inclus en intÃ©gralitÃ©.\n")

    print(f"âœ… Export dâ€™audit Ã©crit : {OUTFILE}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

# ===== scripts/export_repo_min.py =====

# ===== scripts/export_repo_snapshot.py =====
# coding: utf-8
"""
Export complet du code GuignoMap â†’ exports/export_full_YYYYMMDD_HHMMSS.txt
- UTF-8 (sans BOM), normalise les fins de lignes.
- Inclut le contenu des fichiers code/texte utiles.
- Exclut .git, .venv, caches, binaires, gros fichiers.
- Ne lit jamais .streamlit/secrets.toml (sÃ©curitÃ©).
"""

from __future__ import annotations
import sys, os, io, time
from pathlib import Path
from datetime import datetime

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "exports"
OUTDIR.mkdir(parents=True, exist_ok=True)
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTFILE = OUTDIR / f"export_full_{ts}.txt"

# Dossiers Ã  exclure
EXCLUDE_DIRS = {
    ".git", ".github", ".venv", ".vscode", "__pycache__", ".mypy_cache", ".pytest_cache",
    "node_modules", "dist", "build", "storage_local", ".idea"
}

# Fichiers Ã  exclure (par nom exact)
EXCLUDE_FILES = {
    "secrets.toml",  # ne jamais exposer des secrets
}

# Extensions Ã  inclure (code/texte)
INCLUDE_EXTS = {
    ".py", ".txt", ".md", ".ps1", ".bat", ".toml", ".ini", ".cfg",
    ".yml", ".yaml", ".json", ".sql"
}

# Extensions Ã  exclure dâ€™office (binaires/pondÃ©reux)
BINARY_EXTS = {
    ".db", ".sqlite", ".sqlite3", ".pkl", ".zip", ".7z", ".rar", ".exe", ".dll",
    ".png", ".jpg", ".jpeg", ".gif", ".ico", ".pdf", ".parquet"
}

# Taille max (Ko) par fichier pour Ã©viter les Ã©normes blobs
SIZE_LIMIT_KB = 300  # ajuste si besoin

def should_skip(path: Path) -> bool:
    # Exclure par dossier
    for part in path.parts:
        if part in EXCLUDE_DIRS:
            return True
    # Exclure par extension binaire
    if path.suffix.lower() in BINARY_EXTS:
        return True
    # Exclure fichier secrets explicites
    if path.name in EXCLUDE_FILES:
        return True
    # Filtre dâ€™extensions
    if path.suffix and path.suffix.lower() not in INCLUDE_EXTS:
        return True
    # Limite de taille
    try:
        if path.stat().st_size > SIZE_LIMIT_KB * 1024:
            return True
    except Exception:
        return True
    return False

def read_text_safely(path: Path) -> str:
    # Toujours lire en UTF-8 avec remplacement pour Ã©viter les caractÃ¨res corrompus
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            txt = f.read()
    except Exception as e:
        txt = f"<<ERREUR LECTURE {path}: {e}>>"
    # Normaliser fins de lignes
    return txt.replace("\r\n", "\n").replace("\r", "\n")

def main() -> int:
    files: list[Path] = []
    for p in ROOT.rglob("*"):
        if not p.is_file():
            continue
        if should_skip(p):
            continue
        files.append(p)

    files.sort(key=lambda x: str(x).lower())

    header = f"""# GuignoMap - Export de code COMPLET
# Date : {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
# Racine : {ROOT}
# Fichiers inclus : {len(files)}
# Encodage : UTF-8 (sans BOM)
# RÃ¨gles : exclusions .git/.venv/binaires/gros fichiers, NO secrets.toml
"""

    with open(OUTFILE, "w", encoding="utf-8", newline="\n") as out:
        out.write(header + "\n")
        out.write("## INDEX\n")
        for p in files:
            rel = p.relative_to(ROOT)
            out.write(f"- {rel.as_posix()}\n")

        out.write("\n## CONTENU DES FICHIERS\n")
        for p in files:
            rel = p.relative_to(ROOT)
            out.write("\n")
            out.write(f"---8<--- {rel.as_posix()} BEGIN ---\n")
            out.write("```" + f"{p.suffix.lower().lstrip('.') or 'txt'}" + "\n")
            out.write(read_text_safely(p))
            out.write("\n```\n")
            out.write(f"---8<--- {rel.as_posix()} END ---\n")

        out.write("\n## STATISTIQUES\n")
        total_bytes = sum((p.stat().st_size for p in files), 0)
        out.write(f"- Total fichiers exportÃ©s : {len(files)}\n")
        out.write(f"- Poids cumulÃ© (approx) : {total_bytes/1024:.1f} Ko\n")
        out.write(f"- Limite par fichier : {SIZE_LIMIT_KB} Ko\n")

    print(f"âœ… Export Ã©crit : {OUTFILE}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

# ===== scripts/find_mojibake.py =====
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scanner de fichiers pour dÃ©tecter les chaÃ®nes mojibakÃ©es

Parcourt les fichiers texte (.py/.md/.toml/.json/.csv) et signale les lignes
contenant des motifs frÃ©quents de corruption d'encodage.

Usage:
    python scripts/find_mojibake.py
    python scripts/find_mojibake.py --verbose
    python scripts/find_mojibake.py --extensions .py,.md,.txt
"""

import sys
import os
import argparse
from pathlib import Path
from typing import List, Set, Tuple

# Ajouter le rÃ©pertoire parent au path pour importer src/
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Motifs frÃ©quents de mojibake (CP1252 -> UTF-8 mal interprÃ©tÃ©)
MOJIBAKE_PATTERNS = [
    "Ãƒ",      # Ã€ mal encodÃ©
    "âˆšÂ©",     # Ã© mal encodÃ©  
    "âˆšâ‰ ",     # Ã­ mal encodÃ©
    "âˆšâ€ ",     # Ã  mal encodÃ©
    "âˆšÂ®",     # Ã¨ mal encodÃ©
    "âˆšÂ§",     # Ã§ mal encodÃ©
    "Ã‘",      # caractÃ¨res spÃ©ciaux
    "Ã¼",      # caractÃ¨res Ã©tendus
    "Ã”",      # caractÃ¨res maths/spÃ©ciaux
    "âˆ",      # symboles
    "Â´",      # accents isolÃ©s
    "Ë†",      # accents isolÃ©s
    "Â¬",      # symboles logiques
    "âˆ™",      # puces spÃ©ciales
    "ÃƒÂ©",     # Ã© spÃ©cifique
    "ÃƒÂ¨",     # Ã¨ spÃ©cifique  
    "Ãƒ ",     # Ã  spÃ©cifique
    "ÃƒÂ´",     # Ã´ spÃ©cifique
    "ÃƒÂ»",     # Ã» spÃ©cifique
    "Ã¢â‚¬",     # guillemets/tirets problÃ©matiques
]

# Extensions de fichiers Ã  scanner par dÃ©faut
DEFAULT_EXTENSIONS = {'.py', '.md', '.toml', '.json', '.csv', '.txt', '.sql'}

# RÃ©pertoires Ã  ignorer
IGNORE_DIRS = {'.git', '.venv', '__pycache__', 'node_modules', '.pytest_cache', 'backups'}

# Fichiers Ã  ignorer complÃ¨tement (contiennent volontairement des patterns mojibake)
ALLOWLIST_FILES = {"scripts/find_mojibake.py", "scripts/fix_mojibake_db.py", "scripts/fix_mojibake_files.py"}


def find_mojibake_in_file(file_path: Path, patterns: List[str], verbose: bool = False) -> List[Tuple[int, str]]:
    """
    Analyse un fichier et retourne les lignes contenant du mojibake
    
    Returns:
        List de tuples (numÃ©ro_ligne, ligne_contenu)
    """
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            for line_num, line in enumerate(f, 1):
                for pattern in patterns:
                    if pattern in line:
                        issues.append((line_num, line.strip()))
                        if verbose:
                            print(f"  -> L{line_num}: motif '{pattern}' trouvÃ©")
                        break  # Une seule alerte par ligne
                        
    except Exception as e:
        if verbose:
            print(f"âš ï¸  Erreur lecture {file_path}: {e}")
    
    return issues


def scan_directory(root_dir: Path, extensions: Set[str], patterns: List[str], verbose: bool = False) -> dict:
    """
    Scanne rÃ©cursivement un rÃ©pertoire
    
    Returns:
        Dict {file_path: [(line_num, line_content), ...]}
    """
    results = {}
    files_scanned = 0
    
    for file_path in root_dir.rglob('*'):
        # Ignorer les rÃ©pertoires exclus
        if any(part in IGNORE_DIRS for part in file_path.parts):
            continue
        
        # VÃ©rifier si le fichier est dans l'allowlist (chemin relatif)
        try:
            relative_path = file_path.relative_to(root_dir)
            if str(relative_path).replace('\\', '/') in ALLOWLIST_FILES:
                if verbose:
                    print(f"âšª IgnorÃ© (allowlist): {file_path}")
                continue
        except ValueError:
            pass
            
        # VÃ©rifier l'extension
        if file_path.suffix.lower() not in extensions:
            continue
            
        if file_path.is_file():
            files_scanned += 1
            if verbose:
                print(f"ðŸ“„ Scan: {file_path}")
                
            issues = find_mojibake_in_file(file_path, patterns, verbose)
            if issues:
                results[file_path] = issues
    
    if verbose:
        print(f"\nðŸ“Š {files_scanned} fichiers scannÃ©s")
    
    return results


def print_results(results: dict, verbose: bool = False):
    """Affiche les rÃ©sultats du scan"""
    if not results:
        print("âœ… Aucun mojibake dÃ©tectÃ© !")
        return
    
    print(f"ðŸ” {len(results)} fichier(s) avec du mojibake potentiel dÃ©tectÃ©:\n")
    
    total_issues = 0
    for file_path, issues in results.items():
        print(f"ðŸ“ {file_path}")
        for line_num, line_content in issues:
            total_issues += 1
            # Limiter l'affichage de la ligne pour la lisibilitÃ©
            display_line = line_content[:100] + "..." if len(line_content) > 100 else line_content
            print(f"   L{line_num:3d}: {display_line}")
        print()
    
    print(f"ðŸŽ¯ Total: {total_issues} ligne(s) problÃ©matique(s)")


def main():
    """Point d'entrÃ©e principal"""
    parser = argparse.ArgumentParser(description="Scanner de mojibake dans les fichiers texte")
    parser.add_argument("--verbose", "-v", action="store_true", 
                       help="Affichage dÃ©taillÃ© du scan")
    parser.add_argument("--extensions", "-e", type=str, 
                       help="Extensions Ã  scanner (ex: .py,.md,.txt)")
    parser.add_argument("--directory", "-d", type=str, default=".",
                       help="RÃ©pertoire Ã  scanner (dÃ©faut: .)")
    
    args = parser.parse_args()
    
    # Parse des extensions
    if args.extensions:
        extensions = {ext.strip() for ext in args.extensions.split(',')}
        if not all(ext.startswith('.') for ext in extensions):
            print("âŒ Les extensions doivent commencer par un point (ex: .py,.md)")
            return 1
    else:
        extensions = DEFAULT_EXTENSIONS
    
    root_dir = Path(args.directory).resolve()
    if not root_dir.exists():
        print(f"âŒ RÃ©pertoire introuvable: {root_dir}")
        return 1
    
    print(f"ðŸ” Scan mojibake dans: {root_dir}")
    print(f"ðŸ“‹ Extensions: {', '.join(sorted(extensions))}")
    print(f"ðŸŽ¯ Motifs recherchÃ©s: {len(MOJIBAKE_PATTERNS)}")
    if args.verbose:
        print(f"ðŸ”Ž Motifs: {', '.join(MOJIBAKE_PATTERNS[:10])}...")
    print()
    
    # Scan
    results = scan_directory(root_dir, extensions, MOJIBAKE_PATTERNS, args.verbose)
    
    # Affichage
    print_results(results, args.verbose)
    
    return 0 if not results else 1


if __name__ == "__main__":
    sys.exit(main())

# ===== scripts/fix_app_types.py =====
#!/usr/bin/env python3
"""
Script de migration automatique des types DataFrame vers List[Dict] dans app.py
Corrige les incompatibilitÃ©s entre l'ancien db.py et le nouveau db_v5.py
"""

import re
from pathlib import Path

def fix_app_py():
    """Corrige automatiquement les incompatibilitÃ©s de types dans app.py"""
    
    app_path = Path("guignomap/app.py")
    content = app_path.read_text(encoding='utf-8')
    
    # Corrections par regex
    corrections = [
        # 1. .empty sur listes -> len() == 0
        (r'if not (\w+)\.empty:', r'if \1:  # Liste non vide'),
        (r'if (\w+)\.empty:', r'if not \1:  # Liste vide'),
        
        # 2. .iterrows() -> enumerate() ou iteration directe
        (r'for _, (\w+) in (\w+)\.iterrows\(\):', r'for \1 in \2:'),
        (r'for (\w+), (\w+) in (\w+)\.iterrows\(\):', r'for \1, \2 in enumerate(\3):'),
        
        # 3. DataFrame['column'] -> list access pour unassigned
        (r"unassigned\['name'\]\.tolist\(\)", r"unassigned"),
        
        # 4. to_csv() sur listes -> DataFrame conversion
        (r'(\w+)\.to_csv\(([^)]+)\)', r'pd.DataFrame(\1).to_csv(\2)'),
        
        # 5. team_streets filtering (fonction get_team_streets retourne List[str])
        (r"team_streets\[team_streets\['status'\] == '(\w+)'\]", r"[s for s in team_streets if hasattr(s, 'status') and s.status == '\1']"),
        
        # 6. AccÃ¨s par index sur dictionnaires (notes)
        (r"note\[(\d+)\]", r"list(note.values())[\1] if isinstance(note, dict) else note[\1]"),
    ]
    
    for pattern, replacement in corrections:
        content = re.sub(pattern, replacement, content)
    
    # Corrections manuelles spÃ©cifiques
    
    # Fix pour get_team_streets qui doit retourner les donnÃ©es complÃ¨tes, pas juste les noms
    content = content.replace(
        'def get_team_streets(team_id: str) -> List[str]:',
        'def get_team_streets(team_id: str) -> List[Dict[str, Any]]:'
    )
    
    # Fix pour l'utilisation de team_streets dans l'interface Ã©quipe
    content = re.sub(
        r'if team_streets\.empty:',
        'if not team_streets:',
        content
    )
    
    content = re.sub(
        r'done_streets = len\(team_streets\[team_streets\[\'status\'\] == \'terminee\'\]\)',
        'done_streets = len([s for s in team_streets if isinstance(s, dict) and s.get("status") == "terminee"])',
        content
    )
    
    content = re.sub(
        r'in_progress = len\(team_streets\[team_streets\[\'status\'\] == \'en_cours\'\]\)',
        'in_progress = len([s for s in team_streets if isinstance(s, dict) and s.get("status") == "en_cours"])',
        content
    )
    
    # Fix pour l'iteration sur team_streets
    content = re.sub(
        r'for street in team_streets:',
        'for street in team_streets:\n            if isinstance(street, str):\n                street_name = street\n            else:\n                street_name = street.get("name", street)',
        content
    )
    
    # Fix pour les notes dans l'affichage
    content = re.sub(
        r'st\.markdown\(f"â€¢ \*\*#{note\[0\]}\*\* : {note\[1\]} _{note\[2\]}_"\)',
        'st.markdown(f"â€¢ **#{note.get(\'address_number\', \'?\')}** : {note.get(\'comment\', \'\')} _{note.get(\'created_at\', \'\')}_ ")',
        content
    )
    
    # Sauvegarder
    app_path.write_text(content, encoding='utf-8')
    print("âœ… app.py corrigÃ© automatiquement")

if __name__ == "__main__":
    fix_app_py()

# ===== scripts/fix_mojibake_files.py =====
#!/usr/bin/env python3
"""
Fix mojibake (corrupted text due to encoding issues) in text files using ftfy.

Usage:
    python scripts/fix_mojibake_files.py [--paths file1 file2 ...] [--apply]
    
Options:
    --paths: Specific file paths to process (space-separated)
    --apply: Actually write changes (default is DRY-RUN mode)
    
Examples:
    # DRY-RUN on specific files
    python scripts/fix_mojibake_files.py --paths guignomap/app.py docs/README.md
    
    # Apply changes to specific files
    python scripts/fix_mojibake_files.py --apply --paths guignomap/app.py docs/README.md
    
    # DRY-RUN on all files in repo
    python scripts/fix_mojibake_files.py
    
    # Apply changes to all files in repo
    python scripts/fix_mojibake_files.py --apply
"""

import argparse
import difflib
import os
import sys
from pathlib import Path
from typing import List, Set

try:
    import ftfy
except ImportError:
    print("Error: ftfy library not found. Install with: pip install ftfy")
    sys.exit(1)


# Exclusions
EXCLUDED_DIRS = {'.git', '.venv', '__pycache__', 'node_modules', '.pytest_cache'}
EXCLUDED_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.gif', '.zip', '.sqlite', '.db', '.xlsx', '.xls', '.ico', '.pyc', '.pyo'}

# Target extensions for text files
TARGET_EXTENSIONS = {'.py', '.md', '.txt', '.json', '.toml', '.csv', '.yml', '.yaml', '.ini', '.cfg', '.conf'}

# Emoji replacements for correcting corrupted emojis (applied after ftfy)
EMOJI_REPLACEMENTS = {
    "Ã”Ã¸Î©": "ðŸ§¾",
    "Ã¼Ã«Ã®": "ðŸ‘¤",
    "Ã¼Ã®Ãª": "ðŸ”‘",
    "Ã¼Ã¶Ã„": "ðŸ”“",
    "Ã¼Ã©Ã–": "ðŸŽ„",
    "Ã¼Ã¬Ã»": "ðŸ“ž",
    "Ã¼Ã¶âˆ‚": "â³",
    "Ã¼Ã«â€¢": "ðŸ‘¥",
    "Ã¼Ã¬Ã¹": "ðŸ“",
    "Ã¼Ã¥Ã¼": "ðŸ†",
    "Ã¼Ã¬Â±": "ðŸ“„",
    "Ã¼Ã©Ã¢": "ðŸŽ‰",
    "Ã¼Ã¬Ã§": "ðŸ“Œ",
    "Ã¼Ã¬Ã¤": "ðŸ“Š",
    "Ã¼Ãµâ€ ": "ðŸ› ï¸",
    "Ã¼Ã­Ã¦": "ðŸ—ƒï¸",
    "Ã¼Ã¬Ã£": "âœ…",
    "Ã¼Ã¬Â§": "ðŸ“¤",
    "Ã¼Ã¬â€¢": "ðŸ“Š",  # Export icon
    "Ã¼Ã¶Â®": "â„¹ï¸",  # Info icon
    "Ã¼Ã®Ã§": "ðŸ”§",  # Tools icon
    "'ÃºÃ–": "âœ…",  # Success icon
    "'Ã¹Ã¥": "âŒ",  # Error icon
    "â– ": "",      # Remove square boxes
    "â–¡": ""       # Remove empty squares
}


def is_excluded_path(path: Path) -> bool:
    """Check if path should be excluded from processing."""
    # Skip files in excluded directories
    for part in path.parts:
        if part in EXCLUDED_DIRS:
            return True
    
    # Skip files with excluded extensions
    if path.suffix.lower() in EXCLUDED_EXTENSIONS:
        return True
    
    # Skip exports directory for binary/data files
    if 'exports' in path.parts and path.suffix.lower() in {'.csv', '.txt'}:
        # Allow some exports but be careful with data files
        if any(name in path.name.lower() for name in ['export_', 'sanity_', 'streets_template']):
            return True
    
    return False


def should_process_file(path: Path) -> bool:
    """Check if file should be processed (is a target text file)."""
    if is_excluded_path(path):
        return False
    
    # Only process files with target extensions
    if path.suffix.lower() not in TARGET_EXTENSIONS:
        return False
    
    # Must be a regular file
    if not path.is_file():
        return False
    
    return True


def get_files_to_process(base_path: Path, specific_paths: List[str] | None = None) -> List[Path]:
    """Get list of files to process."""
    files = []
    
    if specific_paths:
        # Process specific files
        for path_str in specific_paths:
            path = base_path / path_str
            if path.exists() and path.is_file():
                if should_process_file(path):
                    files.append(path)
                else:
                    print(f"Warning: Skipping excluded file: {path}")
            else:
                print(f"Warning: File not found: {path}")
    else:
        # Process all files recursively
        for root, dirs, filenames in os.walk(base_path):
            # Remove excluded directories from dirs list to skip them
            dirs[:] = [d for d in dirs if d not in EXCLUDED_DIRS]
            
            for filename in filenames:
                path = Path(root) / filename
                if should_process_file(path):
                    files.append(path)
    
    return sorted(files)


def fix_file_mojibake(file_path: Path, dry_run: bool = True) -> tuple[bool, str, str, str]:
    """
    Fix mojibake in a single file using ftfy and custom emoji replacements.
    
    Returns:
        (has_changes, original_content, fixed_content, diff_summary)
    """
    try:
        # Read file content
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            original_content = f.read()
        
        # Fix mojibake using ftfy
        fixed_content = ftfy.fix_text(original_content)
        
        # Apply emoji replacements for .py, .md, .txt files
        if file_path.suffix.lower() in {'.py', '.md', '.txt'}:
            for corrupted, correct in EMOJI_REPLACEMENTS.items():
                fixed_content = fixed_content.replace(corrupted, correct)
        
        # Check if there are changes
        has_changes = original_content != fixed_content
        
        if not has_changes:
            return False, original_content, fixed_content, "No changes needed"
        
        # Generate diff summary
        diff_lines = list(difflib.unified_diff(
            original_content.splitlines(keepends=True),
            fixed_content.splitlines(keepends=True),
            fromfile=f"a/{file_path}",
            tofile=f"b/{file_path}",
            n=2
        ))
        
        # Count changed lines
        changed_lines = sum(1 for line in diff_lines if line.startswith('+') or line.startswith('-'))
        diff_summary = f"{changed_lines} lines changed"
        
        # If not dry run, write the file
        if not dry_run:
            with open(file_path, 'w', encoding='utf-8', newline='') as f:
                f.write(fixed_content)
        
        return has_changes, original_content, fixed_content, diff_summary
        
    except Exception as e:
        return False, "", "", f"Error: {e}"


def show_diff_preview(original: str, fixed: str, max_lines: int = 10) -> str:
    """Show a preview of the diff."""
    diff_lines = list(difflib.unified_diff(
        original.splitlines(keepends=True),
        fixed.splitlines(keepends=True),
        fromfile="original",
        tofile="fixed",
        n=2
    ))
    
    if len(diff_lines) <= max_lines:
        return ''.join(diff_lines)
    else:
        preview = diff_lines[:max_lines]
        preview.append(f"... ({len(diff_lines) - max_lines} more lines)\n")
        return ''.join(preview)


def main():
    parser = argparse.ArgumentParser(description='Fix mojibake in text files using ftfy')
    parser.add_argument('--apply', action='store_true', 
                       help='Actually apply changes (default is DRY-RUN)')
    parser.add_argument('--paths', nargs='*', 
                       help='Specific file paths to process (relative to repo root)')
    parser.add_argument('--show-diff', action='store_true',
                       help='Show diff preview for files with changes')
    
    args = parser.parse_args()
    
    # Get repository root
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent
    
    print(f"Processing files in: {repo_root}")
    print(f"Mode: {'APPLY CHANGES' if args.apply else 'DRY-RUN (no changes written)'}")
    print()
    
    # Get files to process
    files_to_process = get_files_to_process(repo_root, args.paths)
    
    if not files_to_process:
        print("No files to process.")
        return
    
    print(f"Found {len(files_to_process)} files to process:")
    for file_path in files_to_process:
        rel_path = file_path.relative_to(repo_root)
        print(f"  {rel_path}")
    print()
    
    # Process files
    changed_files = []
    error_files = []
    
    for file_path in files_to_process:
        rel_path = file_path.relative_to(repo_root)
        print(f"Processing: {rel_path}")
        
        has_changes, original, fixed, summary = fix_file_mojibake(file_path, dry_run=not args.apply)
        
        if "Error:" in summary:
            print(f"  âŒ {summary}")
            error_files.append(rel_path)
        elif has_changes:
            print(f"  âœ… {summary}")
            changed_files.append(rel_path)
            
            if args.show_diff:
                print("  Diff preview:")
                diff_preview = show_diff_preview(original, fixed)
                for line in diff_preview.splitlines():
                    print(f"    {line}")
                print()
        else:
            print(f"  âœ… {summary}")
    
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"Files processed: {len(files_to_process)}")
    print(f"Files with changes: {len(changed_files)}")
    print(f"Files with errors: {len(error_files)}")
    
    if changed_files:
        print()
        print("Files with changes:")
        for file_path in changed_files:
            status = "APPLIED" if args.apply else "WOULD CHANGE"
            print(f"  {status}: {file_path}")
    
    if error_files:
        print()
        print("Files with errors:")
        for file_path in error_files:
            print(f"  ERROR: {file_path}")
    
    if not args.apply and changed_files:
        print()
        print("To apply these changes, run with --apply flag:")
        if args.paths:
            paths_str = ' '.join(f'"{p}"' for p in args.paths)
            print(f"  python scripts/fix_mojibake_files.py --apply --paths {paths_str}")
        else:
            print("  python scripts/fix_mojibake_files.py --apply")


if __name__ == '__main__':
    main()

# ===== scripts/fix_specific.py =====
#!/usr/bin/env python3
"""
Script de correction fine pour les types de retour dans app.py
"""

import re
from pathlib import Path

def fix_specific_issues():
    """Corrige les problÃ¨mes spÃ©cifiques identifiÃ©s"""
    
    app_path = Path("guignomap/app.py")
    content = app_path.read_text(encoding='utf-8')
    
    # 1. Revert les corrections sur DataFrames (list_streets retourne bien un DataFrame)
    content = re.sub(
        r'if df_all:  # Liste non vide',
        'if not df_all.empty:',
        content
    )
    
    content = re.sub(
        r'if not df_team:  # Liste vide',
        'if df_team.empty:',
        content
    )
    
    # 2. Fix team_streets access patterns
    content = re.sub(
        r'done_streets = len\(\[s for s in team_streets if hasattr\(s, \'status\'\) and s\.status == \'terminee\'\]\)',
        'done_streets = len([s for s in team_streets if s.get("status") == "terminee"])',
        content
    )
    
    content = re.sub(
        r'in_progress = len\(\[s for s in team_streets if hasattr\(s, \'status\'\) and s\.status == \'en_cours\'\]\)',
        'in_progress = len([s for s in team_streets if s.get("status") == "en_cours"])',
        content
    )
    
    # 3. Fix row access in iterrows (certains endroits ont encore des DataFrames)
    content = re.sub(
        r"for row in df_team:\s*street = row\['name'\]\s*status = row\['status'\]\s*notes_count = row\.get\('notes', 0\)",
        """for _, row in df_team.iterrows():
            street = row['name']
            status = row['status'] 
            notes_count = row.get('notes', 0)""",
        content, flags=re.MULTILINE
    )
    
    app_path.write_text(content, encoding='utf-8')
    print("âœ… Corrections spÃ©cifiques appliquÃ©es")

if __name__ == "__main__":
    fix_specific_issues()

# ===== scripts/generate_tree_clean.py =====
#!/usr/bin/env python3
"""
Script pour gÃ©nÃ©rer un arbre propre du projet GuignoMap
Exclut les dossiers polluants comme __pycache__, .venv, backups, exports
"""

import os
from pathlib import Path
from datetime import datetime

def should_exclude(path: Path) -> bool:
    """DÃ©termine si un chemin doit Ãªtre exclu de l'arbre"""
    excludes = {
        '__pycache__',
        '.venv', 
        'backups',
        'exports',
        '.git',
        'node_modules',
        '.pytest_cache',
        '.coverage',
        '.mypy_cache'
    }
    
    # Exclure les dossiers/fichiers spÃ©cifiques
    if path.name in excludes:
        return True
    
    # Exclure les fichiers temporaires
    if path.suffix in {'.pyc', '.pyo', '.pyd', '.so', '.dll'}:
        return True
    
    # Exclure les fichiers de logs
    if path.suffix in {'.log'}:
        return True
        
    # Exclure les bases de donnÃ©es temporaires
    if path.suffix in {'.db', '.sqlite', '.sqlite3'} and 'test' not in path.name.lower():
        return True
    
    # Excluer les fichiers d'export prÃ©cÃ©dents
    if path.name.startswith('export_') and path.suffix == '.txt':
        return True
        
    # Exclure les fichiers zip de backup
    if path.suffix == '.zip' and any(parent.name == 'backups' for parent in path.parents):
        return True
    
    return False

def generate_tree(root_path: Path, prefix: str = "", is_last: bool = True, max_depth: int = 10, current_depth: int = 0) -> list:
    """GÃ©nÃ¨re un arbre des fichiers et dossiers"""
    if current_depth >= max_depth:
        return []
    
    tree_lines = []
    
    if current_depth == 0:
        tree_lines.append(f"{root_path.name}/")
    
    try:
        # Lister tous les Ã©lÃ©ments du dossier
        items = []
        for item in root_path.iterdir():
            if not should_exclude(item):
                items.append(item)
        
        # Trier : dossiers d'abord, puis fichiers, alphabÃ©tiquement
        items.sort(key=lambda x: (x.is_file(), x.name.lower()))
        
        for i, item in enumerate(items):
            is_last_item = i == len(items) - 1
            
            # CrÃ©er le prÃ©fixe pour cet Ã©lÃ©ment
            connector = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
            current_prefix = prefix + connector
            
            if item.is_dir():
                # Dossier
                tree_lines.append(f"{current_prefix}{item.name}/")
                
                # RÃ©cursion pour le contenu du dossier
                next_prefix = prefix + ("    " if is_last_item else "â”‚   ")
                subtree = generate_tree(item, next_prefix, is_last_item, max_depth, current_depth + 1)
                tree_lines.extend(subtree)
            else:
                # Fichier
                size_info = ""
                try:
                    size = item.stat().st_size
                    if size < 1024:
                        size_info = f" ({size}B)"
                    elif size < 1024 * 1024:
                        size_info = f" ({size/1024:.1f}KB)"
                    else:
                        size_info = f" ({size/(1024*1024):.1f}MB)"
                except:
                    size_info = ""
                
                tree_lines.append(f"{current_prefix}{item.name}{size_info}")
    
    except PermissionError:
        tree_lines.append(f"{prefix}â””â”€â”€ [Permission denied]")
    
    return tree_lines

def main():
    """Fonction principale"""
    project_root = Path(__file__).parent
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print("ðŸŒ³ GÃ©nÃ©ration de l'arbre du projet GuignoMap...")
    
    # GÃ©nÃ©rer l'arbre
    tree_lines = generate_tree(project_root)
    
    # CrÃ©er le contenu du fichier
    content = f"""# ================================================================================
# ARBRE DU PROJET GUIGNOMAP - STRUCTURE COMPLÃˆTE
# GÃ©nÃ©rÃ© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Exclusions: __pycache__, .venv, backups/, exports/, .git, *.pyc, *.log
# ================================================================================

"""
    
    content += "\n".join(tree_lines)
    
    # Ajouter des statistiques
    total_lines = len(tree_lines)
    content += f"""

# ================================================================================
# STATISTIQUES
# ================================================================================

ðŸ“Š Total Ã©lÃ©ments affichÃ©s: {total_lines}
ðŸš« Exclusions appliquÃ©es: __pycache__, .venv, backups, exports, *.pyc, *.log, *.db
ðŸ“… GÃ©nÃ©rÃ© le: {datetime.now().strftime('%Y-%m-%d Ã  %H:%M:%S')}

# ================================================================================
# LÃ‰GENDE
# ================================================================================

ðŸ“ Dossiers se terminent par /
ðŸ“„ Fichiers avec taille approximative
â”œâ”€â”€ Ã‰lÃ©ment dans la hiÃ©rarchie  
â””â”€â”€ Dernier Ã©lÃ©ment d'un niveau
â”‚   Continuation de branche
    Espacement pour sous-Ã©lÃ©ments

# ================================================================================
# NOTES IMPORTANTES
# ================================================================================

âœ… INCLUS:
- Tous les fichiers source (.py, .css, .toml, .md, etc.)
- Configuration et documentation 
- Scripts utilitaires et outils
- Fichiers de dÃ©ploiement

âŒ EXCLUS (polluants):
- __pycache__/ et *.pyc (cache Python)
- .venv/ (environnement virtuel)
- backups/ (sauvegardes automatiques)
- exports/ (exports prÃ©cÃ©dents)
- .git/ (mÃ©tadonnÃ©es Git)
- Fichiers temporaires et logs

===============================================================================
FIN ARBRE PROJET GUIGNOMAP - {timestamp}
===============================================================================
"""
    
    # Sauvegarder le fichier
    output_file = project_root / f"project_tree_clean_{timestamp}.txt"
    output_file.write_text(content, encoding='utf-8')
    
    print(f"âœ… Arbre gÃ©nÃ©rÃ©: {output_file.name}")
    print(f"ðŸ“Š {total_lines} Ã©lÃ©ments inclus")
    print(f"ðŸ“„ Fichier: {output_file}")

if __name__ == "__main__":
    main()

# ===== scripts/import_city_excel.py =====
#!/usr/bin/env python3
"""
GuignoMap - Import d'adresses depuis Excel pour une ville
CLI rÃ©utilisable pour l'import autoritatif d'adresses
"""
import sys
import argparse
import logging
from pathlib import Path
import pandas as pd

# Ajouter le rÃ©pertoire parent au path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from guignomap.imports import detect_schema, prepare_dataframe, authoritative_swap
from guignomap.database import get_conn


def setup_logging(verbose: bool = False):
    """Configure le logging"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )


def import_city_addresses(city: str, file_path: Path, verbose: bool = False) -> bool:
    """
    Importe les adresses d'une ville depuis un fichier Excel
    
    Returns:
        bool: True si succÃ¨s, False sinon
    """
    setup_logging(verbose)
    
    try:
        # 1. Lecture du fichier Excel
        logging.info(f"ðŸ“ Lecture de {file_path}")
        if not file_path.exists():
            logging.error(f"Fichier non trouvÃ©: {file_path}")
            return False
        
        df = pd.read_excel(file_path)
        logging.info(f"ðŸ“Š Fichier Excel: {len(df)} lignes, {len(df.columns)} colonnes")
        
        # 2. DÃ©tection automatique du schÃ©ma
        mapping = detect_schema(df, city)
        if 'street' not in mapping or 'number' not in mapping:
            logging.error("Impossible de dÃ©tecter les colonnes rue/numÃ©ro obligatoires")
            logging.error(f"Colonnes disponibles: {list(df.columns)}")
            return False
        
        logging.info(f"ðŸ” SchÃ©ma dÃ©tectÃ©: {mapping}")
        
        # 3. PrÃ©paration des donnÃ©es
        logging.info("ðŸ”§ PrÃ©paration des donnÃ©es...")
        df_prepared = prepare_dataframe(df, mapping, city)
        logging.info(f"âœ… DonnÃ©es prÃ©parÃ©es: {len(df_prepared)} adresses uniques")
        
        # 4. Import en base de donnÃ©es
        logging.info("ðŸ’¾ Import en base de donnÃ©es...")
        with get_conn() as conn:
            stats = authoritative_swap(conn, df_prepared)
        
        # 5. Rapport final
        logging.info("ðŸ“ˆ RAPPORT D'IMPORT")
        logging.info(f"  â€¢ Total Excel original: {len(df):,}")
        logging.info(f"  â€¢ Adresses uniques: {len(df_prepared):,}")
        logging.info(f"  â€¢ Doublons ignorÃ©s: {len(df) - len(df_prepared):,}")
        logging.info(f"  â€¢ Total final en DB: {stats['total_imported']:,}")
        logging.info(f"  â€¢ Assignations prÃ©servÃ©es: {stats['preserved_assignments']:,}")
        
        if len(df) > 0:
            delta_pct = (len(df_prepared) / len(df)) * 100
            logging.info(f"  â€¢ Delta (%): {delta_pct:.1f}%")
        
        logging.info(f"ðŸŽ¯ Import de {city} terminÃ© avec succÃ¨s!")
        return True
        
    except Exception as e:
        logging.error(f"âŒ Erreur lors de l'import: {e}")
        if verbose:
            logging.exception("DÃ©tails de l'erreur:")
        return False


def main():
    """Point d'entrÃ©e principal"""
    parser = argparse.ArgumentParser(
        description="Import d'adresses depuis Excel pour GuignoMap",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  %(prog)s --city mascouche --file imports/mascouche_adresses.xlsx
  %(prog)s --city montreal --file data/mtl.xlsx --verbose
        """
    )
    
    parser.add_argument(
        '--city',
        required=True,
        help='Nom de la ville (utilisÃ© pour les logs et la dÃ©tection)'
    )
    
    parser.add_argument(
        '--file',
        required=True,
        type=Path,
        help='Chemin vers le fichier Excel Ã  importer'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Affichage dÃ©taillÃ© (debug)'
    )
    
    args = parser.parse_args()
    
    # ExÃ©cution de l'import
    success = import_city_addresses(args.city, args.file, args.verbose)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())

# ===== scripts/import_from_excel.py =====
#!/usr/bin/env python3#!/usr/bin/env python3#!/usr/bin/env python3

"""GuignoMap - Wrapper d'import pour Mascouche"""

import sys""""""

import subprocess

from pathlib import PathGuignoMap - Wrapper d'import pour MascoucheGuignoMap - Wrapper d'import pour Mascouche



def main():Wrapper du script import_city_excel.py pour maintenir la compatibilitÃ©Wrapper du script import_city_excel.py pour maintenir la compatibilitÃ©

    script_dir = Path(__file__).parent

    import_script = script_dir / "import_city_excel.py"""""""

    excel_file = script_dir.parent / "imports" / "mascouche_adresses.xlsx"

    import sysimport sys

    print("Lancement de l'import Mascouche...")

    import subprocessimport subprocess

    cmd = [

        sys.executable,from pathlib import Pathfrom pathlib import Path

        str(import_script),

        "--city", "mascouche",

        "--file", str(excel_file)

    ]def main():def main():

    

    try:    """Wrapper qui appelle import_city_excel.py avec les paramÃ¨tres Mascouche"""    """Wrapper qui appelle import_city_excel.py avec les paramÃ¨tres Mascouche"""

        result = subprocess.run(cmd, check=False)

        return result.returncode    script_dir = Path(__file__).parent    script_dir = Path(__file__).parent

    except Exception as e:

        print(f"Erreur: {e}")    import_script = script_dir / "import_city_excel.py"    import_script = script_dir / "import_city_excel.py"

        return 1

    excel_file = script_dir.parent / "imports" / "mascouche_adresses.xlsx"    excel_file = script_dir.parent / "imports" / "mascouche_adresses.xlsx"

if __name__ == "__main__":

    sys.exit(main())        

    print("ðŸ”„ Lancement de l'import Mascouche...")    print("ðŸ”„ Lancement de l'import Mascouche...")

    print(f"Script: {import_script}")    print(f"Script: {import_script}")

    print(f"Fichier: {excel_file}")    print(f"Fichier: {excel_file}")

        

    # Appel du script principal    # Appel du script principal

    cmd = [    cmd = [

        sys.executable,        sys.executable,

        str(import_script),        str(import_script),

        "--city", "mascouche",        "--city", "mascouche",

        "--file", str(excel_file)        "--file", str(excel_file)

    ]    ]

        

    try:    try:

        result = subprocess.run(cmd, check=False)        result = subprocess.run(cmd, check=False)

        return result.returncode        return result.returncode

    except Exception as e:    except Exception as e:

        print(f"âŒ Erreur lors de l'exÃ©cution: {e}")        print(f"âŒ Erreur lors de l'exÃ©cution: {e}")

        return 1        return 1





if __name__ == "__main__":if __name__ == "__main__":

    sys.exit(main())    sys.exit(main())
    
    # VÃ©rifier si c'est une chaÃ®ne vide ou "nan" (insensible Ã  la casse)
    if str_x == "" or str_x.lower() == "nan":
        return True
    
    return False

def _build_street(nomrue, odoparti, odospeci):
    """Construit le nom de rue en privilÃ©giant nomrue, sinon en joignant odoparti + odospeci"""
    # Si nomrue est fiable, l'utiliser directement
    if not _is_null(nomrue):
        return str(nomrue).strip()
    
    # Sinon, joindre proprement odoparti et odospeci en ignorant les valeurs nulles
    parts = []
    
    if not _is_null(odoparti):
        parts.append(str(odoparti).strip())
    
    if not _is_null(odospeci):
        parts.append(str(odospeci).strip())
    
    # Joindre avec un seul espace (pas d'espaces doubles)
    return " ".join(parts) if parts else ""

def _normalize_text(s):
    """Normalise un texte pour crÃ©er une clÃ© d'adresse unique - NE JAMAIS retourner 'nan'"""
    if _is_null(s):
        return ""
    
    # Convertir en string et strip
    text = str(s).strip()
    
    # VÃ©rifier encore une fois "nan" aprÃ¨s conversion string
    if text.lower() == "nan":
        return ""
    
    # Enlever les accents (NFD normalization puis supprimer les diacritiques)
    text = unicodedata.normalize('NFD', text)
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    
    # Remplacer apostrophes typographiques par simples
    text = text.replace("'", "'").replace("'", "'")
    
    # Minuscules
    text = text.lower()
    
    # Compresser les espaces multiples
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def read_and_prepare_excel():
    """Lit le fichier Excel et prÃ©pare les donnÃ©es avec addr_key"""
    if not EXCEL_PATH.exists():
        raise FileNotFoundError(f"Fichier Excel introuvable: {EXCEL_PATH}")

    print(f"ðŸ“– Lecture du fichier Excel: {EXCEL_PATH}")
    df = pd.read_excel(EXCEL_PATH)
    print(f"ðŸ“Š {len(df)} lignes lues depuis Excel")
    
    # Afficher les colonnes disponibles
    print(f"ðŸ“‹ Colonnes disponibles: {sorted(df.columns.tolist())}")
    
    # VÃ©rifier les colonnes requises pour le schÃ©ma Mascouche rÃ©el
    required_cols = {"NoCiv", "nomrue"}
    available_cols = set(df.columns)
    
    if not required_cols.issubset(available_cols):
        missing = required_cols - available_cols
        raise ValueError(f"Colonnes manquantes: {missing}. Colonnes disponibles: {sorted(available_cols)}")
    
    print("ðŸ” DÃ©tection du schÃ©ma Mascouche (colonnes rÃ©elles)")
    
    # Construire les donnÃ©es normalisÃ©es
    result_data = []
    
    for _, row in df.iterrows():
        # street_name: utiliser le nouveau helper _build_street
        street_name = _build_street(
            row.get("nomrue"), 
            row.get("OdoParti"), 
            row.get("OdoSpeci")
        )
        
        # house_number: NoCiv + NoCivSuf (si non nul)
        house_number = str(row["NoCiv"]).strip()
        if not _is_null(row.get("NoCivSuf")):
            house_number += " " + str(row["NoCivSuf"]).strip()
        
        # postal_code: pas disponible, laisser None
        postal_code = None
        
        # sector: pas disponible, laisser None
        sector = None
        
        # CrÃ©er la clÃ© d'adresse normalisÃ©e (JAMAIS de "nan")
        addr_key = f"{_normalize_text(street_name)}|{_normalize_text(house_number)}|{_normalize_text(postal_code or '')}"
        
        result_data.append({
            "street_name": street_name,
            "house_number": house_number,
            "postal_code": postal_code,
            "sector": sector,
            "addr_key": addr_key
        })
    
    result_df = pd.DataFrame(result_data)
    print(f"âœ… {len(result_df)} adresses prÃ©parÃ©es avec clÃ©s normalisÃ©es")
    
    return result_df

def create_staging_table(conn):
    """CrÃ©e la table staging pour l'import"""
    print("ðŸ—ï¸ CrÃ©ation de la table staging")
    
    conn.execute("DROP TABLE IF EXISTS addresses_staging")
    conn.execute("""
        CREATE TABLE addresses_staging (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            street_name TEXT NOT NULL,
            house_number TEXT NOT NULL,
            postal_code TEXT,
            sector TEXT,
            addr_key TEXT NOT NULL UNIQUE
        )
    """)
    
    # Index pour performance
    conn.execute("CREATE UNIQUE INDEX idx_staging_addr_key ON addresses_staging(addr_key)")
    
    print("âœ… Table staging crÃ©Ã©e")

def insert_into_staging(conn, df):
    """InsÃ¨re les donnÃ©es dans la table staging avec dÃ©duplication"""
    print(f"ðŸ“¥ Insertion de {len(df)} adresses dans staging")
    
    cursor = conn.cursor()
    inserted_count = 0
    ignored_count = 0
    
    for _, row in df.iterrows():
        try:
            cursor.execute("""
                INSERT OR IGNORE INTO addresses_staging 
                (street_name, house_number, postal_code, sector, addr_key)
                VALUES (?, ?, ?, ?, ?)
            """, (
                row["street_name"],
                row["house_number"], 
                row["postal_code"],
                row["sector"],
                row["addr_key"]
            ))
            
            if cursor.rowcount > 0:
                inserted_count += 1
            else:
                ignored_count += 1
                
        except Exception as e:
            print(f"âŒ Erreur insertion ligne: {e}")
            ignored_count += 1
    
    conn.commit()
    
    print(f"âœ… Staging: {inserted_count} insÃ©rÃ©es, {ignored_count} doublons ignorÃ©s")
    return inserted_count, ignored_count

def atomic_swap_tables(conn):
    """Ã‰change atomique des tables avec prÃ©servation des assignements"""
    print("ðŸ”„ Ã‰change atomique des tables")
    
    # Nettoyer les tables temporaires qui pourraient exister
    conn.execute("DROP TABLE IF EXISTS addresses_new")
    conn.execute("DROP TABLE IF EXISTS addresses_old")
    
    # VÃ©rifier si addresses existe
    tables = {row[0] for row in conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()}
    addresses_exists = "addresses" in tables
    
    if addresses_exists:
        print("ðŸ“‹ Sauvegarde de l'ancienne table addresses")
        conn.execute("ALTER TABLE addresses RENAME TO addresses_old")
    
    # CrÃ©er la nouvelle table addresses avec schÃ©ma complet
    print("ðŸ—ï¸ CrÃ©ation de la nouvelle table addresses")
    conn.execute("""
        CREATE TABLE addresses_new (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            street_name TEXT NOT NULL,
            house_number TEXT NOT NULL,
            latitude REAL,
            longitude REAL,
            osm_type TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            postal_code TEXT,
            sector TEXT,
            assigned_to TEXT,
            addr_key TEXT NOT NULL UNIQUE
        )
    """)
    
    # Copier les donnÃ©es depuis staging
    print("ðŸ“‹ Copie des donnÃ©es depuis staging")
    conn.execute("""
        INSERT INTO addresses_new (street_name, house_number, postal_code, sector, addr_key)
        SELECT street_name, house_number, postal_code, sector, addr_key
        FROM addresses_staging
    """)
    
    # PrÃ©server les assignements si possible
    preserved_count = 0
    if addresses_exists:
        print("ðŸ”— PrÃ©servation des assignements existants")
        
        # VÃ©rifier si l'ancienne table a une colonne addr_key
        old_columns = {row[1] for row in conn.execute("PRAGMA table_info(addresses_old)").fetchall()}
        
        if "addr_key" in old_columns:
            # L'ancienne table a dÃ©jÃ  addr_key (rÃ©import)
            cursor = conn.execute("""
                UPDATE addresses_new 
                SET assigned_to = (
                    SELECT assigned_to 
                    FROM addresses_old 
                    WHERE addresses_old.addr_key = addresses_new.addr_key 
                    AND COALESCE(addresses_old.assigned_to, '') <> ''
                )
                WHERE EXISTS (
                    SELECT 1 
                    FROM addresses_old 
                    WHERE addresses_old.addr_key = addresses_new.addr_key 
                    AND COALESCE(addresses_old.assigned_to, '') <> ''
                )
            """)
            preserved_count = cursor.rowcount
        else:
            # L'ancienne table n'a pas addr_key, faire correspondance par street_name + house_number
            print("âš ï¸ Ancienne table sans addr_key, correspondance par rue+numÃ©ro")
            cursor = conn.execute("""
                UPDATE addresses_new 
                SET assigned_to = (
                    SELECT assigned_to 
                    FROM addresses_old 
                    WHERE LOWER(TRIM(addresses_old.street_name)) = LOWER(TRIM(addresses_new.street_name))
                    AND LOWER(TRIM(addresses_old.house_number)) = LOWER(TRIM(addresses_new.house_number))
                    AND COALESCE(addresses_old.assigned_to, '') <> ''
                    LIMIT 1
                )
                WHERE EXISTS (
                    SELECT 1 
                    FROM addresses_old 
                    WHERE LOWER(TRIM(addresses_old.street_name)) = LOWER(TRIM(addresses_new.street_name))
                    AND LOWER(TRIM(addresses_old.house_number)) = LOWER(TRIM(addresses_new.house_number))
                    AND COALESCE(addresses_old.assigned_to, '') <> ''
                )
            """)
            preserved_count = cursor.rowcount
        
        print(f"âœ… {preserved_count} assignements prÃ©servÃ©s")
    
    # Supprimer les index existants pour rendre l'opÃ©ration idempotente
    print("ðŸ—‘ï¸ Suppression des index existants")
    conn.execute("DROP INDEX IF EXISTS idx_addr_key")
    conn.execute("DROP INDEX IF EXISTS idx_addresses_street")
    conn.execute("DROP INDEX IF EXISTS idx_addresses_sector")
    conn.execute("DROP INDEX IF EXISTS idx_addresses_assigned")
    
    # CrÃ©er les index
    print("ðŸ”— CrÃ©ation des index")
    conn.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_addr_key ON addresses_new(addr_key)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_addr_sector ON addresses_new(sector)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_addr_assigned ON addresses_new(assigned_to)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_addr_street ON addresses_new(street_name)")
    
    # Finaliser l'Ã©change
    conn.execute("DROP TABLE IF EXISTS addresses")
    conn.execute("ALTER TABLE addresses_new RENAME TO addresses")
    
    # Nettoyer
    conn.execute("DROP TABLE IF EXISTS addresses_old")
    conn.execute("DROP TABLE IF EXISTS addresses_staging")
    
    print("âœ… Ã‰change atomique terminÃ©")
    return preserved_count

def import_mascouche_addresses_authoritative():
    """Import Excel en mode autoritatif avec staging et prÃ©servation assignements"""
    print("=== Import Excel Autoritatif ===")
    
    # 1. Lire et prÃ©parer les donnÃ©es Excel
    df = read_and_prepare_excel()
    total_excel = len(df)
    
    # 2. Traitement en base
    with sqlite3.connect(DB_PATH) as conn:
        # 3. CrÃ©er table staging
        create_staging_table(conn)
        
        # 4. InsÃ©rer dans staging avec dÃ©duplication
        inserted_count, ignored_count = insert_into_staging(conn, df)
        
        # 5. Ã‰change atomique avec prÃ©servation assignements
        preserved_count = atomic_swap_tables(conn)
        
        # 6. Statistiques finales
        final_count = conn.execute("SELECT COUNT(*) FROM addresses").fetchone()[0]
        
        conn.commit()
    
    # 7. Rapport final
    print("\n=== RAPPORT FINAL ===")
    print(f"ðŸ“Š Total lignes Excel: {total_excel}")
    print(f"ðŸ“Š Uniques en staging: {inserted_count}")
    print(f"ðŸ“Š Doublons Excel ignorÃ©s: {ignored_count}")
    print(f"ðŸ“Š Total final addresses: {final_count}")
    print(f"ðŸ“Š Assignements prÃ©servÃ©s: {preserved_count}")
    print("âœ… Import autoritatif terminÃ© avec succÃ¨s!")

if __name__ == "__main__":
    import_mascouche_addresses_authoritative()

# ===== scripts/migrate_address_keys.py =====
#!/usr/bin/env python3
"""
Script de migration pour rÃ©gÃ©nÃ©rer les clÃ©s addr_key selon la nouvelle logique
"""
import sqlite3
import logging
import sys
import os

# Ajouter le rÃ©pertoire parent au PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from guignomap.imports import build_addr_key

# Configuration du logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def migrate_address_keys():
    """RÃ©gÃ©nÃ¨re toutes les clÃ©s addr_key selon la nouvelle logique de normalisation"""
    
    db_path = "guignomap/guigno_map.db"
    
    logger.info(f"ðŸ”„ Migration des clÃ©s addr_key dans {db_path}")
    
    # Connexion Ã  la DB
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    try:
        # 1. RÃ©cupÃ©rer toutes les adresses
        cursor.execute("SELECT id, street_name, house_number, postal_code FROM addresses")
        addresses = cursor.fetchall()
        
        logger.info(f"ðŸ“Š {len(addresses)} adresses Ã  traiter")
        
        # 2. RÃ©gÃ©nÃ©rer les clÃ©s
        updated_count = 0
        
        for addr_id, street_name, house_number, postal_code in addresses:
            # GÃ©nÃ©rer la nouvelle clÃ©
            new_key = build_addr_key(street_name, house_number, postal_code)
            
            # Mettre Ã  jour en DB
            cursor.execute(
                "UPDATE addresses SET addr_key = ? WHERE id = ?",
                (new_key, addr_id)
            )
            updated_count += 1
            
            if updated_count % 1000 == 0:
                logger.info(f"  TraitÃ©: {updated_count}/{len(addresses)}")
        
        # 3. Commit des changements
        conn.commit()
        logger.info(f"âœ… Migration terminÃ©e: {updated_count} clÃ©s rÃ©gÃ©nÃ©rÃ©es")
        
        # 4. VÃ©rification des doublons
        cursor.execute("SELECT addr_key, COUNT(*) as count FROM addresses GROUP BY addr_key HAVING count > 1")
        duplicates = cursor.fetchall()
        
        if duplicates:
            logger.warning(f"âš ï¸  {len(duplicates)} clÃ©s dupliquÃ©es dÃ©tectÃ©es aprÃ¨s migration")
            for key, count in duplicates[:5]:  # Afficher les 5 premiers
                logger.warning(f"   {key}: {count} occurrences")
        else:
            logger.info("âœ… Aucun doublon dÃ©tectÃ©")
            
        # 5. Statistiques finales
        cursor.execute("SELECT COUNT(*) FROM addresses")
        total_addresses = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT addr_key) FROM addresses")
        unique_keys = cursor.fetchone()[0]
        
        logger.info(f"ðŸ“ˆ Statistiques finales:")
        logger.info(f"   Total adresses: {total_addresses}")
        logger.info(f"   ClÃ©s uniques: {unique_keys}")
        logger.info(f"   Ratio unicitÃ©: {unique_keys/total_addresses*100:.1f}%")
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de la migration: {e}")
        conn.rollback()
        raise
    finally:
        conn.close()

if __name__ == "__main__":
    migrate_address_keys()

# ===== scripts/normalize_encoding.py =====
from pathlib import Path
import sys
import io
from typing import Iterable
try:
    # installÃ© via reportlab -> charset-normalizer
    from charset_normalizer import from_path as cn_from_path
except Exception:
    cn_from_path = None

TARGET_NEWLINE = "\n"

DEFAULT_FILES = [
    r"check_admin.py",
    r"README.md",
    r"README_VENV.md",
    r"RESUME_ARBRES_EXPORTS.md",
    r"docs\PHASE1_AUDIT_DATAFRAME.md",
    # Legacy doc moved to legacy/PROBLEME_IPV6_SUPABASE.md
    r"guignomap\app.py",
    r"guignomap\backup.py",
    # Legacy db_v5.py removed
    r"guignomap\osm.py",
    r"scripts\find_mojibake.py",
    # Legacy scripts moved to legacy/scripts/
    r"scripts\generate_audit_optimise.py",
    r"scripts\generate_tree_clean.py",
    # Legacy migration scripts moved to legacy/scripts/
    r"scripts\run_all_tests.py",
    r"scripts\show_hash_stats.py",
    r"scripts\validate_structure.py",
    r"src\database\operations.py",
    r"src\storage\__init__.py",
    r"src\utils\adapters.py",
    r"tests\auth\test_passwords_smoke.py",
    r"tools\quick_sanity.py",
]

def detect_encoding(path: Path) -> str:
    # 1) essayer utf-8 / utf-8-sig en lecture stricte
    for enc in ("utf-8", "utf-8-sig"):
        try:
            path.read_text(encoding=enc)
            return enc
        except Exception:
            pass
    # 2) charset-normalizer si dispo
    if cn_from_path:
        res = cn_from_path(path)
        if res:
            best = res.best()
            if best:
                return best.encoding or "utf-8"
    # 3) fallback latin-1 (ne lÃ¨ve jamais)
    return "latin-1"

def convert_file(path: Path) -> bool:
    enc = detect_encoding(path)
    try:
        raw = path.read_text(encoding=enc, errors="replace")
    except Exception as e:
        print(f"!! Read failed {path}: {e}")
        return False
    # normaliser fins de ligne
    normalized = raw.replace("\r\n", "\n").replace("\r", "\n")
    # Ã©crire en utf-8 sans BOM
    try:
        path.write_text(normalized, encoding="utf-8", newline="\n")
        print(f"âœ“ Converted to UTF-8 no BOM (from {enc}): {path}")
        return True
    except Exception as e:
        print(f"!! Write failed {path}: {e}")
        return False

def main(files: Iterable[str]):
    root = Path(".").resolve()
    ok = True
    for f in files:
        p = (root / f).resolve()
        if p.exists() and p.is_file():
            ok &= convert_file(p)
        else:
            print(f"-- skip (not found): {p}")
    sys.exit(0 if ok else 1)

if __name__ == "__main__":
    # si aucun argument: utiliser DEFAULT_FILES
    args = sys.argv[1:]
    files = args if args else DEFAULT_FILES
    main(files)

# ===== scripts/run_all_tests.py =====
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script runner pour tous les tests GuignoMap
ExÃ©cute les tests unitaires et de validation du projet
"""
import os
import sys
import subprocess
from pathlib import Path

def run_command(cmd, cwd=None):
    """ExÃ©cute une commande et retourne le code de sortie"""
    print(f"ðŸ”„ ExÃ©cution: {cmd}")
    if cwd:
        print(f"   ðŸ“ Dans: {cwd}")
    
    try:
        result = subprocess.run(
            cmd,
            shell=True,
            cwd=cwd,
            capture_output=False,
            text=True
        )
        return result.returncode
    except Exception as e:
        print(f"âŒ Erreur lors de l'exÃ©cution: {e}")
        return 1

def main():
    """Point d'entrÃ©e principal"""
    print("ðŸ§ª GuignoMap - Runner de tests")
    print("=" * 50)
    
    # Chemins
    project_root = Path(__file__).parent.parent
    tests_dir = project_root / "tests"
    
    exit_code = 0
    
    # 1. Tests d'authentification
    print("\nðŸ“ Tests d'authentification")
    auth_test = tests_dir / "auth" / "test_passwords_smoke.py"
    if auth_test.exists():
        code = run_command(f"python {auth_test}", cwd=project_root)
        if code != 0:
            exit_code = code
    else:
        print("âš ï¸ Aucun test d'auth trouvÃ©")
    
    # 2. Validation de structure
    print("\nðŸ“ Validation de structure")
    validate_script = project_root / "scripts" / "validate_structure.py"
    if validate_script.exists():
        code = run_command(f"python scripts/validate_structure.py", cwd=project_root)
        if code != 0:
            print("âš ï¸ Validation de structure Ã©chouÃ©e (peut contenir des warnings)")
    else:
        print("âš ï¸ Script de validation non trouvÃ©")
    
    # 3. Sanity check rapide
    print("\nðŸ“ Sanity check (BD)")
    sanity_script = project_root / "tools" / "quick_sanity.py"
    if sanity_script.exists():
        code = run_command(f"python tools/quick_sanity.py", cwd=project_root)
        if code != 0:
            print("âš ï¸ Sanity check Ã©chouÃ©")
    else:
        print("âš ï¸ Script sanity non trouvÃ©")
    
    # RÃ©sumÃ©
    print("\n" + "=" * 50)
    if exit_code == 0:
        print("âœ… Tous les tests terminÃ©s (vÃ©rifiez les dÃ©tails ci-dessus)")
    else:
        print(f"âŒ Des tests ont Ã©chouÃ© (code: {exit_code})")
    
    print("ðŸ Runner terminÃ©")
    return exit_code

if __name__ == "__main__":
    sys.exit(main())

# ===== scripts/sample_addresses.py =====
import sqlite3

c = sqlite3.connect('guignomap/guigno_map.db')
print('Ã‰chantillon alÃ©atoire d\'adresses:')
for row in c.execute("SELECT street_name, house_number, postal_code, sector, latitude, longitude FROM addresses ORDER BY RANDOM() LIMIT 5").fetchall():
    geo = f"({row[4]}, {row[5]})" if row[4] and row[5] else "Non gÃ©ocodÃ©"
    print(f"  {row[0]} {row[1]} | Postal: {row[2] or 'N/A'} | Secteur: {row[3] or 'N/A'} | Geo: {geo}")
c.close()

# ===== scripts/seed_address_demo.py =====
# scripts/seed_address_demo.py
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire parent au path pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

import guignomap.database as db

def main():
    print("=== Demo d'assignation d'adresses ===")
    
    # Initialiser la DB
    db.init_db()
    
    # 1. Trouver une Ã©quipe cible (premiÃ¨re non-ADMIN ou 'Equipe 1')
    try:
        teams = db.get_teams_list()  # [(id, name), ...]
        if not teams:
            print("âŒ Aucune Ã©quipe trouvÃ©e. CrÃ©ez d'abord des Ã©quipes.")
            return
        
        # Chercher une Ã©quipe non-ADMIN
        target_team = None
        for team_id, team_name in teams:
            if team_id != "ADMIN":
                target_team = (team_id, team_name)
                break
        
        # Fallback : chercher 'Equipe 1'
        if not target_team:
            for team_id, team_name in teams:
                if team_id == "Equipe 1" or team_name == "Equipe 1":
                    target_team = (team_id, team_name)
                    break
        
        if not target_team:
            # Prendre la premiÃ¨re Ã©quipe disponible
            target_team = teams[0]
        
        team_id, team_name = target_team
        print(f"ðŸŽ¯ Ã‰quipe cible : {team_name} (ID: {team_id})")
        
    except Exception as e:
        print(f"âŒ Erreur lors de la rÃ©cupÃ©ration des Ã©quipes : {e}")
        return
    
    # 2. RÃ©cupÃ©rer 5 adresses non assignÃ©es
    try:
        unassigned_df = db.get_unassigned_addresses(limit=5)
        
        if unassigned_df.empty:
            print("âœ… Aucune adresse non assignÃ©e trouvÃ©e (toutes sont dÃ©jÃ  assignÃ©es)")
            return
        
        print(f"ðŸ“ {len(unassigned_df)} adresse(s) non assignÃ©e(s) trouvÃ©e(s) :")
        for _, row in unassigned_df.iterrows():
            sector_info = f" (secteur: {row['sector']})" if row['sector'] else ""
            print(f"  - ID:{row['id']} | {row['street_name']} {row['house_number']}{sector_info}")
        
        # Extraire les IDs
        address_ids = unassigned_df['id'].tolist()
        
    except Exception as e:
        print(f"âŒ Erreur lors de la rÃ©cupÃ©ration des adresses : {e}")
        return
    
    # 3. Assigner les adresses Ã  l'Ã©quipe
    try:
        assigned_count = db.assign_addresses_to_team(address_ids, team_id)
        print(f"âœ… {assigned_count} adresse(s) assignÃ©e(s) Ã  l'Ã©quipe '{team_name}' (ID: {team_id})")
        
        # Invalider les caches
        try:
            db.invalidate_caches()
            print("ðŸ”„ Caches invalidÃ©s")
        except:
            pass
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'assignation : {e}")
        return
    
    # 4. RÃ©sumÃ© final
    print("\n=== RÃ©sumÃ© ===")
    try:
        # Compter les adresses assignÃ©es Ã  cette Ã©quipe
        team_addresses = db.get_team_addresses(team_id, limit=1000)
        total_assigned = len(team_addresses)
        print(f"ðŸ“Š L'Ã©quipe '{team_name}' a maintenant {total_assigned} adresse(s) assignÃ©e(s)")
        
        # Compter les adresses encore non assignÃ©es
        remaining_unassigned = db.get_unassigned_addresses(limit=1000)
        remaining_count = len(remaining_unassigned)
        print(f"ðŸ“Š Il reste {remaining_count} adresse(s) non assignÃ©e(s) au total")
        
    except Exception as e:
        print(f"âš ï¸ Impossible de gÃ©nÃ©rer le rÃ©sumÃ© complet : {e}")
    
    print("âœ… Demo terminÃ©e avec succÃ¨s !")

if __name__ == "__main__":
    main()

# ===== scripts/setup_nouvelle_ville.py =====
#!/usr/bin/env python3
"""
Script de setup pour une nouvelle ville
Usage: python scripts/setup_nouvelle_ville.py "Nom De Ville"

CrÃ©e un clone de la base de donnÃ©es SQLite pour une nouvelle ville.
"""

import sys
import re
from pathlib import Path
import shutil


def slugify(text):
    """Convertit un nom de ville en slug utilisable comme nom de fichier"""
    # Convertir en minuscules
    text = text.lower()
    # Remplacer espaces et caractÃ¨res spÃ©ciaux par des underscores
    text = re.sub(r'[^a-z0-9]+', '_', text)
    # Supprimer les underscores en dÃ©but/fin
    text = text.strip('_')
    return text


def main():
    if len(sys.argv) != 2:
        print("Usage: python scripts/setup_nouvelle_ville.py \"Nom De Ville\"")
        sys.exit(1)
    
    ville_nom = sys.argv[1].strip()
    if not ville_nom:
        print("Erreur: Le nom de ville ne peut pas Ãªtre vide")
        sys.exit(1)
    
    # CrÃ©er le slug
    slug = slugify(ville_nom)
    if not slug:
        print(f"Erreur: Impossible de crÃ©er un slug valide pour '{ville_nom}'")
        sys.exit(1)
    
    # Chemins
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    source_db = project_root / "guignomap" / "guigno_map.db"
    data_dir = project_root / "data"
    target_db = data_dir / f"{slug}.db"
    
    # VÃ©rifier que la DB source existe
    if not source_db.exists():
        print(f"Erreur: Base de donnÃ©es source introuvable: {source_db}")
        sys.exit(1)
    
    # CrÃ©er le dossier data/ s'il n'existe pas
    data_dir.mkdir(exist_ok=True)
    
    # VÃ©rifier si le fichier cible existe dÃ©jÃ 
    if target_db.exists():
        print(f"Attention: Le fichier {target_db} existe dÃ©jÃ  et sera Ã©crasÃ©")
    
    try:
        # Copier la base de donnÃ©es
        shutil.copy2(source_db, target_db)
        print(f"âœ… Base de donnÃ©es crÃ©Ã©e pour '{ville_nom}'")
        print(f"   Slug: {slug}")
        print(f"   Fichier: {target_db.absolute()}")
        
    except Exception as e:
        print(f"Erreur lors de la copie: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

# ===== scripts/show_hash_stats.py =====
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script administrateur pour afficher les statistiques des algorithmes de hash
Utilise count_hash_algorithms() de src.database.operations

Usage:
    python scripts/show_hash_stats.py
"""

import sys
import os

# Ajouter le rÃ©pertoire parent au path pour importer src/
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

def main():
    """Affiche les statistiques des algorithmes de hash"""
    try:
        from guignomap.database import count_hash_algorithms
        
        print("ðŸ” Statistiques des algorithmes de hash des Ã©quipes")
        print("=" * 55)
        
        stats = count_hash_algorithms()
        
        if not stats:
            print("âŒ Aucune Ã©quipe avec mot de passe trouvÃ©e")
            return 1
        
        total = sum(v for k, v in stats.items() if k != "empty")
        total_with_empty = total + stats.get("empty", 0)
        
        print(f"ðŸ“Š Total Ã©quipes actives: {total_with_empty}")
        print(f"ðŸ”‘ Ã‰quipes avec mot de passe: {total}")
        print(f"ðŸš« Ã‰quipes sans mot de passe: {stats.get('empty', 0)}")
        print()
        
        if total > 0:
            print("ðŸ“ˆ RÃ©partition par algorithme:")
            print("-" * 30)
            
            for algo, count in sorted(stats.items()):
                if algo == "empty":
                    continue
                    
                if count > 0:
                    percent = (count / total) * 100
                    icon = "âœ…" if algo == "argon2" else "âš ï¸" if algo == "bcrypt" else "âŒ"
                    print(f"{icon} {algo:12} : {count:3d} ({percent:5.1f}%)")
            
            print()
            
            # Recommandations
            bcrypt_count = stats.get("bcrypt", 0)
            other_count = sum(v for k, v in stats.items() if k not in ["argon2", "bcrypt", "empty"])
            
            if bcrypt_count > 0:
                print(f"ðŸ”„ {bcrypt_count} Ã©quipes utilisent encore bcrypt (migration auto au login)")
            if other_count > 0:
                print(f"âš ï¸  {other_count} Ã©quipes utilisent des algorithmes legacy (nÃ©cessitent intervention)")
            if stats.get("argon2", 0) == total:
                print("ðŸŽ‰ Migration complÃ¨te vers Argon2 !")
        
        return 0
        
    except ImportError as e:
        print(f"âŒ Erreur d'import: {e}")
        print("Assurez-vous d'Ãªtre dans le bon rÃ©pertoire et que l'environnement Python est activÃ©")
        return 1
    except Exception as e:
        print(f"âŒ Erreur inattendue: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

# ===== scripts/smoke_create_map.py =====
import sys, pathlib, importlib
import pandas as pd
from importlib.util import spec_from_file_location, module_from_spec

ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

mod = None
# Tentatives d'import par paquet (aprÃ¨s ajout __init__.py)
for candidate in ("guignomap.app", "src.app"):
    try:
        mod = importlib.import_module(candidate)
        break
    except Exception:
        continue

# Fallback: import direct par chemin
app_path = ROOT / "guignomap" / "app.py"
if mod is None and app_path.exists():
    spec = spec_from_file_location("app_fallback", str(app_path))
    if spec is not None and spec.loader is not None:
        m = module_from_spec(spec)
        spec.loader.exec_module(m)
        mod = m

if mod is None or not hasattr(mod, "create_map"):
    raise SystemExit("[ERREUR] create_map introuvable (guignomap.app/src.app ou fallback fichier)")

# DataFrame minimal conforme Ã  create_map(df, geo)
df = pd.DataFrame([
    {"name": "Rue Test A", "status": "a_faire", "team": "", "notes": "0"},
    {"name": "Rue Test B", "status": "terminee", "team": "EQUIPE1", "notes": "2"},
])

try:
    mod.create_map(df, {})  # geo vide: on valide la boucle DataFrame
    print("create_map(df) OK")
except Exception as e:
    print("create_map(df) FAILED:", e)
    sys.exit(1)

# ===== scripts/test_helpers.py =====
from guignomap import database as db
print("helpers:", all(hasattr(db,n) for n in ["assign_addresses_to_team","get_unassigned_addresses","get_team_addresses"]))
try:
    ua = db.get_unassigned_addresses()
    print("Unassigned sample (top 3):", ua.head(3).to_dict(orient="records"))
except Exception as e:
    print("No unassigned yet or table missing:", e)

# ===== scripts/validate_production_ready.py =====
#!/usr/bin/env python3
"""
Script de validation de la prÃªte pour la production
Tests d'encodage, charge simulÃ©e et robustesse de la base de donnÃ©es.

Usage: python scripts/validate_production_ready.py
"""

import sys
import time
from pathlib import Path

# Ajouter le rÃ©pertoire racine au path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from guignomap.database import get_conn


def test_encoding_and_special_characters():
    """Test l'encodage et les caractÃ¨res spÃ©ciaux"""
    print("ðŸ”¤ Test encodage et caractÃ¨res spÃ©ciaux...")
    
    conn = get_conn()
    try:
        # Commencer une transaction
        conn.execute("BEGIN TRANSACTION")
        
        # CrÃ©er une table temporaire pour le test d'encodage
        conn.execute("""
            CREATE TEMP TABLE test_encoding (
                id INTEGER PRIMARY KEY,
                street_name TEXT,
                comment TEXT
            )
        """)
        
        # InsÃ©rer temporairement un enregistrement avec accents
        test_data = {
            "street_name": "Ã‰rable",
            "comment": "FranÃ§ais: Ã©Ã¨Ã  Ã‰ÃˆÃ€ Ã§ Ã± Ã¼ ÃŸ ä¸­æ–‡ ðŸŽ„"
        }
        
        conn.execute("""
            INSERT INTO test_encoding (street_name, comment)
            VALUES (?, ?)
        """, (test_data["street_name"], test_data["comment"]))
        
        # VÃ©rifier round-trip
        cursor = conn.execute("""
            SELECT street_name, comment 
            FROM test_encoding 
            WHERE street_name = ?
        """, (test_data["street_name"],))
        
        row = cursor.fetchone()
        if not row:
            raise ValueError("Enregistrement de test non trouvÃ©")
        
        retrieved_street, retrieved_comment = row
        
        # VÃ©rifier l'intÃ©gritÃ© des donnÃ©es
        if retrieved_street != test_data["street_name"]:
            raise ValueError(f"Street name corrompu: {retrieved_street} != {test_data['street_name']}")
        if retrieved_comment != test_data["comment"]:
            raise ValueError(f"Comment corrompu: {retrieved_comment} != {test_data['comment']}")
        
        print("   âœ… Encodage UTF-8 et caractÃ¨res spÃ©ciaux: OK")
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur encodage: {e}")
        return False
    finally:
        # ROLLBACK pour ne rien laisser
        conn.execute("ROLLBACK")


def test_simulated_load():
    """Test de charge simulÃ©e avec 1000+ rues"""
    print("âš¡ Test charge simulÃ©e (1000+ rues)...")
    
    conn = get_conn()
    try:
        # CrÃ©er une table temporaire
        conn.execute("""
            CREATE TEMP TABLE temp_streets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                sector TEXT,
                team TEXT,
                status TEXT DEFAULT 'a_faire'
            )
        """)
        
        # Copier les rues existantes avec suffixe unique
        start_time = time.time()
        conn.execute("""
            INSERT INTO temp_streets (name, sector, team, status)
            SELECT name || ' [SIM]', sector, team, status
            FROM streets
        """)
        
        # Ajouter des rues supplÃ©mentaires pour atteindre 1000+
        cursor = conn.execute("SELECT COUNT(*) FROM temp_streets")
        current_count = cursor.fetchone()[0]
        
        if current_count < 1000:
            needed = 1000 - current_count
            print(f"   ðŸ“Š Ajout de {needed} rues supplÃ©mentaires...")
            
            for i in range(needed):
                conn.execute("""
                    INSERT INTO temp_streets (name, sector, team, status)
                    VALUES (?, ?, ?, ?)
                """, (f"Rue Test {i+1:04d} [SIM]", f"Secteur {(i % 10) + 1}", 
                      "ADMIN" if i % 5 == 0 else "", "a_faire"))
        
        load_time = time.time() - start_time
        
        # VÃ©rifier le count total
        cursor = conn.execute("SELECT COUNT(*) FROM temp_streets")
        total_count = cursor.fetchone()[0]
        
        # Tests de performance sur requÃªtes filtrÃ©es
        start_time = time.time()
        
        # Test 1: Filter par status
        cursor = conn.execute("SELECT COUNT(*) FROM temp_streets WHERE status = 'a_faire'")
        status_count = cursor.fetchone()[0]
        
        # Test 2: Filter par team
        cursor = conn.execute("SELECT COUNT(*) FROM temp_streets WHERE team = 'ADMIN'")
        team_count = cursor.fetchone()[0]
        
        # Test 3: Filter par sector avec LIKE
        cursor = conn.execute("SELECT COUNT(*) FROM temp_streets WHERE sector LIKE 'Secteur%'")
        sector_count = cursor.fetchone()[0]
        
        # Test 4: RequÃªte complexe avec ORDER BY
        cursor = conn.execute("""
            SELECT status, COUNT(*) as count 
            FROM temp_streets 
            GROUP BY status 
            ORDER BY count DESC
        """)
        status_stats = cursor.fetchall()
        
        query_time = time.time() - start_time
        
        print(f"   ðŸ“Š Rues chargÃ©es: {total_count}")
        print(f"   â±ï¸  Temps de chargement: {load_time:.3f}s")
        print(f"   â±ï¸  Temps requÃªtes: {query_time:.3f}s")
        print(f"   ðŸ” Filtres - Status 'a_faire': {status_count}, Team 'ADMIN': {team_count}, Secteurs: {sector_count}")
        
        # Validation des performances
        if load_time > 5.0:
            print(f"   âš ï¸  Chargement lent: {load_time:.3f}s > 5s")
        if query_time > 2.0:
            print(f"   âš ï¸  RequÃªtes lentes: {query_time:.3f}s > 2s")
        
        if total_count >= 1000 and load_time <= 10.0 and query_time <= 5.0:
            print("   âœ… Test de charge: OK")
            return True
        else:
            print("   âŒ Test de charge: Ã‰CHEC")
            return False
            
    except Exception as e:
        print(f"   âŒ Erreur charge simulÃ©e: {e}")
        return False
    finally:
        # Nettoyer la table temporaire (elle sera automatiquement supprimÃ©e)
        try:
            conn.execute("DROP TABLE IF EXISTS temp_streets")
        except:
            pass  # Table temporaire dÃ©jÃ  supprimÃ©e


def test_database_integrity():
    """Test l'intÃ©gritÃ© de la base de donnÃ©es"""
    print("ðŸ”§ Test intÃ©gritÃ© base de donnÃ©es...")
    
    conn = get_conn()
    try:
        # VÃ©rifier les tables principales
        cursor = conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name IN ('streets', 'teams', 'notes')
        """)
        tables = [row[0] for row in cursor.fetchall()]
        
        required_tables = ['streets', 'teams', 'notes']
        missing_tables = [t for t in required_tables if t not in tables]
        
        if missing_tables:
            print(f"   âŒ Tables manquantes: {missing_tables}")
            return False
        
        # VÃ©rifier les contraintes
        cursor = conn.execute("SELECT COUNT(*) FROM streets")
        streets_count = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(*) FROM teams")
        teams_count = cursor.fetchone()[0]
        
        if streets_count == 0:
            print("   âŒ Aucune rue dans la base")
            return False
        
        if teams_count == 0:
            print("   âŒ Aucune Ã©quipe dans la base")
            return False
        
        print(f"   ðŸ“Š {streets_count} rues, {teams_count} Ã©quipes")
        print("   âœ… IntÃ©gritÃ© base de donnÃ©es: OK")
        return True
        
    except Exception as e:
        print(f"   âŒ Erreur intÃ©gritÃ©: {e}")
        return False


def main():
    """Fonction principale de validation"""
    print("ðŸš€ Validation Production Ready - GuignoMap")
    print("=" * 50)
    
    tests_results = []
    
    # Test 1: Encodage
    tests_results.append(test_encoding_and_special_characters())
    
    # Test 2: Charge simulÃ©e
    tests_results.append(test_simulated_load())
    
    # Test 3: IntÃ©gritÃ©
    tests_results.append(test_database_integrity())
    
    # RÃ©sumÃ© final
    print("\n" + "=" * 50)
    passed_tests = sum(tests_results)
    total_tests = len(tests_results)
    
    print(f"ðŸ“‹ RÃ©sultats: {passed_tests}/{total_tests} tests rÃ©ussis")
    
    if all(tests_results):
        print("ðŸŽ‰ READY: PASS - Application prÃªte pour la production!")
        sys.exit(0)
    else:
        print("âŒ READY: FAIL - ProblÃ¨mes dÃ©tectÃ©s, correction requise")
        sys.exit(1)


if __name__ == "__main__":
    main()

# ===== scripts/validate_structure.py =====
#!/usr/bin/env python3
"""
GuignoMap Structure Validation Script
=====================================

Validates the project structure for:
- No circular imports
- UTF-8 encoding (strict)
- No hardcoded paths
- Single database layer

Usage:
    python scripts/validate_structure.py
"""

import os
import sys
import ast
import chardet
import re
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
import importlib.util


class StructureValidator:
    """Validates GuignoMap project structure and coding standards."""
    
    def __init__(self, project_root: Optional[str] = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.errors = []
        self.warnings = []
        
    def log_error(self, message: str):
        """Log a validation error."""
        self.errors.append(f"âŒ ERROR: {message}")
        
    def log_warning(self, message: str):
        """Log a validation warning."""
        self.warnings.append(f"âš ï¸  WARNING: {message}")
        
    def log_info(self, message: str):
        """Log an info message."""
        print(f"â„¹ï¸  {message}")

    def get_python_files(self) -> List[Path]:
        """Get all Python files in the project, excluding certain directories."""
        exclude_dirs = {'.venv', '__pycache__', '.git', 'node_modules', 'backups'}
        python_files = []
        
        for root, dirs, files in os.walk(self.project_root):
            # Remove excluded directories from traversal
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if file.endswith('.py'):
                    python_files.append(Path(root) / file)
                    
        return python_files

    def no_circular_imports(self) -> bool:
        """Check for circular imports in Python files."""
        self.log_info("Checking for circular imports...")
        
        python_files = self.get_python_files()
        imports_graph = {}
        
        # Build import graph
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    tree = ast.parse(f.read(), filename=str(py_file))
                
                module_path = str(py_file.relative_to(self.project_root)).replace('\\', '/').replace('.py', '').replace('/', '.')
                imports_graph[module_path] = set()
                
                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            imports_graph[module_path].add(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            imports_graph[module_path].add(node.module)
                            
            except Exception as e:
                self.log_warning(f"Could not parse {py_file}: {e}")
                continue
        
        # Check for cycles using DFS
        def has_cycle(node, visited, rec_stack):
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in imports_graph.get(node, set()):
                if neighbor in imports_graph:  # Only check internal modules
                    if neighbor not in visited:
                        if has_cycle(neighbor, visited, rec_stack):
                            return True
                    elif neighbor in rec_stack:
                        self.log_error(f"Circular import detected: {node} -> {neighbor}")
                        return True
            
            rec_stack.remove(node)
            return False
        
        visited = set()
        has_cycles = False
        
        for module in imports_graph:
            if module not in visited:
                if has_cycle(module, visited, set()):
                    has_cycles = True
        
        if not has_cycles:
            self.log_info("âœ… No circular imports found")
            
        return not has_cycles

    def utf8_encoding(self) -> bool:
        """Check that all Python, Markdown, and TOML files are UTF-8 encoded."""
        self.log_info("Checking UTF-8 encoding...")
        
        # Get all target files (Python, Markdown, TOML)
        target_files = []
        exclude_dirs = {'.venv', '__pycache__', '.git', 'node_modules', 'backups'}
        
        for root, dirs, files in os.walk(self.project_root):
            # Remove excluded directories from traversal
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            
            for file in files:
                if file.endswith(('.py', '.md', '.toml')):
                    target_files.append(Path(root) / file)
        
        all_utf8 = True
        
        for file_path in target_files:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                
                # Handle empty files
                if len(raw_data) == 0:
                    continue  # Empty files are OK
                    
                result = chardet.detect(raw_data)
                encoding = result.get('encoding', '') if result else ''
                encoding = encoding.lower() if encoding else 'unknown'
                
                # Be strict: only accept pure UTF-8 (no BOM, no other encodings)
                if encoding != 'utf-8' and encoding != 'ascii':
                    self.log_error(f"File {file_path} is not UTF-8 encoded (detected: {encoding})")
                    all_utf8 = False
                    
                # Check for BOM
                if raw_data.startswith(b'\xef\xbb\xbf'):
                    self.log_error(f"File {file_path} has UTF-8 BOM (should be UTF-8 without BOM)")
                    all_utf8 = False
                    
            except Exception as e:
                self.log_warning(f"Could not check encoding of {file_path}: {e}")
                
        if all_utf8:
            self.log_info("âœ… All target files are UTF-8 encoded without BOM")
            
        return all_utf8

    def no_hardcoded_paths(self) -> bool:
        """Check for hardcoded paths in Python files."""
        self.log_info("Checking for hardcoded paths...")
        
        python_files = self.get_python_files()
        hardcoded_patterns = [
            r'["\']C:\\',  # Windows absolute paths
            r'["\'][A-Za-z]:\\',  # Drive letters
            r'["\']\/home\/',  # Linux home paths
            r'["\']\/usr\/',   # Linux system paths
            r'["\']\/var\/',   # Linux var paths
        ]
        
        # Compile patterns
        compiled_patterns = [re.compile(pattern) for pattern in hardcoded_patterns]
        has_hardcoded = False
        
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                for i, line in enumerate(content.split('\n'), 1):
                    # Skip comments
                    if line.strip().startswith('#'):
                        continue
                        
                    for pattern in compiled_patterns:
                        if pattern.search(line):
                            self.log_error(f"Hardcoded path in {py_file}:{i}: {line.strip()}")
                            has_hardcoded = True
                            
            except Exception as e:
                self.log_warning(f"Could not check {py_file}: {e}")
                
        if not has_hardcoded:
            self.log_info("âœ… No hardcoded paths found")
            
        return not has_hardcoded

    def single_db_layer(self) -> bool:
        """Check that database layer is unified (no db_v5 imports, operations.py exists)."""
        self.log_info("Checking single database layer...")
        
        # Check that operations.py exists
        operations_file = self.project_root / 'src' / 'database' / 'operations.py'
        if not operations_file.exists():
            self.log_error("src/database/operations.py does not exist")
            return False
            
        # Check for any remaining db_v5 imports in active code
        python_files = self.get_python_files()
        db_v5_imports = False
        
        for py_file in python_files:
            # Skip the compatibility shim and this validation file
            if py_file.name == 'db_v5.py' or py_file.name == 'validate_structure.py':
                continue
                
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # Check for actual import statements (not comments)
                for i, line in enumerate(content.split('\n'), 1):
                    line_stripped = line.strip()
                    # Skip comments and docstrings
                    if line_stripped.startswith('#') or line_stripped.startswith('"""') or line_stripped.startswith("'''"):
                        continue
                        
                    if 'db_v5' in line and ('import' in line or 'from' in line):
                        if re.search(r'(^|[^#]*)import.*db_v5|from.*db_v5.*import', line):
                            self.log_error(f"Found db_v5 import in {py_file}:{i}: {line.strip()}")
                            db_v5_imports = True
                            
            except Exception as e:
                self.log_warning(f"Could not check {py_file}: {e}")
        
        # Check that main imports use operations
        app_py = self.project_root / 'guignomap' / 'app.py'
        if app_py.exists():
            try:
                with open(app_py, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if 'from src.database import operations' not in content:
                        self.log_warning("app.py should import from src.database.operations")
            except Exception as e:
                self.log_warning(f"Could not check app.py: {e}")
        
        if not db_v5_imports:
            self.log_info("âœ… Single database layer confirmed (no db_v5 imports found)")
            
        return not db_v5_imports

    def validate_project(self) -> bool:
        """Run all validations and return overall result."""
        print("ðŸ” GuignoMap Structure Validation")
        print("=" * 50)
        
        results = []
        
        # Run all validation checks
        results.append(self.no_circular_imports())
        results.append(self.utf8_encoding())
        results.append(self.no_hardcoded_paths())
        results.append(self.single_db_layer())
        
        print("\n" + "=" * 50)
        print("ðŸ“‹ VALIDATION REPORT")
        print("=" * 50)
        
        # Print warnings
        if self.warnings:
            print("\nâš ï¸  WARNINGS:")
            for warning in self.warnings:
                print(f"   {warning}")
        
        # Print errors
        if self.errors:
            print("\nâŒ ERRORS:")
            for error in self.errors:
                print(f"   {error}")
        
        # Overall result
        all_passed = all(results) and not self.errors
        
        print(f"\nðŸŽ¯ OVERALL RESULT: {'âœ… PASSED' if all_passed else 'âŒ FAILED'}")
        
        if all_passed:
            print("   All validations passed! Project structure is clean.")
        else:
            print(f"   {len(self.errors)} error(s) found. Please fix before proceeding.")
            
        return all_passed


def main():
    """Main entry point."""
    validator = StructureValidator()
    success = validator.validate_project()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

# ===== scripts/verify_addresses.py =====
#!/usr/bin/env python3
"""
GuignoMap - VÃ©rification robuste des adresses importÃ©es
Compare Excel source vs DB et dÃ©tecte les anomalies
"""
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime
import pandas as pd

# Ajouter le rÃ©pertoire parent au path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from guignomap.imports import detect_schema, prepare_dataframe
from guignomap.database import get_conn


def setup_logging(verbose: bool = False):
    """Configure le logging"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )


def verify_addresses_robust(city: str, excel_file: Path, verbose: bool = False) -> bool:
    """
    VÃ©rification robuste avec comparaison Excel vs DB
    
    Returns:
        bool: True si vÃ©rification OK, False si anomalies dÃ©tectÃ©es
    """
    setup_logging(verbose)
    
    try:
        # 1. Rechargement et prÃ©paration Excel
        logging.info(f"ðŸ”„ Rechargement du fichier Excel: {excel_file}")
        if not excel_file.exists():
            logging.error(f"Fichier Excel introuvable: {excel_file}")
            return False
        
        df_excel = pd.read_excel(excel_file)
        logging.info(f"ðŸ“Š Excel: {len(df_excel)} lignes, {len(df_excel.columns)} colonnes")
        
        # 2. PrÃ©paration des donnÃ©es Excel (mÃªme logique que l'import)
        mapping = detect_schema(df_excel, city)
        if 'street' not in mapping or 'number' not in mapping:
            logging.error("Impossible de dÃ©tecter les colonnes rue/numÃ©ro")
            return False
        
        df_prepared = prepare_dataframe(df_excel, mapping, city)
        logging.info(f"âœ… Excel prÃ©parÃ©: {len(df_prepared)} adresses uniques")
        
        # 3. Lecture de la base de donnÃ©es
        logging.info("ðŸ” Analyse de la base de donnÃ©es...")
        with get_conn() as conn:
            cursor = conn.cursor()
            
            # Statistiques DB
            cursor.execute("SELECT COUNT(*) FROM addresses")
            db_total = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT addr_key) FROM addresses")
            db_unique_keys = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM addresses WHERE assigned_to IS NOT NULL AND assigned_to != ''")
            db_assigned = cursor.fetchone()[0]
            
            logging.info(f"ðŸ’¾ DB: {db_total} adresses, {db_unique_keys} clÃ©s uniques, {db_assigned} assignÃ©es")
        
        # 4. Comparaisons et analyses
        anomalies = []
        verification_ok = True
        
        # Test 1: Comparaison des totaux
        excel_unique = len(df_prepared)
        doublons_excel = len(df_excel) - excel_unique
        delta_total = abs(db_total - excel_unique)
        
        logging.info("ðŸ“ˆ ANALYSE COMPARATIVE")
        logging.info(f"  â€¢ Excel original: {len(df_excel):,}")
        logging.info(f"  â€¢ Excel unique: {excel_unique:,}")
        logging.info(f"  â€¢ Doublons Excel supprimÃ©s: {doublons_excel:,}")
        logging.info(f"  â€¢ DB total: {db_total:,}")
        logging.info(f"  â€¢ Delta (DB vs Excel unique): {delta_total:,}")
        
        if delta_total > 0:
            verification_ok = False
            anomalies.append({
                'type': 'DELTA_TOTAL',
                'description': f'DB ({db_total}) != Excel unique ({excel_unique})',
                'valeur_attendue': excel_unique,
                'valeur_reelle': db_total,
                'ecart': delta_total
            })
        
        # Test 2: VÃ©rification unicitÃ© clÃ©s
        if db_unique_keys != db_total:
            verification_ok = False
            doublons_db = db_total - db_unique_keys
            anomalies.append({
                'type': 'DOUBLONS_DB',
                'description': f'ClÃ©s non uniques en DB: {doublons_db} doublons',
                'valeur_attendue': db_total,
                'valeur_reelle': db_unique_keys,
                'ecart': doublons_db
            })
            logging.warning(f"âš ï¸ Doublons en DB: {doublons_db}")
        
        # Test 3: VÃ©rification cohÃ©rence des clÃ©s (Ã©chantillon)
        if db_total > 0:
            with get_conn() as conn:
                sample_db = pd.read_sql_query("""
                    SELECT street_name, house_number, postal_code, addr_key 
                    FROM addresses 
                    LIMIT 100
                """, conn)
                
                # Recalculer les clÃ©s et comparer
                from guignomap.imports import build_addr_key
                sample_db['addr_key_recalc'] = sample_db.apply(
                    lambda row: build_addr_key(row['street_name'], row['house_number'], row['postal_code']),
                    axis=1
                )
                
                key_mismatches = sample_db[sample_db['addr_key'] != sample_db['addr_key_recalc']]
                if len(key_mismatches) > 0:
                    verification_ok = False
                    anomalies.append({
                        'type': 'CLES_INCOHERENTES',
                        'description': f'ClÃ©s addr_key incohÃ©rentes: {len(key_mismatches)} sur 100 Ã©chantillonnÃ©es',
                        'valeur_attendue': 0,
                        'valeur_reelle': len(key_mismatches),
                        'ecart': len(key_mismatches)
                    })
                    logging.warning(f"âš ï¸ ClÃ©s incohÃ©rentes dÃ©tectÃ©es: {len(key_mismatches)}")
        
        # 5. Export des anomalies si nÃ©cessaire
        if anomalies:
            exports_dir = Path("exports/maintenance")
            exports_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            anomalies_file = exports_dir / f"verify_anomalies_{city}_{timestamp}.csv"
            
            df_anomalies = pd.DataFrame(anomalies)
            df_anomalies.to_csv(anomalies_file, index=False, encoding='utf-8')
            
            logging.warning(f"ðŸ“ Anomalies exportÃ©es: {anomalies_file}")
        
        # 6. Rapport final
        if verification_ok:
            logging.info("âœ… VÃ‰RIFICATION RÃ‰USSIE - Aucune anomalie dÃ©tectÃ©e")
            logging.info(f"  â€¢ CohÃ©rence Excel â†” DB: âœ…")
            logging.info(f"  â€¢ UnicitÃ© clÃ©s DB: âœ…") 
            logging.info(f"  â€¢ IntÃ©gritÃ© addr_key: âœ…")
        else:
            logging.error("âŒ ANOMALIES DÃ‰TECTÃ‰ES")
            for anomalie in anomalies:
                logging.error(f"  â€¢ {anomalie['type']}: {anomalie['description']}")
        
        return verification_ok
        
    except Exception as e:
        logging.error(f"âŒ Erreur lors de la vÃ©rification: {e}")
        if verbose:
            logging.exception("DÃ©tails de l'erreur:")
        return False


def main():
    """Point d'entrÃ©e principal"""
    parser = argparse.ArgumentParser(
        description="VÃ©rification robuste des adresses GuignoMap",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  %(prog)s --city mascouche --file imports/mascouche_adresses.xlsx
  %(prog)s --city montreal --file data/mtl.xlsx --verbose
        """
    )
    
    parser.add_argument(
        '--city',
        required=True,
        help='Nom de la ville'
    )
    
    parser.add_argument(
        '--file',
        required=True,
        type=Path,
        help='Chemin vers le fichier Excel source'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Affichage dÃ©taillÃ© (debug)'
    )
    
    args = parser.parse_args()
    
    # ExÃ©cution de la vÃ©rification
    success = verify_addresses_robust(args.city, args.file, args.verbose)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())

# ===== scripts/verify_addresses_exact.py =====
# scripts/verify_addresses_exact.py
import pandas as pd
import sqlite3
import sys
from pathlib import Path

DB_PATH = Path("guignomap/guigno_map.db")
EXCEL_PATH = Path("imports/mascouche_adresses.xlsx")

def count_valid_excel_lines():
    """Compte les lignes valides dans le fichier Excel"""
    if not EXCEL_PATH.exists():
        print(f"âŒ Fichier Excel introuvable: {EXCEL_PATH}")
        return 0
    
    print(f"ðŸ“– Lecture du fichier Excel: {EXCEL_PATH}")
    df = pd.read_excel(EXCEL_PATH)
    
    # Filtrer les lignes valides (au minimum NoCiv et nomrue non vides)
    valid_df = df.dropna(subset=['NoCiv', 'nomrue'])
    valid_df = valid_df[
        (valid_df['NoCiv'].astype(str).str.strip() != '') & 
        (valid_df['nomrue'].astype(str).str.strip() != '')
    ]
    
    total_lines = len(df)
    valid_lines = len(valid_df)
    invalid_lines = total_lines - valid_lines
    
    print(f"ðŸ“Š Total lignes Excel: {total_lines}")
    print(f"ðŸ“Š Lignes valides: {valid_lines}")
    if invalid_lines > 0:
        print(f"âš ï¸ Lignes invalides (NoCiv ou nomrue vide): {invalid_lines}")
    
    return valid_lines

def count_db_addresses():
    """Compte les adresses dans la base de donnÃ©es"""
    if not DB_PATH.exists():
        print(f"âŒ Base de donnÃ©es introuvable: {DB_PATH}")
        return 0
    
    with sqlite3.connect(DB_PATH) as conn:
        # VÃ©rifier que la table addresses existe
        tables = {row[0] for row in conn.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()}
        if "addresses" not in tables:
            print("âŒ Table 'addresses' introuvable dans la base")
            return 0
        
        # Compter les adresses
        count = conn.execute("SELECT COUNT(*) FROM addresses").fetchone()[0]
        print(f"ðŸ“Š Adresses en base: {count}")
        
        return count

def check_duplicates():
    """VÃ©rifie s'il y a des doublons dans la table addresses"""
    duplicates = []
    
    with sqlite3.connect(DB_PATH) as conn:
        # VÃ©rifier les doublons par addr_key
        cursor = conn.execute("""
            SELECT addr_key, COUNT(*) as count
            FROM addresses 
            GROUP BY addr_key 
            HAVING COUNT(*) > 1
            ORDER BY count DESC, addr_key
        """)
        
        for row in cursor.fetchall():
            addr_key, count = row
            duplicates.append((addr_key, count))
        
        if duplicates:
            print(f"âŒ {len(duplicates)} clÃ©(s) d'adresse en doublon dÃ©tectÃ©e(s):")
            for addr_key, count in duplicates[:10]:  # Montrer max 10 exemples
                print(f"  - '{addr_key}': {count} occurrences")
            if len(duplicates) > 10:
                print(f"  ... et {len(duplicates) - 10} autre(s)")
        else:
            print("âœ… Aucun doublon dÃ©tectÃ© (addr_key unique)")
    
    return duplicates

def check_data_integrity():
    """VÃ©rifie l'intÃ©gritÃ© des donnÃ©es"""
    issues = []
    
    with sqlite3.connect(DB_PATH) as conn:
        # VÃ©rifier les adresses avec street_name ou house_number vide
        empty_street = conn.execute("SELECT COUNT(*) FROM addresses WHERE street_name IS NULL OR TRIM(street_name) = ''").fetchone()[0]
        empty_house = conn.execute("SELECT COUNT(*) FROM addresses WHERE house_number IS NULL OR TRIM(house_number) = ''").fetchone()[0]
        
        if empty_street > 0:
            issues.append(f"âŒ {empty_street} adresse(s) avec street_name vide")
        
        if empty_house > 0:
            issues.append(f"âŒ {empty_house} adresse(s) avec house_number vide")
        
        # VÃ©rifier les addr_key vides
        empty_key = conn.execute("SELECT COUNT(*) FROM addresses WHERE addr_key IS NULL OR TRIM(addr_key) = ''").fetchone()[0]
        if empty_key > 0:
            issues.append(f"âŒ {empty_key} adresse(s) avec addr_key vide")
        
        # Statistiques des assignements
        assigned = conn.execute("SELECT COUNT(*) FROM addresses WHERE assigned_to IS NOT NULL AND TRIM(assigned_to) <> ''").fetchone()[0]
        unassigned = conn.execute("SELECT COUNT(*) FROM addresses WHERE assigned_to IS NULL OR TRIM(assigned_to) = ''").fetchone()[0]
        
        print(f"ðŸ“Š Adresses assignÃ©es: {assigned}")
        print(f"ðŸ“Š Adresses non assignÃ©es: {unassigned}")
        
        if issues:
            for issue in issues:
                print(issue)
        else:
            print("âœ… IntÃ©gritÃ© des donnÃ©es OK")
    
    return issues

def verify_addresses_exact():
    """VÃ©rification complÃ¨te post-import"""
    print("=== VÃ©rification Post-Import ===")
    
    exit_code = 0
    
    # 1. Compter les lignes Excel valides
    n_excel = count_valid_excel_lines()
    
    # 2. Compter les adresses en base
    n_db = count_db_addresses()
    
    # 3. VÃ©rifier les doublons
    duplicates = check_duplicates()
    
    # 4. VÃ©rifier l'intÃ©gritÃ©
    integrity_issues = check_data_integrity()
    
    print("\n=== RÃ‰SUMÃ‰ FINAL ===")
    
    # Comparaison Excel vs DB
    if n_excel == 0 and n_db == 0:
        print("âš ï¸ Aucune donnÃ©e trouvÃ©e (Excel et DB vides)")
        exit_code = 1
    elif n_excel == 0:
        print("âš ï¸ Fichier Excel vide mais DB contient des donnÃ©es")
        exit_code = 1
    elif n_db == 0:
        print("âŒ DB vide mais Excel contient des donnÃ©es")
        exit_code = 1
    else:
        diff = abs(n_excel - n_db)
        if diff == 0:
            print(f"âœ… Correspondance parfaite: {n_excel} lignes Excel = {n_db} adresses DB")
        else:
            # VÃ©rifier si c'est un Ã©cart attendu (dÃ©duplication)
            excel_higher = n_excel > n_db
            if excel_higher and diff <= 20 and not duplicates:
                print(f"âš ï¸ Diff attendu (dÃ©duplication): {n_excel} lignes Excel â†’ {n_db} adresses DB (diff: {diff})")
                # Ne pas mettre exit_code = 1 dans ce cas (tolÃ©rance dÃ©duplication)
            else:
                print(f"âŒ Ã‰cart dÃ©tectÃ©: {n_excel} lignes Excel â‰  {n_db} adresses DB (diff: {diff})")
                exit_code = 1
    
    # VÃ©rification doublons
    if duplicates:
        print(f"âŒ {len(duplicates)} doublon(s) dÃ©tectÃ©(s)")
        exit_code = 1
    else:
        print("âœ… Aucun doublon")
    
    # VÃ©rification intÃ©gritÃ©
    if integrity_issues:
        print(f"âŒ {len(integrity_issues)} problÃ¨me(s) d'intÃ©gritÃ©")
        exit_code = 1
    else:
        print("âœ… IntÃ©gritÃ© OK")
    
    # RÃ©sultat final
    if exit_code == 0:
        print("\nðŸŽ‰ VÃ‰RIFICATION RÃ‰USSIE - Toutes les donnÃ©es sont cohÃ©rentes")
    else:
        print("\nðŸ’¥ VÃ‰RIFICATION Ã‰CHOUÃ‰E - Des problÃ¨mes ont Ã©tÃ© dÃ©tectÃ©s")
    
    return exit_code

if __name__ == "__main__":
    exit_code = verify_addresses_exact()
    sys.exit(exit_code)

# ===== scripts/verify_import.py =====
# scripts/verify_import.py
import sqlite3
from pathlib import Path

DB_PATH = Path("guignomap/guigno_map.db")

def verify_import():
    with sqlite3.connect(DB_PATH) as conn:
        print("=== SchÃ©ma final de la table addresses ===")
        for row in conn.execute("PRAGMA table_info(addresses)").fetchall():
            print(f"{row[1]}: {row[2]}")
        
        print(f"\n=== Nombre total d'adresses ===")
        total = conn.execute("SELECT COUNT(*) FROM addresses").fetchone()[0]
        print(f"Total: {total}")
        
        print(f"\n=== Ã‰chantillon des donnÃ©es importÃ©es ===")
        for row in conn.execute("SELECT street_name, house_number, postal_code, sector FROM addresses LIMIT 5").fetchall():
            print(f"{row[0]} {row[1]} (postal: {row[2]}, sector: {row[3]})")
        
        print(f"\n=== RÃ©partition par secteur ===")
        for row in conn.execute("SELECT sector, COUNT(*) FROM addresses GROUP BY sector ORDER BY COUNT(*) DESC LIMIT 10").fetchall():
            print(f"Secteur '{row[0]}': {row[1]} adresses")

if __name__ == "__main__":
    verify_import()

# ===== scripts/verify_real_encoding.py =====
#!/usr/bin/env python3
"""
Script de vÃ©rification de l'encodage rÃ©el des fichiers
Teste si les fichiers sont vraiment lisibles en UTF-8
"""
import sys
from pathlib import Path

def check_file(filepath):
    """VÃ©rifie l'encodage rÃ©el d'un fichier"""
    try:
        # Test 1: UTF-8 strict
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            return "UTF-8", len(content), "OK"
    except UnicodeDecodeError as e:
        # Test 2: UTF-8 avec BOM
        try:
            with open(filepath, 'r', encoding='utf-8-sig') as f:
                content = f.read()
                return "UTF-8-BOM", len(content), "Has BOM"
        except:
            # Test 3: Windows-1252
            try:
                with open(filepath, 'r', encoding='windows-1252') as f:
                    content = f.read()
                    return "Windows-1252", len(content), "Legacy"
            except:
                return "Unknown", 0, str(e)

def main():
    # Tester les fichiers problÃ©matiques rapportÃ©s par validate_structure
    problem_files = [
        "guignomap/app.py",
        "src/database/operations.py", 
        "scripts/validate_structure.py",
        "check_admin.py",
        "README.md",
        "tools/quick_sanity.py"
    ]

    print("ðŸ” VÃ©rification de l'encodage rÃ©el des fichiers")
    print("=" * 50)
    
    all_utf8 = True
    
    for file in problem_files:
        if Path(file).exists():
            encoding, size, status = check_file(file)
            print(f"{file}: {encoding} ({size} chars) - {status}")
            if encoding not in ["UTF-8", "UTF-8-BOM"]:
                all_utf8 = False
        else:
            print(f"{file}: NOT FOUND")
    
    print("=" * 50)
    if all_utf8:
        print("âœ… Tous les fichiers testÃ©s sont lisibles en UTF-8")
        print("   Le problÃ¨me vient de la dÃ©tection charset, pas de l'encodage rÃ©el")
    else:
        print("âŒ Certains fichiers nÃ©cessitent une conversion")

if __name__ == "__main__":
    main()


# ============================================================================
# SCRIPTS LEGACY (legacy/scripts/)
# ============================================================================

# ===== legacy/scripts/fix_mojibake_db.py =====
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Correcteur de mojibake dans la base de donnÃ©es

Analyse et corrige les chaÃ®nes mojibakÃ©es dans la DB (labels, titres, settings).
Ã‰VITE les notes libres des bÃ©nÃ©voles pour Ã©viter les faux-positifs.

Usage:
    python scripts/fix_mojibake_db.py                    # DRY-RUN (affichage only)
    python scripts/fix_mojibake_db.py --apply            # Application des corrections
    python scripts/fix_mojibake_db.py --verbose          # Mode dÃ©taillÃ©
"""

import sys
import os
import argparse
from typing import Dict, List, Tuple, Optional

# Ajouter le rÃ©pertoire parent au path pour importer src/
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

# Mapping de correction des mojibakes les plus frÃ©quents  
MOJIBAKE_FIXES = {
    # CaractÃ¨res accentuÃ©s franÃ§ais courants
    "ÃƒÂ©": "Ã©",
    "ÃƒÂ¨": "Ã¨", 
    "Ãƒ ": "Ã ",
    "ÃƒÂ´": "Ã´",
    "ÃƒÂ»": "Ã»",
    "ÃƒÂ®": "Ã®",
    "ÃƒÂ§": "Ã§",
    "ÃƒÂ¢": "Ã¢",
    "ÃƒÂ¹": "Ã¹",
    "ÃƒÂ«": "Ã«",
    "ÃƒÂ¯": "Ã¯",
    "ÃƒÂ¼": "Ã¼",
    
    # Majuscules accentuÃ©es basiques
    "Ãƒâ€°": "Ã‰",
    "Ãƒâ‚¬": "Ã€", 
    "Ãƒâ€¡": "Ã‡",
    
    # Espaces et caractÃ¨res de contrÃ´le
    "Ã‚ ": " ",   # espace insÃ©cable mojibakÃ©e
    "Ã‚": "",     # caractÃ¨re de contrÃ´le isolÃ©
}


def detect_mojibake(text: str) -> bool:
    """DÃ©tecte si une chaÃ®ne contient probablement du mojibake"""
    if not text:
        return False
    
    # Recherche des patterns dans notre mapping
    for pattern in MOJIBAKE_FIXES.keys():
        if pattern in text:
            return True
    
    # Recherche de patterns supplÃ©mentaires (caractÃ¨res suspects en sÃ©quence)
    suspicious_patterns = ["Ãƒ", "Ã¢â‚¬", "âˆš", "Ã‚", "Ã‹", "Ã‘"]
    for pattern in suspicious_patterns:
        if pattern in text:
            return True
    
    return False


def fix_mojibake(text: str) -> Tuple[str, bool]:
    """
    Corrige le mojibake dans une chaÃ®ne
    
    Returns:
        (texte_corrigÃ©, a_Ã©tÃ©_modifiÃ©)
    """
    if not text:
        return text, False
    
    original = text
    result = text
    
    # Appliquer les corrections du mapping
    for pattern, replacement in MOJIBAKE_FIXES.items():
        result = result.replace(pattern, replacement)
    
    return result, result != original


def get_correctable_text_fields() -> Dict[str, List[str]]:
    """
    Retourne les tables et champs texte que l'on peut corriger en toute sÃ©curitÃ©
    
    Ã‰VITE les notes libres pour Ã©viter les faux-positifs
    """
    return {
        "teams": ["name"],       # Nom d'Ã©quipe (safe Ã  corriger)
        "streets": ["name"],     # Nom de rue (safe Ã  corriger) 
        # "notes": [],           # Ã‰VITÃ‰: notes libres des bÃ©nÃ©voles
        # "activity_log": [],    # Ã‰VITÃ‰: logs peuvent contenir des donnÃ©es brutes
    }


def scan_database_mojibake(verbose: bool = False) -> List[Tuple[str, str, int, str, str]]:
    """
    Scanne la DB pour dÃ©tecter le mojibake
    
    Returns:
        List de (table, field, record_id, old_value, new_value)
    """
    try:
        from src.database.connection import get_session
        from sqlalchemy import text
    except ImportError as e:
        print(f"âŒ Erreur d'import: {e}")
        return []
    
    issues = []
    correctable_fields = get_correctable_text_fields()
    
    try:
        with get_session() as session:
            for table, fields in correctable_fields.items():
                if verbose:
                    print(f"ðŸ” Scan table: {table}")
                
                for field in fields:
                    if verbose:
                        print(f"  ðŸ“‹ Champ: {field}")
                    
                    # RequÃªte pour rÃ©cupÃ©rer les enregistrements
                    query = f"SELECT id, {field} FROM {table} WHERE {field} IS NOT NULL"
                    result = session.execute(text(query))
                    
                    for row in result:
                        record_id, value = row
                        
                        if detect_mojibake(value):
                            fixed_value, was_changed = fix_mojibake(value)
                            if was_changed:
                                issues.append((table, field, record_id, value, fixed_value))
                                if verbose:
                                    print(f"    ðŸ”§ ID {record_id}: '{value}' -> '{fixed_value}'")
    
    except Exception as e:
        print(f"âŒ Erreur scan DB: {e}")
        return []
    
    return issues


def apply_fixes(issues: List[Tuple[str, str, int, str, str]], verbose: bool = False) -> int:
    """
    Applique les corrections dans la base de donnÃ©es
    
    Returns:
        Nombre de corrections appliquÃ©es
    """
    try:
        from src.database.connection import get_session
        from sqlalchemy import text
    except ImportError as e:
        print(f"âŒ Erreur d'import: {e}")
        return 0
    
    applied = 0
    
    try:
        with get_session() as session:
            for table, field, record_id, old_value, new_value in issues:
                if verbose:
                    print(f"ðŸ”§ Correction {table}.{field} ID {record_id}")
                    print(f"   Avant: {old_value}")
                    print(f"   AprÃ¨s: {new_value}")
                
                # Application de la correction
                query = f"UPDATE {table} SET {field} = :new_value WHERE id = :record_id"
                session.execute(text(query), {
                    "new_value": new_value,
                    "record_id": record_id
                })
                applied += 1
            
            session.commit()
            
    except Exception as e:
        print(f"âŒ Erreur application corrections: {e}")
        return 0
    
    return applied


def print_issues_report(issues: List[Tuple[str, str, int, str, str]]):
    """Affiche un rapport des problÃ¨mes dÃ©tectÃ©s"""
    if not issues:
        print("âœ… Aucun mojibake dÃ©tectÃ© dans la DB !")
        return
    
    print(f"ðŸ” {len(issues)} problÃ¨me(s) de mojibake dÃ©tectÃ©(s):\n")
    
    # Grouper par table
    by_table = {}
    for table, field, record_id, old_value, new_value in issues:
        if table not in by_table:
            by_table[table] = []
        by_table[table].append((field, record_id, old_value, new_value))
    
    for table, table_issues in by_table.items():
        print(f"ðŸ“‹ Table '{table}': {len(table_issues)} problÃ¨me(s)")
        
        for field, record_id, old_value, new_value in table_issues:
            print(f"   ðŸ”§ {field} (ID {record_id}):")
            print(f"      Avant: {old_value}")
            print(f"      AprÃ¨s: {new_value}")
        print()


def main():
    """Point d'entrÃ©e principal"""
    parser = argparse.ArgumentParser(description="Correcteur de mojibake dans la DB")
    parser.add_argument("--apply", action="store_true",
                       help="Applique les corrections (sinon DRY-RUN)")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Affichage dÃ©taillÃ©")
    
    args = parser.parse_args()
    
    print("ðŸ” Scanner de mojibake dans la base de donnÃ©es")
    print("âš ï¸  Note: Ã©vite les notes libres pour Ã©viter les faux-positifs")
    
    if not args.apply:
        print("ðŸ”¬ Mode DRY-RUN: aucune modification ne sera appliquÃ©e")
    
    print()
    
    # Scan
    print("ðŸ” Analyse de la base de donnÃ©es...")
    issues = scan_database_mojibake(args.verbose)
    
    # Rapport
    print_issues_report(issues)
    
    if issues and args.apply:
        print("ðŸ”§ Application des corrections...")
        applied = apply_fixes(issues, args.verbose)
        print(f"âœ… {applied} correction(s) appliquÃ©e(s)")
    elif issues:
        print(f"ðŸ’¡ Utilisez --apply pour corriger {len(issues)} problÃ¨me(s)")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())

# ===== legacy/scripts/generate_audit_optimise.py =====
#!/usr/bin/env python3
"""
Script de gÃ©nÃ©ration d'export optimisÃ© pour audit GuignoMap
Version allÃ©gÃ©e - SANS backups, __pycache__, logs, etc.
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import subprocess

# Configuration
OUTPUT_FILE = "export_audit_optimise.txt"
PROJECT_ROOT = Path(__file__).parent.parent

# Liste OPTIMISÃ‰E des fichiers essentiels pour l'audit
FICHIERS_AUDIT_ESSENTIELS = [
    # Configuration principale
    ".streamlit/config.toml",
    ".streamlit/secrets.toml.example",  # Template secrets
    "streamlit_app.py",
    "requirements.txt",
    "requirements_freeze.txt",
    ".gitignore",
    "runtime.txt",                      # Version Python Streamlit Cloud
    "alembic.ini",                      # Configuration Alembic
    
    # Application principale (SEULEMENT les fichiers sources)
    "guignomap/__init__.py",
    "guignomap/app.py",
    "guignomap/db_v5.py",
    "guignomap/db.py",                  # Legacy DB (pour migration)
    "guignomap/backup.py",              # SystÃ¨me de sauvegarde
    "guignomap/osm.py",
    "guignomap/validators.py", 
    "guignomap/reports.py",
    "guignomap/assets/styles.css",
    
    # Architecture source
    "src/__init__.py",
    "src/config.py",
    "src/auth/passwords.py",
    "src/database/connection.py",
    "src/database/models.py",
    "src/database/db_v5.py",            # VÃ©rifier si diffÃ©rent de guignomap/db_v5.py
    "src/storage/__init__.py",          # Init module storage
    "src/storage/cloud.py",
    "src/storage/local.py",
    "src/utils/__init__.py",            # Init module utils
    "src/utils/adapters.py",
    
    # Scripts utilitaires essentiels
    "scripts/sanity_db_pandas.py",
    "scripts/smoke_create_map.py",
    "scripts/migrate_password_hashes.py",    # Migration Argon2
    "scripts/migrate_sqlite_to_postgres.py", # Migration DB
    "tools/quick_sanity.py",
    
    # Migration Alembic
    "src/database/migrations/env.py",
    "src/database/migrations/script.py.mako",  # Template migration
    
    # Tests essentiels
    "tests/manual/test_db_connection.py",
    "tests/manual/test_db_simple.py",
    
    # Configuration dÃ©veloppement
    ".vscode/tasks.json",               # TÃ¢ches automatisÃ©es
    ".devcontainer/devcontainer.json",  # Configuration Dev Container
]

# Fichiers Ã  EXCLURE explicitement 
FICHIERS_A_EXCLURE = [
    "__pycache__",
    ".venv",
    "backups",
    "logs", 
    "*.log",
    "*.pyc",
    "*.pyo",
    "*.db",
    "*.sqlite",
    "*.zip",
    "node_modules",
    ".git",
    "exports/export_*.txt",  # Ã‰viter les anciens exports
]

def get_system_info_minimal():
    """RÃ©cupÃ¨re les informations systÃ¨me essentielles"""
    try:
        python_version = subprocess.check_output([sys.executable, "--version"], text=True).strip()
        
        # Packages essentiels seulement
        essential_packages = [
            "streamlit", "pandas", "folium", "sqlalchemy", "psycopg2-binary", 
            "passlib", "boto3", "reportlab", "xlsxwriter", "overpy"
        ]
        
        pip_output = subprocess.check_output([sys.executable, "-m", "pip", "list"], text=True)
        essential_pip = []
        for line in pip_output.split('\n'):
            for pkg in essential_packages:
                if line.lower().startswith(pkg.lower()):
                    essential_pip.append(line)
                    break
        
        git_commit = subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
        git_branch = subprocess.check_output(["git", "branch", "--show-current"], text=True).strip()
        
        return {
            "python_version": python_version,
            "essential_packages": "\n".join(essential_pip),
            "git_commit": git_commit,
            "git_branch": git_branch,
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    except Exception as e:
        return {"error": str(e)}

def read_file_safe(file_path):
    """Lit un fichier de maniÃ¨re sÃ©curisÃ©e avec limitation de taille"""
    try:
        full_path = PROJECT_ROOT / file_path
        if not full_path.exists():
            return None
        
        # Limiter la taille des fichiers lus (max 500KB)
        size = full_path.stat().st_size
        if size > 500 * 1024:  # 500KB max
            return f"## {file_path} (FICHIER TROP VOLUMINEUX - {size/1024:.1f}KB)\nâŒ Fichier non inclus (> 500KB)\n\n"
        
        # Essayer plusieurs encodages pour Ã©viter les caractÃ¨res corrompus
        encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']
        content = None
        
        for encoding in encodings:
            try:
                with open(full_path, 'r', encoding=encoding) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            # En dernier recours, mode binaire et dÃ©codage manuel
            with open(full_path, 'rb') as f:
                raw_content = f.read()
                content = raw_content.decode('utf-8', errors='ignore')
        
        # DÃ©terminer l'extension pour le formatage
        suffix = full_path.suffix.lower()
        if suffix in ['.py']:
            lang = 'python'
        elif suffix in ['.css']:
            lang = 'css'
        elif suffix in ['.toml']:
            lang = 'toml'
        elif suffix in ['.json']:
            lang = 'json'
        elif suffix in ['.txt', '.md']:
            lang = 'text'
        else:
            lang = 'text'
        
        lines_count = len(content.splitlines())
        size_kb = len(content.encode('utf-8')) / 1024
        
        return f"""## {file_path} ({lines_count} lignes, {size_kb:.1f}KB)
```{lang}
{content}
```

"""
    except Exception as e:
        return f"âŒ ERREUR lecture {file_path}: {str(e)}\n"

def main():
    """Fonction principale optimisÃ©e"""
    print(f"ðŸš€ GÃ©nÃ©ration de l'export d'audit OPTIMISÃ‰ pour GuignoMap...")
    
    # Obtenir les infos systÃ¨me
    sys_info = get_system_info_minimal()
    
    # Commencer le fichier
    content = f"""# ================================================================================
# GUIGNOMAP - EXPORT OPTIMISÃ‰ POUR AUDIT
# Date: {sys_info.get('date', 'Inconnue')}
# Python: {sys_info.get('python_version', 'Inconnu')}
# Git: {sys_info.get('git_branch', 'main')} ({sys_info.get('git_commit', 'unknown')[:8]})
# ================================================================================

# ================================================================================
# PACKAGES ESSENTIELS
# ================================================================================

{sys_info.get('essential_packages', 'Packages non disponibles')}

# ================================================================================
# STRUCTURE PROJET (Fichiers essentiels seulement)
# ================================================================================

GuignoMap/
â”œâ”€â”€ .devcontainer/devcontainer.json     # ðŸ”§ Configuration Dev Container  
â”œâ”€â”€ .streamlit/
â”‚   â”œâ”€â”€ config.toml                     # â­ Configuration Streamlit
â”‚   â””â”€â”€ secrets.toml.example            # ðŸ“‹ Template secrets
â”œâ”€â”€ .vscode/tasks.json                  # ðŸ”§ TÃ¢ches automatisÃ©es VS Code
â”œâ”€â”€ streamlit_app.py                    # â­ Point d'entrÃ©e Cloud
â”œâ”€â”€ requirements.txt                    # ðŸ“¦ DÃ©pendances production
â”œâ”€â”€ requirements_freeze.txt             # ðŸ“¦ Versions exactes
â”œâ”€â”€ runtime.txt                         # ðŸ Version Python (Cloud)
â”œâ”€â”€ alembic.ini                         # ðŸ”§ Configuration Alembic
â”œâ”€â”€ .gitignore                          # ðŸš« Exclusions Git
â”‚
â”œâ”€â”€ guignomap/                          # ðŸŽ¯ Application principale
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                          # â­ Interface Streamlit (2000+ lignes)
â”‚   â”œâ”€â”€ db_v5.py                        # â­ Base de donnÃ©es SQLAlchemy
â”‚   â”œâ”€â”€ db.py                           # ðŸ”„ Legacy DB (migration)
â”‚   â”œâ”€â”€ backup.py                       # ðŸ’¾ SystÃ¨me de sauvegarde
â”‚   â”œâ”€â”€ osm.py                          # â­ IntÃ©gration OpenStreetMap
â”‚   â”œâ”€â”€ validators.py                   # ðŸ›¡ï¸ Validation sÃ©curisÃ©e
â”‚   â”œâ”€â”€ reports.py                      # ðŸ“Š GÃ©nÃ©ration rapports
â”‚   â””â”€â”€ assets/styles.css               # ðŸŽ¨ Styles CSS
â”‚
â”œâ”€â”€ src/                                # ðŸ—ï¸ Architecture modulaire
â”‚   â”œâ”€â”€ __init__.py                     # Init module principal
â”‚   â”œâ”€â”€ config.py                       # â­ Configuration centralisÃ©e
â”‚   â”œâ”€â”€ auth/passwords.py               # ðŸ›¡ï¸ Authentification Argon2
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py               # â­ Connexions PostgreSQL
â”‚   â”‚   â”œâ”€â”€ models.py                   # â­ ModÃ¨les SQLAlchemy
â”‚   â”‚   â”œâ”€â”€ db_v5.py                    # ðŸ”„ OpÃ©rations DB (duplicate?)
â”‚   â”‚   â””â”€â”€ migrations/
â”‚   â”‚       â”œâ”€â”€ env.py                  # Configuration Alembic
â”‚   â”‚       â””â”€â”€ script.py.mako          # Template migration
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py                 # Init module storage
â”‚   â”‚   â”œâ”€â”€ cloud.py                    # â˜ï¸ Stockage S3
â”‚   â”‚   â””â”€â”€ local.py                    # ðŸ’» Stockage local
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py                 # Init module utils
â”‚       â””â”€â”€ adapters.py                 # ðŸ”„ Adaptateurs donnÃ©es
â”‚
â”œâ”€â”€ scripts/                            # ðŸ”§ Scripts utilitaires
â”‚   â”œâ”€â”€ sanity_db_pandas.py             # ðŸ” VÃ©rification DB
â”‚   â”œâ”€â”€ smoke_create_map.py             # ðŸ—ºï¸ Test carte
â”‚   â”œâ”€â”€ migrate_password_hashes.py      # ðŸ” Migration Argon2
â”‚   â””â”€â”€ migrate_sqlite_to_postgres.py   # ðŸ”„ Migration PostgreSQL
â”‚
â”œâ”€â”€ tests/manual/                       # ðŸ§ª Tests manuels
â”‚   â”œâ”€â”€ test_db_connection.py           # Test connexion DB
â”‚   â””â”€â”€ test_db_simple.py               # Test simple DB
â”‚
â””â”€â”€ tools/
    â””â”€â”€ quick_sanity.py                 # âš¡ VÃ©rification rapide

# ================================================================================
# FICHIERS SOURCES COMPLETS
# ================================================================================

"""
    
    # Lire tous les fichiers essentiels
    print(f"ðŸ“ Lecture de {len(FICHIERS_AUDIT_ESSENTIELS)} fichiers essentiels...")
    
    fichiers_lus = 0
    fichiers_manquants = []
    taille_totale = 0
    
    for fichier in FICHIERS_AUDIT_ESSENTIELS:
        print(f"  ðŸ“„ {fichier}")
        file_content = read_file_safe(fichier)
        
        if file_content is None:
            fichiers_manquants.append(fichier)
            continue
        elif "âŒ" in file_content and "ERREUR" in file_content:
            fichiers_manquants.append(fichier)
            content += file_content
        else:
            content += file_content
            fichiers_lus += 1
            taille_totale += len(file_content.encode('utf-8'))
    
    # Ajouter JSON essentiels (s'ils existent et sont raisonnables)
    json_essentiels = ["guignomap/osm_addresses.json"]  # Seulement celui-ci
    
    content += """# ================================================================================
# DONNÃ‰ES JSON ESSENTIELLES
# ================================================================================

"""
    
    for json_file in json_essentiels:
        json_content = read_file_safe(json_file)
        if json_content and "âŒ" not in json_content:
            content += json_content
    
    # RÃ©sumÃ© final optimisÃ©
    content += f"""
# ================================================================================
# RÃ‰SUMÃ‰ DE L'EXPORT OPTIMISÃ‰
# ================================================================================

âœ… Fichiers inclus: {fichiers_lus}/{len(FICHIERS_AUDIT_ESSENTIELS)}
ðŸ“Š Taille totale: {taille_totale/1024:.1f}KB
ðŸš« Exclusions: __pycache__, backups, logs, .venv, *.pyc, *.db

"""
    
    if fichiers_manquants:
        content += f"âš ï¸ Fichiers non trouvÃ©s ({len(fichiers_manquants)}):\n"
        for fichier in fichiers_manquants:
            content += f"  - {fichier}\n"
        content += "\n"
    
    content += f"""
# ================================================================================
# POINTS CLÃ‰S POUR L'AUDIT
# ================================================================================

ðŸ” SÃ‰CURITÃ‰ (PrioritÃ© 1):
- src/auth/passwords.py          # Hachage Argon2
- guignomap/validators.py        # Anti-XSS/injection
- src/config.py                  # Gestion secrets

ðŸ“Š BASE DE DONNÃ‰ES (PrioritÃ© 2):  
- guignomap/db_v5.py            # ORM SQLAlchemy
- src/database/connection.py     # Pool PostgreSQL
- src/database/models.py         # SchÃ©ma DB

ðŸŽ¨ INTERFACE (PrioritÃ© 3):
- guignomap/app.py              # Interface Streamlit
- guignomap/osm.py              # Cartes OpenStreetMap
- streamlit_app.py              # Point d'entrÃ©e

âš™ï¸ CONFIGURATION:
- .streamlit/config.toml        # ThÃ¨me et config
- requirements.txt              # DÃ©pendances

ARCHITECTURE: Streamlit + PostgreSQL + Folium + S3
DÃ‰PLOIEMENT: Compatible Streamlit Cloud
VERSION: v3.0 Production

===============================================================================
FIN EXPORT OPTIMISÃ‰ - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
===============================================================================
"""
    
    # Ã‰crire le fichier final
    output_path = PROJECT_ROOT / OUTPUT_FILE
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    final_size = len(content.encode('utf-8')) / 1024
    print(f"âœ… Export optimisÃ© gÃ©nÃ©rÃ©: {output_path}")
    print(f"ðŸ“Š Fichiers inclus: {fichiers_lus}/{len(FICHIERS_AUDIT_ESSENTIELS)}")
    print(f"ðŸ“„ Taille finale: {final_size:.1f}KB")
    print(f"ðŸš« Exclusions: __pycache__, backups, logs, .venv")
    
    if fichiers_manquants:
        print(f"âš ï¸  {len(fichiers_manquants)} fichiers non trouvÃ©s")

if __name__ == "__main__":
    main()

# ===== legacy/scripts/migrate_password_hashes.py =====
#!/usr/bin/env python3
"""
Script de migration manuelle des mots de passe vers Argon2
Migration batch de tous les hashes legacy vers Argon2

Usage:
    python scripts/migrate_password_hashes.py [--dry-run] [--team=TEAM_ID]
    
Options:
    --dry-run    : Simulation sans modifications
    --team=ID    : Migrer seulement l equipe specifiee
"""
import sys
import os
import argparse
import getpass
from pathlib import Path

# Ajout du chemin du projet pour les imports
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.database.operations import count_hash_algorithms, get_session
from guignomap.auth import hash_password, detect_hash_algo
from sqlalchemy import text


def main():
    """Point d entree principal"""
    print("Migration des mots de passe vers Argon2")
    print("Script disponible mais necessite implementation complete")


if __name__ == "__main__":
    main()

# ===== legacy/scripts/reset_teams.py =====
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.database.connection import get_session
from sqlalchemy import text
from src.database import operations

TO_DELETE = ["EQUIPE1","EQUIPE2"]
NEW_TEAMS = [("EQUIPE1","Equipe 1","admin123"),
             ("EQUIPE2","Equipe 2","admin123")]

def list_teams(label):
    with get_session() as s:
        rows = s.execute(text("SELECT id,name,active,length(COALESCE(password_hash,'')) ph_len FROM teams ORDER BY id")).fetchall()
        print(f"{label}:")
        for r in rows:
            print("  ", r)

list_teams("Avant suppression")

# Supprimer (sauf ADMIN)
with get_session() as s:
    for tid in TO_DELETE:
        s.execute(text("DELETE FROM teams WHERE id=:id AND id<>'ADMIN'"), {"id": tid})
    s.commit()

list_teams("AprÃ¨s suppression")

# RecrÃ©er proprement (nouveau hash)
for tid, name, pwd in NEW_TEAMS:
    ok = operations.create_team(tid, name, pwd)
    print(f"create_team({tid}) ->", ok, "| verify ->", operations.verify_team(tid, pwd))

list_teams("AprÃ¨s recrÃ©ation")

# ===== legacy/scripts/sanity_db_pandas.py =====
from sqlalchemy import text
import pandas as pd
import sys, pathlib

# 1) Injecte la racine du repo dans sys.path
ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# 2) Import de get_session() depuis src.database.connection
try:
    from src.database.connection import get_session
except Exception as e:
    raise SystemExit(f"[ERREUR] Import get_session introuvable depuis src.database.connection: {e}")

# 3) Test SELECT 1 + version pandas
try:
    with get_session() as s:
        print("DB OK:", s.execute(text("SELECT 1")).scalar_one())
except Exception as e:
    raise SystemExit(f"[ERREUR] DB SELECT 1 a Ã©chouÃ©: {e}")

print("pandas OK:", pd.__version__)
