# ================================================================================
# EXPORT AUDIT - GUIGNOMAP
# Contenu: ENVIRONNEMENT, D√âPENDANCES (pip freeze), SOURCES .py COMPLETS
# ================================================================================

# ================================================================================
# ENVIRONNEMENT & VERSIONS
# ================================================================================
Horodatage local : 2025-09-16T11:17:37-04:00
Racine projet    : C:\Users\nick\guignomap_clone\GuignoMap
Python           : 3.13.6
Pip              : pip 25.2 from C:\Users\nick\guignomap_clone\GuignoMap\.venv\Lib\site-packages\pip (python 3.13)
OS/Platform      : Windows-11-10.0.26100-SP0  (AMD64)
WSL              : non
Git              :
  Branche courante : main
  Commit : 043e9d0
  Status :
  # branch.oid 043e9d0533c0104305cb74345467bf8b90c77f1f
# branch.head main
# branch.upstream origin/main
# branch.ab +0 -0
1 .M N... 100644 100644 100644 247b9372a3e23ecabf3f1fc8c021a7c4c6108512 247b9372a3e23ecabf3f1fc8c021a7c4c6108512 requirements_freeze.txt
? RESUME_ARBRES_EXPORTS.md
? generate_tree_clean.py
? project_tree_clean_20250916_110318.txt
? project_tree_complete.txt
? requirements_freeze_bom.txt
? requirements_freeze_old.txt
? scripts/generate_audit_export.py
? scripts/generate_audit_optimise.py
? scripts/generate_export_txt.py

# ================================================================================
# D√âPENDANCES INSTALL√âES (pip freeze)
# ================================================================================
alembic==1.16.5
altair==5.5.0
argon2-cffi==25.1.0
argon2-cffi-bindings==25.1.0
attrs==25.3.0
bcrypt==4.3.0
blinker==1.9.0
boto3==1.34.162
botocore==1.34.162
branca==0.8.1
cachetools==5.5.2
certifi==2025.8.3
cffi==2.0.0
charset-normalizer==3.4.3
click==8.2.1
colorama==0.4.6
folium==0.20.0
gitdb==4.0.12
GitPython==3.1.45
greenlet==3.2.4
idna==3.10
Jinja2==3.1.6
jmespath==1.0.1
jsonschema==4.25.1
jsonschema-specifications==2025.9.1
Mako==1.3.10
markdown-it-py==4.0.0
MarkupSafe==3.0.2
mdurl==0.1.2
narwhals==2.5.0
numpy==2.3.3
overpy==0.7
packaging==24.2
pandas==2.3.2
passlib==1.7.4
pillow==10.4.0
plotly==6.3.0
protobuf==5.29.5
psycopg2-binary==2.9.10
pyarrow==21.0.0
pycparser==2.23
pydeck==0.9.1
Pygments==2.19.2
python-dateutil==2.9.0.post0
pytz==2025.2
referencing==0.36.2
reportlab==4.4.3
requests==2.32.5
rich==13.9.4
rpds-py==0.27.1
s3transfer==0.10.4
six==1.17.0
smmap==5.0.2
SQLAlchemy==2.0.43
streamlit==1.39.1
streamlit-folium==0.25.1
tenacity==9.1.2
toml==0.10.2
tornado==6.5.2
typing_extensions==4.15.0
tzdata==2025.2
urllib3==2.5.0
watchdog==5.0.3
xlsxwriter==3.2.8
xyzservices==2025.4.0

# ================================================================================
# SOURCES PYTHON COMPLETS (fichiers .py)
#   Exclusions: __pycache__, .venv, backups/, exports/, .git, logs/, *.pyc, *.log, *.db, *.sqlite
# ================================================================================

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: generate_tree_clean.py
# --- encoding: utf-8 | size: 6389 bytes | sha256: 80e61fbd42fb293cc036ad3c37658abfb63c2210a9741018d99251644bf88a0c | mtime: 2025-09-16T11:08:45-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Script pour g√©n√©rer un arbre propre du projet GuignoMap
Exclut les dossiers polluants comme __pycache__, .venv, backups, exports
"""

import os
from pathlib import Path
from datetime import datetime

def should_exclude(path: Path) -> bool:
    """D√©termine si un chemin doit √™tre exclu de l'arbre"""
    excludes = {
        '__pycache__',
        '.venv', 
        'backups',
        'exports',
        '.git',
        'node_modules',
        '.pytest_cache',
        '.coverage',
        '.mypy_cache'
    }
    
    # Exclure les dossiers/fichiers sp√©cifiques
    if path.name in excludes:
        return True
    
    # Exclure les fichiers temporaires
    if path.suffix in {'.pyc', '.pyo', '.pyd', '.so', '.dll'}:
        return True
    
    # Exclure les fichiers de logs
    if path.suffix in {'.log'}:
        return True
        
    # Exclure les bases de donn√©es temporaires
    if path.suffix in {'.db', '.sqlite', '.sqlite3'} and 'test' not in path.name.lower():
        return True
    
    # Excluer les fichiers d'export pr√©c√©dents
    if path.name.startswith('export_') and path.suffix == '.txt':
        return True
        
    # Exclure les fichiers zip de backup
    if path.suffix == '.zip' and any(parent.name == 'backups' for parent in path.parents):
        return True
    
    return False

def generate_tree(root_path: Path, prefix: str = "", is_last: bool = True, max_depth: int = 10, current_depth: int = 0) -> list:
    """G√©n√®re un arbre des fichiers et dossiers"""
    if current_depth >= max_depth:
        return []
    
    tree_lines = []
    
    if current_depth == 0:
        tree_lines.append(f"{root_path.name}/")
    
    try:
        # Lister tous les √©l√©ments du dossier
        items = []
        for item in root_path.iterdir():
            if not should_exclude(item):
                items.append(item)
        
        # Trier : dossiers d'abord, puis fichiers, alphab√©tiquement
        items.sort(key=lambda x: (x.is_file(), x.name.lower()))
        
        for i, item in enumerate(items):
            is_last_item = i == len(items) - 1
            
            # Cr√©er le pr√©fixe pour cet √©l√©ment
            connector = "‚îî‚îÄ‚îÄ " if is_last_item else "‚îú‚îÄ‚îÄ "
            current_prefix = prefix + connector
            
            if item.is_dir():
                # Dossier
                tree_lines.append(f"{current_prefix}{item.name}/")
                
                # R√©cursion pour le contenu du dossier
                next_prefix = prefix + ("    " if is_last_item else "‚îÇ   ")
                subtree = generate_tree(item, next_prefix, is_last_item, max_depth, current_depth + 1)
                tree_lines.extend(subtree)
            else:
                # Fichier
                size_info = ""
                try:
                    size = item.stat().st_size
                    if size < 1024:
                        size_info = f" ({size}B)"
                    elif size < 1024 * 1024:
                        size_info = f" ({size/1024:.1f}KB)"
                    else:
                        size_info = f" ({size/(1024*1024):.1f}MB)"
                except:
                    size_info = ""
                
                tree_lines.append(f"{current_prefix}{item.name}{size_info}")
    
    except PermissionError:
        tree_lines.append(f"{prefix}‚îî‚îÄ‚îÄ [Permission denied]")
    
    return tree_lines

def main():
    """Fonction principale"""
    project_root = Path(__file__).parent
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    print("üå≥ G√©n√©ration de l'arbre du projet GuignoMap...")
    
    # G√©n√©rer l'arbre
    tree_lines = generate_tree(project_root)
    
    # Cr√©er le contenu du fichier
    content = f"""# ================================================================================
# ARBRE DU PROJET GUIGNOMAP - STRUCTURE COMPL√àTE
# G√©n√©r√© le: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
# Exclusions: __pycache__, .venv, backups/, exports/, .git, *.pyc, *.log
# ================================================================================

"""
    
    content += "\n".join(tree_lines)
    
    # Ajouter des statistiques
    total_lines = len(tree_lines)
    content += f"""

# ================================================================================
# STATISTIQUES
# ================================================================================

üìä Total √©l√©ments affich√©s: {total_lines}
üö´ Exclusions appliqu√©es: __pycache__, .venv, backups, exports, *.pyc, *.log, *.db
üìÖ G√©n√©r√© le: {datetime.now().strftime('%Y-%m-%d √† %H:%M:%S')}

# ================================================================================
# L√âGENDE
# ================================================================================

üìÅ Dossiers se terminent par /
üìÑ Fichiers avec taille approximative
‚îú‚îÄ‚îÄ √âl√©ment dans la hi√©rarchie  
‚îî‚îÄ‚îÄ Dernier √©l√©ment d'un niveau
‚îÇ   Continuation de branche
    Espacement pour sous-√©l√©ments

# ================================================================================
# NOTES IMPORTANTES
# ================================================================================

‚úÖ INCLUS:
- Tous les fichiers source (.py, .css, .toml, .md, etc.)
- Configuration et documentation 
- Scripts utilitaires et outils
- Fichiers de d√©ploiement

‚ùå EXCLUS (polluants):
- __pycache__/ et *.pyc (cache Python)
- .venv/ (environnement virtuel)
- backups/ (sauvegardes automatiques)
- exports/ (exports pr√©c√©dents)
- .git/ (m√©tadonn√©es Git)
- Fichiers temporaires et logs

===============================================================================
FIN ARBRE PROJET GUIGNOMAP - {timestamp}
===============================================================================
"""
    
    # Sauvegarder le fichier
    output_file = project_root / f"project_tree_clean_{timestamp}.txt"
    output_file.write_text(content, encoding='utf-8')
    
    print(f"‚úÖ Arbre g√©n√©r√©: {output_file.name}")
    print(f"üìä {total_lines} √©l√©ments inclus")
    print(f"üìÑ Fichier: {output_file}")

if __name__ == "__main__":
    main()
# --------------------------------------------------------------------------------
# <<< END FILE: generate_tree_clean.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/__init__.py
# --- encoding: utf-8 | size: 0 bytes | sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 | mtime: 2025-09-14T14:16:51-04:00
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/__init__.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/app.py
# --- encoding: utf-8 | size: 77224 bytes | sha256: b5565b644dfd15364137d66804ed79001688a88c5ccdd670715bf7cec34d857d | mtime: 2025-09-15T23:08:55-04:00
# --------------------------------------------------------------------------------
"""
Guigno-Map - Application de gestion de collecte de denr√©es
Le Relais de Mascouche
Version 3.0 - Production
"""

from pathlib import Path
import time
from datetime import datetime
import pandas as pd
import streamlit as st

# Configuration Streamlit (doit √™tre la premi√®re commande Streamlit)
st.set_page_config(
    page_title="Guigno-Map | Relais de Mascouche",
    page_icon="üéÅ",
    layout="wide",
    initial_sidebar_state="expanded"
)

import folium
from streamlit_folium import st_folium

# Import des modules locaux
from src.database import db_v5 as db
from guignomap.validators import validate_and_clean_input
from guignomap.osm import build_geometry_cache, load_geometry_cache, build_addresses_cache, load_addresses_cache, CACHE_FILE
from src.utils.adapters import to_dataframe

# --- Utilitaire de compatibilit√© pandas Styler ---
from typing import Callable, Any

def style_map_compat(df: pd.DataFrame, fn: Callable[[Any], str], subset: Any = None):
    """Applique un style cellule-√†-cellule en utilisant Styler.map si disponible,
    sinon fallback dynamique vers applymap sans exposer l'attribut (OK pour Pylance).
    
    Args:
        df: DataFrame √† styliser
        fn: Fonction qui prend une valeur cellule et retourne une string CSS
        subset: Colonnes √† cibler (ex: ['status'] ou None pour toutes)
    """
    styler = df.style
    if hasattr(styler, "map"):
        # Pandas 2.4+ : utilise la nouvelle API map()
        return styler.map(fn, subset=subset)
    # Pandas < 2.4 : fallback vers applymap (sans r√©f√©rence statique)
    return getattr(styler, "applymap")(fn, subset=subset)

# --- Mapping des statuts pour l'affichage ---
STATUS_TO_LABEL = {"a_faire": "√Ä faire", "en_cours": "En cours", "terminee": "Termin√©e"}
LABEL_TO_STATUS = {v: k for k, v in STATUS_TO_LABEL.items()}

ASSETS = Path(__file__).parent / "assets"

# Initialisation session
if "auth" not in st.session_state:
    st.session_state.auth = None

# ============================================
# COMPOSANTS UI
# ============================================

def inject_css():
    """Charge le CSS depuis le fichier externe"""
    css_file = ASSETS / "styles.css"
    if css_file.exists():
        css = css_file.read_text(encoding="utf-8")
        st.markdown(f"<style>{css}</style>", unsafe_allow_html=True)

def render_header():
    """Header moderne avec logo Guignol√©e et design festif"""
    
    # Container principal avec fond festif
    st.markdown("""
    <div style="
        background: linear-gradient(135deg, #c41e3a 0%, #165b33 100%);
        border-radius: 20px;
        padding: 2rem;
        margin-bottom: 2rem;
        position: relative;
        overflow: hidden;
        box-shadow: 0 10px 30px rgba(0,0,0,0.3);
    ">
        <!-- Flocons de neige anim√©s en CSS -->
        <div style="position: absolute; width: 100%; height: 100%; opacity: 0.1;">
            <span style="position: absolute; top: 10%; left: 10%; font-size: 2rem;">‚ùÑÔ∏è</span>
            <span style="position: absolute; top: 20%; left: 80%; font-size: 1.5rem;">‚ùÑÔ∏è</span>
            <span style="position: absolute; top: 60%; left: 30%; font-size: 1.8rem;">‚ùÑÔ∏è</span>
        </div>
    """, unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns([2, 5, 2])
    
    with col1:
        # Logo Guignol√©e
        if (ASSETS / "guignolee.png").exists():
            st.image(str(ASSETS / "guignolee.png"), width=150)
    
    with col2:
        st.markdown("""
        <div style="text-align: center;">
            <h1 style="
                color: white;
                font-family: 'Manrope', sans-serif;
                font-size: 2.5rem;
                margin: 0;
                text-shadow: 3px 3px 6px rgba(0,0,0,0.5);
                letter-spacing: 2px;
            ">üéÖ GUIGNOL√âE 2025 üéÅ</h1>
            <p style="
                color: #FFD700;
                font-size: 1.2rem;
                margin: 0.5rem 0 0 0;
                font-weight: 600;
            ">Le Relais de Mascouche - 1er d√©cembre</p>
            <p style="
                color: rgba(255,255,255,0.9);
                font-size: 1rem;
                margin-top: 0.5rem;
            ">Syst√®me de gestion de collecte</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        # Stats en temps r√©el
        stats = db.extended_stats()
        progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
        
        st.markdown(f"""
        <div style="
            background: rgba(255,255,255,0.2);
            border-radius: 15px;
            padding: 1rem;
            text-align: center;
        ">
            <div style="color: #FFD700; font-size: 2rem; font-weight: bold;">
                {progress:.0f}%
            </div>
            <div style="color: white; font-size: 0.9rem;">
                Compl√©t√©
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("</div>", unsafe_allow_html=True)

def render_login_card(role="benevole"):
    """Carte de connexion moderne avec design festif"""
    
    # Container de connexion stylis√©
    st.markdown("""
    <div style="
        max-width: 400px;
        margin: 3rem auto;
        background: linear-gradient(135deg, rgba(255,255,255,0.1), rgba(255,255,255,0.05));
        backdrop-filter: blur(10px);
        border: 2px solid rgba(255,215,0,0.3);
        border-radius: 20px;
        padding: 2rem;
        box-shadow: 0 20px 40px rgba(0,0,0,0.3);
    ">
    """, unsafe_allow_html=True)
    
    # Ic√¥ne et titre
    if role == "superviseur" or role == "gestionnaire":
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="font-size: 3rem;">üëî</div>
            <h2 style="color: #FFD700; margin: 1rem 0;">Espace Gestionnaire</h2>
            <p style="color: #cbd5e1;">G√©rez la collecte et les √©quipes</p>
        </div>
        """, unsafe_allow_html=True)
        
        with st.form("login_gestionnaire", clear_on_submit=False):
            password = st.text_input(
                "üîê Mot de passe",
                type="password",
                placeholder="Entrez le mot de passe gestionnaire"
            )
            
            col1, col2, col3 = st.columns([1,2,1])
            with col2:
                submit = st.form_submit_button(
                    "üöÄ Connexion",
                    width="stretch"
                )
            
            if submit:
                if db.verify_team("ADMIN", password):
                    st.session_state.auth = {"role": "supervisor", "team_id": "ADMIN"}
                    st.success("‚úÖ Bienvenue dans l'espace gestionnaire!")
                    st.snow()
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("‚ùå Mot de passe incorrect")
    
    else:  # B√©n√©vole
        st.markdown("""
        <div style="text-align: center; margin-bottom: 2rem;">
            <div style="font-size: 3rem;">üéÖ</div>
            <h2 style="color: #FFD700; margin: 1rem 0;">Espace B√©n√©vole</h2>
            <p style="color: #cbd5e1;">Acc√©dez √† vos rues assign√©es</p>
        </div>
        """, unsafe_allow_html=True)
        
        with st.form("login_benevole", clear_on_submit=False):
            col1, col2 = st.columns(2)
            
            with col1:
                team_id = st.text_input(
                    "üë• Identifiant d'√©quipe",
                    placeholder="Ex: EQ001"
                )
            
            with col2:
                password = st.text_input(
                    "üîê Mot de passe",
                    type="password",
                    placeholder="Mot de passe √©quipe"
                )
            
            col1, col2, col3 = st.columns([1,2,1])
            with col2:
                submit = st.form_submit_button(
                    "üéÑ Connexion",
                    width="stretch"
                )
            
            if submit:
                if db.verify_team(team_id, password):
                    st.session_state.auth = {"role": "volunteer", "team_id": team_id}
                    st.success(f"‚úÖ Bienvenue √©quipe {team_id}!")
                    st.snow()
                    time.sleep(1)
                    st.rerun()
                else:
                    st.error("‚ùå Identifiants incorrects")
    
    st.markdown("</div>", unsafe_allow_html=True)
    
    # Aide en bas
    st.markdown("""
    <div style="text-align: center; margin-top: 2rem; color: #8b92a4;">
        <small>
        Besoin d'aide? Contactez votre gestionnaire<br>
        üìû 450-474-4133
        </small>
    </div>
    """, unsafe_allow_html=True)

def render_metrics(stats):
    """Affiche les m√©triques principales"""
    progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Rues", stats['total'])
    
    with col2:
        st.metric("Rues Termin√©es", stats['done'])
    
    with col3:
        st.metric("En Cours", stats.get('partial', 0))
    
    with col4:
        st.metric("Progression", f"{progress:.1f}%")

def render_dashboard_gestionnaire(geo):
    """Dashboard moderne pour gestionnaires avec KPIs visuels"""
    
    # KPIs principaux en cartes color√©es
    stats = db.extended_stats()
    progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
    
    st.markdown("### üìä Tableau de bord en temps r√©el")
    
    # Ligne de KPIs avec ic√¥nes festives
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #22c55e, #16a34a);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(34,197,94,0.3);
        ">
            <div style="font-size: 2.5rem;">üèòÔ∏è</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{stats['total']}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Total Rues</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(59,130,246,0.3);
        ">
            <div style="font-size: 2.5rem;">‚úÖ</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{stats['done']}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Termin√©es</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #f59e0b, #d97706);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(245,158,11,0.3);
        ">
            <div style="font-size: 2.5rem;">üö∂</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{stats.get('partial', 0)}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">En cours</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        # Nombre d'√©quipes actives
        teams_count = len(db.teams())
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #8b5cf6, #7c3aed);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(139,92,246,0.3);
        ">
            <div style="font-size: 2.5rem;">üë•</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{teams_count}</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">√âquipes</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col5:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #c41e3a, #165b33);
            padding: 1.5rem;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(196,30,58,0.3);
        ">
            <div style="font-size: 2.5rem;">üéØ</div>
            <div style="color: white; font-size: 2rem; font-weight: bold;">{progress:.0f}%</div>
            <div style="color: rgba(255,255,255,0.9); font-size: 0.9rem;">Progression</div>
        </div>
        """, unsafe_allow_html=True)
    
    # Barre de progression visuelle
    st.markdown("### üéÑ Progression globale")
    st.progress(progress / 100)
    
    # Graphique par secteur (si disponible)
    st.markdown("### üìà Performance par √©quipe")
    try:
        teams_stats = db.stats_by_team()
        if teams_stats:  # Liste non vide
            # Convertir en DataFrame pour plotly
            import pandas as pd
            teams_df = pd.DataFrame(teams_stats)
            
            # Calculer le pourcentage de progression
            teams_df['progress'] = ((teams_df['completed'] / teams_df['total_streets']) * 100).fillna(0)
            
            # Graphique en barres color√©es
            import plotly.express as px
            fig = px.bar(
                teams_df, 
                x='id', 
                y='progress',
                color='progress',
                color_continuous_scale=['#ef4444', '#f59e0b', '#22c55e'],
                labels={'team': '√âquipe', 'progress': 'Progression (%)'},
                title="Performance des √©quipes"
            )
            fig.update_layout(
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)',
                font_color='white'
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Aucune statistique d'√©quipe disponible")
    except Exception as e:
        st.warning("Graphiques non disponibles (module plotly manquant)")
        # Fallback vers un tableau simple
        try:
            teams_stats = db.stats_by_team()
            if teams_stats:  # Liste non vide
                st.dataframe(to_dataframe(teams_stats), use_container_width=True)
        except:
            st.info("Aucune statistique d'√©quipe disponible")

def add_persistent_legend(m):
    """Ajoute une l√©gende persistante pour les 4 √©tats des rues via contr√¥le HTML"""
    legend_html = """
    <div id='gm-legend' class='leaflet-control-layers leaflet-control' 
         style='position: absolute; bottom: 10px; right: 10px; z-index: 1000;
                background: white; border: 2px solid rgba(0,0,0,0.2); 
                border-radius: 5px; padding: 10px; box-shadow: 0 1px 5px rgba(0,0,0,0.2);
                font-family: "Helvetica Neue", Arial, Helvetica, sans-serif; 
                font-size: 12px; line-height: 18px; color: #333;'>
        <strong style='margin-bottom: 8px; display: block;'>L√©gende</strong>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px solid #28a745; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>Termin√©e</span>
        </div>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px solid #f1c40f; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>En cours</span>
        </div>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px solid #ff4d4f; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>Assign√©e (√† faire)</span>
        </div>
        <div style='margin: 4px 0; display: flex; align-items: center;'>
            <span style='width: 20px; height: 0; border-top: 3px dashed #ff4d4f; 
                         display: inline-block; margin-right: 8px;'></span>
            <span>Non assign√©e</span>
        </div>
    </div>
    """
    m.get_root().html.add_child(folium.Element(legend_html))

def create_map(df, geo):
    """Cr√©e la carte Folium centr√©e sur Mascouche avec toutes les rues"""
    # 1) Coercition s√ªre en DataFrame
    if not isinstance(df, pd.DataFrame):
        try:
            df = pd.DataFrame(df)
        except Exception:
            df = pd.DataFrame([])
    
    # Limites de Mascouche
    bounds = {
        "north": 45.78,
        "south": 45.70,
        "east": -73.55,
        "west": -73.70
    }
    center = [(bounds["north"] + bounds["south"]) / 2, 
              (bounds["east"] + bounds["west"]) / 2]
    
    # Cr√©er la carte
    m = folium.Map(
        location=center,
        zoom_start=13,  # Zoom optimis√© pour voir toute la ville
        tiles="https://{s}.tile.openstreetmap.fr/osmfr/{z}/{x}/{y}.png",
        attr='¬© OpenStreetMap France',
        control_scale=True,
        max_bounds=True,
        min_zoom=11,
        max_zoom=18,
        prefer_canvas=True,
        zoom_control=True,
        scrollWheelZoom=True
    )
    
    # Ajouter plusieurs couches de fond
    folium.TileLayer(
        tiles='https://{s}.tile.openstreetmap.fr/osmfr/{z}/{x}/{y}.png',
        attr='¬© OpenStreetMap France',
        name='OSM France (D√©taill√©)',
        overlay=False,
        control=True
    ).add_to(m)

    folium.TileLayer(
        tiles='https://{s}.basemaps.cartocdn.com/rastertiles/voyager/{z}/{x}/{y}.png',
        attr='¬© CARTO',
        name='CARTO Voyager',
        overlay=False,
        control=True
    ).add_to(m)

    folium.TileLayer(
        tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Street_Map/MapServer/tile/{z}/{y}/{x}',
        attr='¬© Esri',
        name='Esri WorldStreetMap',
        overlay=False,
        control=True
    ).add_to(m)

    # Ajouter le contr√¥le des couches
    folium.LayerControl().add_to(m)
    
    # D√©finir les limites de la carte sur Mascouche
    m.fit_bounds([[bounds["south"], bounds["west"]], 
                  [bounds["north"], bounds["east"]]])
    
    if not geo:
        st.warning("Aucune donn√©e g√©om√©trique disponible")
        return m
    
    # Construire le lookup des infos DB
    street_info = {}
    if not df.empty:  # DataFrame non vide
        for idx, row in df.iterrows():
            name = str(row.get('name', '')) if pd.notna(row.get('name', '')) else ''
            status = row.get('status', 'a_faire')
            status = status if pd.notna(status) else 'a_faire'
            team = row.get('team', '')
            team = team if pd.notna(team) else ''
            notes = str(row.get('notes', '0')) if pd.notna(row.get('notes', '0')) else '0'
            
            street_info[name] = {
                'status': status,
                'team': str(team).strip() if team else '',
                'notes': notes
            }
    
    # Couleurs par statut
    status_colors = {
        'terminee': '#22c55e',  # Vert
        'en_cours': '#f59e0b',  # Orange
        'a_faire': '#ef4444'    # Rouge
    }
    
    # Compteurs pour stats
    stats = {"total": 0, "assigned": 0, "unassigned": 0}
    
    # Ajouter TOUTES les rues de la g√©om√©trie
    for name, paths in geo.items():
        stats["total"] += 1
        
        # Info depuis DB ou d√©faut (rouge pointill√©)
        info = street_info.get(name, {
            'status': 'a_faire',
            'team': '',
            'notes': '0'
        })
        
        status = info['status']
        team = info['team']
        notes = info['notes']
        
        # Style: TOUJOURS pointill√© si pas d'√©quipe
        has_team = bool(team)
        color = status_colors.get(status, '#ef4444')  # Rouge par d√©faut
        opacity = 0.9 if has_team else 0.7
        dash = None if has_team else '8,12'  # Pointill√©s si non assign√©
        weight = 7 if has_team else 5
        
        if has_team:
            stats["assigned"] += 1
        else:
            stats["unassigned"] += 1
        
        # Tooltip informatif
        tooltip_html = f"""
        <div style='font-family: sans-serif'>
            <strong style='font-size: 14px'>{name}</strong><br>
            <span style='color: {color}'>‚óè Statut: {status.replace('_', ' ').title()}</span><br>
            <span>üìã √âquipe: {team if team else '‚ö†Ô∏è NON ASSIGN√âE'}</span><br>
            <span>üìù Notes: {notes}</span>
        </div>
        """
        
        # Ajouter chaque segment de la rue
        for path in paths:
            if path and len(path) >= 2:
                folium.PolyLine(
                    path,
                    color=color,
                    weight=weight,
                    opacity=opacity,
                    dash_array=dash,
                    tooltip=folium.Tooltip(tooltip_html, sticky=True)
                ).add_to(m)
    
    # Ajouter un marqueur au centre-ville
    folium.Marker(
        [45.7475, -73.6005],
        popup="Centre-ville de Mascouche",
        tooltip="Centre-ville",
        icon=folium.Icon(color='red', icon='info-sign')
    ).add_to(m)
    
    # Ajouter la l√©gende persistante
    add_persistent_legend(m)
    
    return m


# ============================================
# UTILITAIRES EXPORT
# ============================================

def export_excel_professionnel(conn):
    """Export Excel avec mise en forme professionnelle"""
    try:
        from reports import ReportGenerator
        generator = ReportGenerator()
        return generator.generate_excel()
    except ImportError:
        # Fallback si les d√©pendances ne sont pas install√©es
        return db.export_to_csv()


# ============================================
# FONCTIONNALIT√âS AVANC√âES
# ============================================

def detect_mobile():
    """D√©tecte si l'utilisateur est sur mobile"""
    try:
        # R√©cup√©rer les param√®tres de l'URL pour forcer le mode mobile
        query_params = st.experimental_get_query_params()
        if 'mobile' in query_params:
            return True
        
        # Mobile-first approach pour l'instant
        return True
    except:
        return False

def show_notification(message, type="success"):
    """Affiche une notification stylis√©e"""
    icons = {
        "success": "‚úÖ",
        "error": "‚ùå",
        "warning": "‚ö†Ô∏è",
        "info": "‚ÑπÔ∏è"
    }
    colors = {
        "success": "#22c55e",
        "error": "#ef4444", 
        "warning": "#f59e0b",
        "info": "#3b82f6"
    }
    
    st.markdown(f"""
    <div style="
        position: fixed;
        top: 20px;
        right: 20px;
        background: {colors[type]};
        color: white;
        padding: 1rem 1.5rem;
        border-radius: 10px;
        box-shadow: 0 5px 15px rgba(0,0,0,0.3);
        z-index: 9999;
        animation: slideIn 0.3s ease-out;
    ">
        <strong>{icons[type]} {message}</strong>
    </div>
    <style>
    @keyframes slideIn {{
        from {{ transform: translateX(100%); opacity: 0; }}
        to {{ transform: translateX(0); opacity: 1; }}
    }}
    </style>
    """, unsafe_allow_html=True)

def show_team_badges(team_id):
    """Affiche les badges de r√©ussite de l'√©quipe"""
    try:
        df = db.list_streets(team=team_id)
        done = len(df[df['status'] == 'terminee'])
        total = len(df)
        
        badges = []
        if done >= 1:
            badges.append("üèÜ Premi√®re rue!")
        if done >= total * 0.25:
            badges.append("ü•â 25% compl√©t√©")
        if done >= total * 0.5:
            badges.append("ü•à 50% compl√©t√©")
        if done >= total * 0.75:
            badges.append("ü•á 75% compl√©t√©")
        if done == total:
            badges.append("üåü CHAMPION!")
        
        if badges:
            st.markdown(f"""
            <div style="
                background: linear-gradient(135deg, #FFD700, #FFA500);
                padding: 1rem;
                border-radius: 10px;
                text-align: center;
                margin: 1rem 0;
            ">
                <strong>Vos badges:</strong><br>
                <div style="font-size: 2rem; margin-top: 0.5rem;">
                    {' '.join(badges)}
                </div>
            </div>
            """, unsafe_allow_html=True)
    except:
        pass

def generate_sms_list(conn):
    """G√©n√®re une liste de t√©l√©phones pour SMS de groupe"""
    try:
        # Cette fonction n√©cessiterait une table de t√©l√©phones
        # Pour l'instant, retourne un exemple
        return "# Liste des t√©l√©phones b√©n√©voles\n# 450-XXX-XXXX\n# 438-XXX-XXXX"
    except:
        return "Liste non disponible"

def page_export_gestionnaire(conn):
    """Section export avec formats multiples"""
    
    st.markdown("### üìä Centre d'export des donn√©es")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div style="text-align: center; padding: 1rem; border: 2px dashed #ccc; border-radius: 10px;">
            <h4>ÔøΩ Rapport PDF</h4>
            <p><small>Format professionnel pour pr√©sentation</small></p>
        </div>
        """, unsafe_allow_html=True)
        
        try:
            from reports import ReportGenerator
            generator = ReportGenerator()
            pdf_data = generator.generate_pdf()
            st.download_button(
                "üì• T√©l√©charger PDF",
                pdf_data,
                "rapport_guignolee_2025.pdf",
                "application/pdf",
                width="stretch"
            )
        except ImportError:
            st.button("PDF (Installer reportlab)", disabled=True, width="stretch")
    
    with col2:
        st.markdown("""
        <div style="text-align: center; padding: 1rem; border: 2px dashed #ccc; border-radius: 10px;">
            <h4>üìä Excel d√©taill√©</h4>
            <p><small>Avec graphiques et mise en forme</small></p>
        </div>
        """, unsafe_allow_html=True)
        
        try:
            excel_data = export_excel_professionnel(conn)
            st.download_button(
                "üì• T√©l√©charger Excel",
                excel_data,
                "guignolee_2025.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                width="stretch"
            )
        except:
            st.button("Excel (Non disponible)", disabled=True, width="stretch")
    
    with col3:
        st.markdown("""
        <div style="text-align: center; padding: 1rem; border: 2px dashed #ccc; border-radius: 10px;">
            <h4>üì± Liste SMS</h4>
            <p><small>T√©l√©phones des b√©n√©voles</small></p>
        </div>
        """, unsafe_allow_html=True)
        
        sms_list = generate_sms_list(conn)
        st.download_button(
            "üì• Liste t√©l√©phones",
            sms_list,
            "telephones_benevoles.txt",
            "text/plain",
            width="stretch"
        )


# ============================================
# PAGES
# ============================================

def page_accueil(conn, geo):
    """Page d'accueil"""
    st.markdown("### üéÅ Bienvenue sur Guigno-Map!")
    st.info("S√©lectionnez votre mode dans le menu de gauche pour commencer.")
    
    st.markdown("---")
    st.markdown("#### üìä Aper√ßu de la collecte")
    
    stats = db.extended_stats()
    render_metrics(stats)
    
    df_all = db.list_streets()
    if not df_all.empty:  # Liste non vide
        m = create_map(df_all, geo)
        st_folium(m, height=800, width=None, returned_objects=[])

def page_accueil_v2(geo):
    """Page d'accueil festive avec compte √† rebours"""
    
    # Compte √† rebours jusqu'au 1er d√©cembre
    from datetime import datetime, timedelta
    target = datetime(2025, 12, 1, 8, 0, 0)
    now = datetime.now()
    diff = target - now
    
    if diff.days > 0:
        st.markdown(f"""
        <div style="
            background: linear-gradient(135deg, #c41e3a, #165b33);
            padding: 2rem;
            border-radius: 20px;
            text-align: center;
            margin-bottom: 2rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        ">
            <h2 style="color: #FFD700; margin: 0;">‚è∞ Compte √† rebours Guignol√©e</h2>
            <div style="font-size: 3rem; color: white; margin: 1rem 0;">
                {diff.days} jours {diff.seconds//3600} heures
            </div>
            <p style="color: rgba(255,255,255,0.9);">avant le grand jour!</p>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div style="
            background: linear-gradient(135deg, #FFD700, #FFA500);
            padding: 2rem;
            border-radius: 20px;
            text-align: center;
            margin-bottom: 2rem;
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        ">
            <h2 style="color: #c41e3a; margin: 0;">üéâ C'EST AUJOURD'HUI!</h2>
            <div style="font-size: 2rem; color: #165b33; margin: 1rem 0;">
                Bonne Guignol√©e 2025!
            </div>
        </div>
        """, unsafe_allow_html=True)
    
    # Hero section festif
    st.markdown("""
    <div style="
        text-align: center;
        padding: 3rem 1rem;
        background: linear-gradient(135deg, rgba(196,30,58,0.1), rgba(22,91,51,0.1));
        border-radius: 20px;
        margin-bottom: 2rem;
    ">
        <h1 style="font-size: 3rem; margin: 0;">üéÑ Bienvenue sur Guigno-Map üéÑ</h1>
        <p style="font-size: 1.3rem; color: #666; margin: 1rem 0;">
            Votre plateforme digitale pour la Guignol√©e 2025
        </p>
        <p style="color: #888;">
            G√©rez efficacement votre collecte de denr√©es avec une interface moderne
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Stats visuelles am√©lior√©es
    stats = db.extended_stats()
    progress = (stats['done'] / stats['total'] * 100) if stats['total'] > 0 else 0
    
    st.markdown("### üìä √âtat de la collecte en temps r√©el")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #22c55e, #16a34a);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(34,197,94,0.3);
        ">
            <div style="font-size: 3rem;">üèòÔ∏è</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{stats['total']}</div>
            <div>Total Rues</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(59,130,246,0.3);
        ">
            <div style="font-size: 3rem;">‚úÖ</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{stats['done']}</div>
            <div>Compl√©t√©es</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #f59e0b, #d97706);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(245,158,11,0.3);
        ">
            <div style="font-size: 3rem;">üö∂</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{stats.get('partial', 0)}</div>
            <div>En Cours</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        st.markdown(f"""
        <div style="
            text-align: center;
            padding: 2rem;
            background: linear-gradient(135deg, #c41e3a, #165b33);
            border-radius: 15px;
            color: white;
            box-shadow: 0 4px 15px rgba(196,30,58,0.3);
        ">
            <div style="font-size: 3rem;">üéØ</div>
            <div style="font-size: 2.5rem; font-weight: bold;">{progress:.0f}%</div>
            <div>Progression</div>
        </div>
        """, unsafe_allow_html=True)
    
    # Barre de progression globale
    st.markdown("### üéÑ Progression globale de la collecte")
    st.progress(progress / 100)
    
    # Carte festive
    st.markdown("### üó∫Ô∏è Vue d'ensemble de Mascouche")
    df_all = db.list_streets()
    if not df_all.empty:  # Liste non vide
        m = create_map(df_all, geo)
        st_folium(m, height=750, width=None, returned_objects=[])
    
    # CSS pour r√©duire l'espace apr√®s la carte
    st.markdown("""
    <style>
    div[data-testid="stVerticalBlock"] > div:has(iframe) {
        margin-bottom: 0 !important;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Call to action
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem;
        background: linear-gradient(135deg, rgba(255,215,0,0.1), rgba(255,215,0,0.05));
        border: 2px solid rgba(255,215,0,0.3);
        border-radius: 15px;
        margin-top: 1rem;
    ">
        <h3>üéÖ Pr√™t √† participer ?</h3>
        <p>Choisissez votre r√¥le dans le menu de gauche pour commencer</p>
        <p style="font-size: 0.9rem; color: #666;">
            B√©n√©voles : Acc√©dez √† vos rues assign√©es<br>
            Gestionnaires : Supervisez toute la collecte
        </p>
    </div>
    """, unsafe_allow_html=True)

def page_benevole(geo):
    """Interface b√©n√©vole moderne avec vue limit√©e"""
    
    if not st.session_state.auth or st.session_state.auth.get("role") != "volunteer":
        render_login_card("benevole")
        return
    
    team_id = st.session_state.auth["team_id"]
    
    # Header d'√©quipe personnalis√©
    st.markdown(f"""
    <div style="
        background: linear-gradient(135deg, #165b33, #c41e3a);
        padding: 1.5rem;
        border-radius: 15px;
        margin-bottom: 2rem;
        text-align: center;
    ">
        <h2 style="color: white; margin: 0;">üéÖ √âquipe {team_id}</h2>
        <p style="color: #FFD700; margin: 0.5rem 0 0 0;">Bonne collecte!</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Stats de l'√©quipe
    df_team = db.list_streets(team=team_id)
    if df_team.empty:  # Liste vide
        st.warning("Aucune rue assign√©e. Contactez votre superviseur.")
        return
    
    done = len(df_team[df_team['status'] == 'terminee'])
    total = len(df_team)
    progress = (done / total * 100) if total > 0 else 0
    
    # Mini dashboard √©quipe
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üìç Vos rues", total)
    with col2:
        st.metric("‚úÖ Compl√©t√©es", done)
    with col3:
        st.metric("üéØ Progression", f"{progress:.0f}%")
    
    # Syst√®me de badges
    show_team_badges(team_id)
    
    # Barre de progression
    st.progress(progress / 100)
    
    # Tabs modernis√©s
    tab1, tab2, tab3 = st.tabs(["üó∫Ô∏è Ma carte", "üìù Collecte", "üìä Historique"])
    
    with tab1:
        # CARTE LIMIT√âE AUX RUES DE L'√âQUIPE
        st.markdown("### Vos rues assign√©es")
        
        # Cr√©er une carte avec SEULEMENT les rues de l'√©quipe
        m = folium.Map(
            location=[45.7475, -73.6005],
            zoom_start=14,
            tiles='https://{s}.basemaps.cartocdn.com/rastertiles/voyager/{z}/{x}/{y}.png',
            attr='¬© CARTO'
        )
        
        # Filtrer geo pour n'afficher QUE les rues de l'√©quipe
        team_streets = df_team['name'].tolist()
        
        for street_name in team_streets:
            if street_name in geo:
                status = df_team[df_team['name'] == street_name]['status'].iloc[0]
                
                # Couleurs selon statut
                colors = {
                    'terminee': '#22c55e',
                    'en_cours': '#f59e0b',
                    'a_faire': '#ef4444'
                }
                color = colors.get(status, '#ef4444')
                
                # Ajouter les segments de cette rue
                for path in geo[street_name]:
                    if path and len(path) >= 2:
                        folium.PolyLine(
                            path,
                            color=color,
                            weight=8,  # Plus √©pais pour mobile
                            opacity=0.9,
                            tooltip=f"{street_name} - {status.replace('_', ' ').title()}"
                        ).add_to(m)
        
        # Centrer sur les rues de l'√©quipe
        if team_streets and team_streets[0] in geo:
            first_street = geo[team_streets[0]][0]
            if first_street:
                m.location = first_street[0]
        
        st_folium(m, height=650, width=None, returned_objects=[])
    
    with tab2:
        st.markdown("### üìã Checklist de collecte")
        
        # Liste interactive des rues
        for _, row in df_team.iterrows():
            name = str(row.get('name', '')) if pd.notna(row.get('name', '')) else ''
            status = row.get('status', 'a_faire')
            status = status if pd.notna(status) else 'a_faire'
            notes = str(row.get('notes', '0')) if pd.notna(row.get('notes', '0')) else '0'
            
            # Carte de rue stylis√©e
            status_emoji = {'terminee': '‚úÖ', 'en_cours': 'üö∂', 'a_faire': '‚≠ï'}
            status_color = {'terminee': '#22c55e', 'en_cours': '#f59e0b', 'a_faire': '#ef4444'}
            
            with st.expander(f"{status_emoji.get(status, '‚≠ï')} **{name}** ({notes} notes)"):
                
                # Changement rapide de statut
                col1, col2, col3 = st.columns(3)
                with col1:
                    if st.button("‚≠ï √Ä faire", key=f"todo_{name}", width="stretch"):
                        db.set_status(name, 'a_faire')
                        st.rerun()
                with col2:
                    if st.button("üö∂ En cours", key=f"progress_{name}", width="stretch"):
                        db.set_status(name, 'en_cours')
                        st.rerun()
                with col3:
                    if st.button("‚úÖ Termin√©e", key=f"done_{name}", width="stretch"):
                        db.set_status(name, 'terminee')
                        st.rerun()
                
                st.markdown("---")
                
                # Ajout de note rapide
                st.markdown("**Ajouter une note:**")
                with st.form(f"note_{name}", clear_on_submit=True):
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        num = st.text_input("N¬∞", placeholder="123")
                    with col2:
                        note = st.text_input("Note", placeholder="Personne absente")
                    
                    if st.form_submit_button("‚ûï Ajouter"):
                        if num and note:
                            db.add_note_for_address(name, team_id, num, note)
                            st.success("Note ajout√©e!")
                            st.rerun()
                
                # Notes existantes
                notes_list = db.get_street_addresses_with_notes(name)
                if notes_list:  # Liste non vide
                    st.markdown("**Notes existantes:**")
                    for n in notes_list:
                        st.markdown(f"‚Ä¢ **{n['address_number']}** : {n['comment']}")
    
    with tab3:
        st.markdown("### üìä Votre historique")
        try:
            notes = db.get_team_notes(team_id)
            if notes:  # Liste non vide
                st.dataframe(to_dataframe(notes), use_container_width=True)
            else:
                st.info("Aucune note encore")
        except:
            st.info("Historique non disponible")

def page_benevole_v2(geo):
    """Interface b√©n√©vole moderne v4.1 avec vue 'Mes rues'"""
    
    # V√©rifier l'authentification
    if not st.session_state.auth or st.session_state.auth.get("role") != "volunteer":
        # Afficher la page de connexion b√©n√©vole
        return page_benevole(geo)
    
    # Interface b√©n√©vole connect√© avec tabs
    st.header("üéÖ Espace B√©n√©vole")
    team_id = st.session_state.auth.get("team", "√âquipe inconnue")
    st.markdown(f"**√âquipe:** {team_id}")
    
    # Tabs pour b√©n√©voles
    tabs = st.tabs([
        "üèòÔ∏è Mes rues",
        "üó∫Ô∏è Carte de terrain", 
        "üìù Journal d'activit√©"
    ])
    
    with tabs[0]:
        # Nouvelle vue "Mes rues" v4.1
        page_benevole_mes_rues()
    
    with tabs[1]:
        # Carte traditionnelle (r√©utilise l'ancienne interface)
        page_benevole(geo)
    
    with tabs[2]:
        # Journal d'activit√© de l'√©quipe
        st.markdown("### üìù Journal d'activit√© de votre √©quipe")
        try:
            # Afficher les activit√©s r√©centes de l'√©quipe
            from db_v5 import recent_activity
            activities = recent_activity(20)
            
            if activities:
                team_activities = [a for a in activities if a.get('team_id') == team_id]
                for activity in team_activities:
                    action = activity.get('action', '')
                    details = activity.get('details', '')
                    created_at = activity.get('created_at', '')
                    st.markdown(f"**{created_at}** - {action}: {details}")
            else:
                st.info("Aucune activit√© enregistr√©e pour votre √©quipe")
                
        except Exception as e:
            st.info("Journal d'activit√© temporairement indisponible")
            st.caption(f"Erreur: {e}")

def page_gestionnaire_v2(geo):
    """Interface gestionnaire moderne (ancien superviseur)"""
    st.header("üëî Tableau de Bord Gestionnaire")
    
    # V√©rifier l'authentification
    if not st.session_state.auth or st.session_state.auth.get("role") != "supervisor":
        render_login_card("gestionnaire")
        return
    
    # Dashboard moderne
    render_dashboard_gestionnaire(geo)
    
    # Tabs
    tabs = st.tabs([
        "üìä Vue d'ensemble",
        "üë• √âquipes",
        "üó∫Ô∏è Assignation",
        "üì• Export",
        "üõ† Tech"
    ])
    
    with tabs[0]:
        # Carte g√©n√©rale
        st.markdown("### Carte g√©n√©rale")
        df_all = db.list_streets()
        if not df_all.empty:  # Liste non vide
            m = create_map(df_all, geo)
            st_folium(m, height=800, width=None, returned_objects=[])
        
        # Activit√© r√©cente
        st.markdown("### Activit√© r√©cente")
        try:
            recent = db.recent_activity(limit=10)
            if recent:  # Liste non vide
                st.dataframe(to_dataframe(recent), use_container_width=True)
            else:
                st.info("Aucune activit√© r√©cente")
        except:
            st.info("Historique d'activit√© non disponible")
    
    with tabs[1]:
        # Gestion des √©quipes
        st.subheader("üë• Gestion des √©quipes", anchor=False)
        
        # === Formulaire de cr√©ation d'√©quipe (robuste) ===
        with st.expander("‚ûï Cr√©er une nouvelle √©quipe", expanded=False):
            with st.form("create_team_form", clear_on_submit=True):
                team_id_in = st.text_input(
                    "Identifiant d'√©quipe", 
                    key="new_team_id", 
                    placeholder="Ex: EQUIPE1",
                    help="Lettres et chiffres uniquement, max 20 caract√®res"
                )
                team_name_in = st.text_input(
                    "Nom d'√©quipe", 
                    key="new_team_name", 
                    placeholder="Ex: √âquipe Centre",
                    help="Nom descriptif de l'√©quipe"
                )
                
                # Toggle pour afficher/masquer les mots de passe
                show_pw = st.checkbox("Afficher les mots de passe", value=False)
                pw_type = "default" if show_pw else "password"
                
                pwd_in = st.text_input(
                    "Mot de passe", 
                    type=pw_type, 
                    key="new_team_pwd", 
                    placeholder="Minimum 4 caract√®res",
                    help="Tout caract√®re accept√©, min 4 / max 128"
                )
                pwd_conf = st.text_input(
                    "Confirmer le mot de passe", 
                    type=pw_type, 
                    key="new_team_pwd_conf", 
                    placeholder="Retapez le mot de passe",
                    help="Doit correspondre au mot de passe ci-dessus"
                )
                
                submitted = st.form_submit_button("‚úÖ Cr√©er l'√©quipe", width="stretch")

            if submitted:
                # Validation avec validators.py
                ok_id, team_id = validate_and_clean_input("team_id", team_id_in)
                ok_name, team_name = validate_and_clean_input("text", team_name_in)
                ok_pw, password = validate_and_clean_input("password", pwd_in)
                
                if not ok_id:
                    st.error("‚ùå Identifiant d'√©quipe invalide (lettres/chiffres, max 20)")
                elif not ok_name:
                    st.error("‚ùå Nom d'√©quipe invalide ou vide")
                elif not ok_pw:
                    st.error("‚ùå Mot de passe invalide (minimum 4 caract√®res)")
                elif pwd_in != pwd_conf:
                    st.error("‚ùå Les mots de passe ne correspondent pas")
                else:
                    # Tentative de cr√©ation avec db.create_team
                    try:
                        created = db.create_team(team_id, team_name, password)
                        if created:
                            st.toast(f"‚úÖ √âquipe {team_id} cr√©√©e avec succ√®s", icon="‚úÖ")
                            st.rerun()
                        else:
                            st.error("‚ùå √âchec de cr√©ation (ID d√©j√† existant ?)")
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de la cr√©ation: {e}")
        
        # === Liste des √©quipes (sans doublon de titre) ===
        try:
            teams_df = db.get_all_teams()
            if teams_df:  # Liste non vide
                st.dataframe(to_dataframe(teams_df), use_container_width=True)
            else:
                st.info("Aucune √©quipe cr√©√©e")
        except Exception as e:
            st.info("Liste des √©quipes non disponible")
    
    with tabs[2]:
        # Assignation v4.1
        page_assignations_v41()
    
    with tabs[3]:
        # Export am√©lior√© v4.1
        page_export_gestionnaire_v41()

    with tabs[4]:
        st.markdown("### üõ† Op√©rations techniques (prot√©g√©es)")

        # -- PIN stock√© dans secrets (config.toml -> [secrets] TECH_PIN="xxxx")
        try:
            TECH_PIN = st.secrets.get("TECH_PIN", "")
        except:
            TECH_PIN = ""  # Pas de fichier secrets.toml

        if "tech_ok" not in st.session_state:
            st.session_state.tech_ok = False

        if not st.session_state.tech_ok:
            pin = st.text_input("Entrer le PIN technique", type="password")
            if st.button("D√©verrouiller"):
                if TECH_PIN and pin == TECH_PIN:
                    st.session_state.tech_ok = True
                    st.success("Acc√®s technique d√©verrouill√©.")
                    st.rerun()
                else:
                    st.error("PIN invalide.")
            st.stop()

        st.info("‚ö†Ô∏è Ces actions sont lourdes et n'affectent pas les statuts/notes. Elles r√©g√©n√®rent les caches OSM.")

        # --- Reconstruire le cache g√©om√©trique (lourd)
        with st.expander("üîÑ Reconstruire cache OSM (g√©om√©tries)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirm = st.checkbox("Je comprends les implications")
            with col2:
                safety = st.text_input('√âcrire "REBUILD" pour confirmer')

            if st.button("Lancer la reconstruction"):
                if confirm and safety.strip().upper() == "REBUILD":
                    with st.spinner("Construction du cache‚Ä¶"):
                        build_geometry_cache()       # reconstruit le fichier osm_cache.json
                        st.cache_data.clear()        # purge cache Streamlit
                    st.success("‚úÖ Cache OSM mis √† jour (g√©om√©tries).")
                    st.rerun()
                else:
                    st.warning("Confirmation incompl√®te.")

        # --- Reconstruire/Importer le cache des adresses
        with st.expander("üìç Mettre √† jour les adresses (OSM)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirmA = st.checkbox("Je confirme")
            with col2:
                safetyA = st.text_input('√âcrire "IMPORT" pour confirmer')

            if st.button("Lancer la mise √† jour des adresses"):
                if confirmA and safetyA.strip().upper() == "IMPORT":
                    with st.spinner("T√©l√©chargement des adresses OSM‚Ä¶"):
                        build_addresses_cache()
                        addr_cache = load_addresses_cache()
                        count = db.import_addresses_from_cache(addr_cache)
                    st.success(f"‚úÖ {count} adresses import√©es depuis OSM.")
                    st.rerun()
                else:
                    st.warning("Confirmation incompl√®te.")

        # --- Gestion des backups
        with st.expander("üíæ Gestion des backups", expanded=False):
            backup_mgr = db.get_backup_manager()  # Sans DB_PATH, utilise config SQLAlchemy
            
            col1, col2 = st.columns([2, 1])
            with col1:
                if st.button("üîÑ Cr√©er un backup manuel", width="stretch"):
                    backup_file = backup_mgr.create_backup("manual")
                    if backup_file:
                        st.success(f"Backup cr√©√© : {Path(backup_file).name}")
            
            with col2:
                if st.button("üìã Voir les backups", width="stretch"):
                    backups = backup_mgr.list_backups()
                    if backups:
                        for backup in backups[:5]:  # Montrer les 5 derniers
                            st.text(f"‚Ä¢ {backup['name']} ({backup['size']})")
                    else:
                        st.info("Aucun backup disponible")

def page_superviseur(conn, geo):
    """Interface superviseur"""
    st.header("üéØ Tableau de Bord Superviseur")
    
    # V√©rifier l'authentification
    if not st.session_state.auth or st.session_state.auth.get("role") != "supervisor":
        render_login_card("superviseur")
        return
    
    # Dashboard moderne
    render_dashboard_gestionnaire(geo)
    
    # Tabs
    tabs = st.tabs([
        "üìä Vue d'ensemble",
        "üë• √âquipes",
        "üó∫Ô∏è Assignation",
        "üì• Export",
        "üõ† Tech"
    ])
    
    with tabs[0]:
        # Carte g√©n√©rale
        st.markdown("### Carte g√©n√©rale")
        df_all = db.list_streets()
        if not df_all.empty:  # Liste non vide
            m = create_map(df_all, geo)
            st_folium(m, height=800, width=None, returned_objects=[])
        
        # Activit√© r√©cente
        st.markdown("### Activit√© r√©cente")
        recent = db.recent_activity(limit=10)
        if recent:  # Liste non vide
            st.dataframe(to_dataframe(recent), use_container_width=True)
    
    with tabs[1]:
        # Gestion des √©quipes
        st.markdown("### Gestion des √©quipes")
        
        with st.expander("Cr√©er une √©quipe"):
            with st.form("new_team", clear_on_submit=True):
                new_id = st.text_input("Identifiant")
                new_name = st.text_input("√âquipe")
                new_pass = st.text_input("Mot de passe", type="password")
                
                if st.form_submit_button("Cr√©er"):
                    if all([new_id, new_name, new_pass]):
                        if db.create_team(new_id, new_name, new_pass):
                            st.success(f"√âquipe {new_id} cr√©√©e")
                            st.rerun()
        
        # Liste des √©quipes
        teams_df = db.get_all_teams()
        if teams_df:  # Liste non vide
            st.dataframe(to_dataframe(teams_df), use_container_width=True)
    
    with tabs[2]:
        # Assignation
        st.markdown("### Assignation des rues")
        
        unassigned = db.get_unassigned_streets()
        
        if unassigned:  # Liste non vide
            with st.form("assign"):
                team = st.selectbox("√âquipe", db.teams())
                streets = st.multiselect("Rues", unassigned)
                
                if st.form_submit_button("Assigner"):
                    if team and streets:
                        db.assign_streets_to_team(streets, team)
                        st.success("Rues assign√©es!")
                        st.rerun()
        else:
            st.success("Toutes les rues sont assign√©es!")
        
        # Tableau des assignations
        df_all = db.list_streets()
        if not df_all.empty:  # Liste non vide
            st.dataframe(
                df_all[['name', 'sector', 'team', 'status']],
                use_container_width=True
            )
    
    with tabs[3]:
        # Export
        st.markdown("### Export des donn√©es")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.download_button(
                "üì• Export rues (CSV)",
                db.export_to_csv(),
                "rapport_rues.csv",
                "text/csv",
                width="stretch"
            )
        
        with col2:
            st.download_button(
                "üì• Export notes (CSV)",
                db.export_notes_csv(),
                "rapport_notes.csv",
                "text/csv",
                width="stretch"
            )

    with tabs[4]:
        st.markdown("### üõ† Op√©rations techniques (prot√©g√©es)")

        # -- PIN stock√© dans secrets (config.toml -> [secrets] TECH_PIN="xxxx")  
        try:
            TECH_PIN = st.secrets.get("TECH_PIN", "")
        except:
            TECH_PIN = ""  # Pas de fichier secrets.toml

        if "tech_ok" not in st.session_state:
            st.session_state.tech_ok = False

        if not st.session_state.tech_ok:
            pin = st.text_input("Entrer le PIN technique", type="password")
            if st.button("D√©verrouiller"):
                if TECH_PIN and pin == TECH_PIN:
                    st.session_state.tech_ok = True
                    st.success("Acc√®s technique d√©verrouill√©.")
                    st.rerun()
                else:
                    st.error("PIN invalide.")
            st.stop()

        st.info("‚ö†Ô∏è Ces actions sont lourdes et n'affectent pas les statuts/notes. Elles r√©g√©n√®rent les caches OSM.")

        # --- Reconstruire le cache g√©om√©trique (lourd)
        with st.expander("üîÑ Reconstruire cache OSM (g√©om√©tries)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirm = st.checkbox("Je comprends les implications")
            with col2:
                safety = st.text_input('√âcrire "REBUILD" pour confirmer')

            if st.button("Lancer la reconstruction"):
                if confirm and safety.strip().upper() == "REBUILD":
                    with st.spinner("Construction du cache‚Ä¶"):
                        build_geometry_cache()       # reconstruit le fichier osm_cache.json
                        st.cache_data.clear()        # purge cache Streamlit
                    st.success("‚úÖ Cache OSM mis √† jour (g√©om√©tries).")
                    st.rerun()
                else:
                    st.warning("Confirmation incompl√®te.")

        # --- Reconstruire/Importer le cache des adresses
        with st.expander("üìç Mettre √† jour les adresses (OSM)", expanded=False):
            col1, col2 = st.columns([1,2])
            with col1:
                confirmA = st.checkbox("Je confirme")
            with col2:
                safetyA = st.text_input('√âcrire "IMPORT" pour confirmer')

            if st.button("Lancer la mise √† jour des adresses"):
                if confirmA and safetyA.strip().upper() == "IMPORT":
                    with st.spinner("T√©l√©chargement des adresses OSM‚Ä¶"):
                        build_addresses_cache()
                        addr_cache = load_addresses_cache()
                        count = db.import_addresses_from_cache(addr_cache)
                    st.success(f"‚úÖ {count} adresses import√©es depuis OSM.")
                    st.rerun()
                else:
                    st.warning("Confirmation incompl√®te.")

# ============================================
# MAIN
# ============================================

# ================================================================================
# NOUVELLES FONCTIONS v4.1 - SUPERVISEUR ET B√âN√âVOLE
# ================================================================================

def page_assignations_v41():
    """Panneau d'assignations v4.1 pour superviseurs"""
    
    try:
        # ===== Bloc Assignations (refactor propre) =====
        st.subheader("üó∫Ô∏è Assignations par secteur", anchor=False)
        
        # Compteur de rues non assign√©es (banni√®re info)
        unassigned_count = db.get_unassigned_streets_count()
        if unassigned_count > 0:
            st.info(f"‚ö†Ô∏è {unassigned_count} rue(s) non assign√©e(s)")
        
        with st.container():
            c1, c2, c3 = st.columns([1, 1.2, 0.7], vertical_alignment="bottom")
            
            with c1:
                # R√©cup√©rer la liste des secteurs
                liste_secteurs = db.get_sectors_list()
                secteur = st.selectbox(
                    "SECTEUR √Ä ASSIGNER",
                    options=[""] + (liste_secteurs if liste_secteurs else []),
                    index=0,
                    key="assign_sector",
                    help="Choisissez le secteur √† assigner",
                    label_visibility="visible",
                )
            
            with c2:
                # R√©cup√©rer la liste des √©quipes
                teams = db.get_teams_list()
                liste_equipes = [f"{team[1]} ({team[0]})" for team in teams] if teams else []
                
                if liste_equipes:
                    team_display = st.selectbox(
                        "√âQUIPE", 
                        options=[""] + liste_equipes, 
                        index=0, 
                        key="assign_team"
                    )
                    # Extraire l'ID de l'√©quipe
                    team = ""
                    if team_display and team_display != "":
                        team = team_display.split("(")[-1].rstrip(")")
                else:
                    st.info("Aucune √©quipe disponible")
                    team = None
            
            with c3:
                disabled = not (secteur and team)
                if st.button("üéØ Assigner tout le secteur", width="stretch", disabled=disabled):
                    # Appel m√©tier : assigner toutes les rues non assign√©es du secteur √† l'√©quipe
                    if secteur and team:
                        try:
                            nb = db.bulk_assign_sector(secteur, team)
                            if nb > 0:
                                st.toast(f"‚úÖ {nb} rue(s) assign√©e(s) √† l'√©quipe {team}", icon="‚úÖ")
                                st.rerun()
                            else:
                                st.toast("‚ÑπÔ∏è Aucune rue non assign√©e dans ce secteur", icon="‚ÑπÔ∏è")
                        except Exception as e:
                            st.error(f"Erreur lors de l'assignation: {e}")
        
        # ===== Tableau d'√©tat (uniforme, sans style sp√©cial) =====
        st.markdown("### üìã √âtat des assignations")
        
        df = db.list_streets()
        if not df.empty:  # Liste non vide
            df_disp = df.assign(
                Statut=df["status"].map(STATUS_TO_LABEL).fillna("√Ä faire")
            ).rename(columns={
                "name": "Rue", 
                "sector": "Secteur", 
                "team": "√âquipe"
            })[["Rue", "Secteur", "√âquipe", "Statut"]]
            
            st.dataframe(df_disp, use_container_width=True)  # aucun Styler, aucun CSS cellule
        else:
            st.info("Aucune rue trouv√©e")
            
    except Exception as e:
        st.error(f"Erreur dans le panneau d'assignations: {e}")
        st.info("Fonctionnalit√© temporairement indisponible")

def page_export_gestionnaire_v41():
    """Page d'export v4.1 avec nouvelles fonctionnalit√©s"""
    st.markdown("### üì• Export des donn√©es")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Export CSV standard
        try:
            st.download_button(
                "üì• Export CSV Standard",
                db.export_to_csv(),
                "rapport_rues.csv",
                "text/csv",
                width="stretch"
            )
        except Exception as e:
            st.button("üì• CSV (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    with col2:
        # Export Excel professionnel
        try:
            from reports import ReportGenerator
            generator = ReportGenerator()
            excel_data = generator.generate_excel()
            st.download_button(
                "üìä Export Excel Pro",
                excel_data,
                "guignolee_2025_rapport.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                width="stretch"
            )
        except ImportError:
            st.button("üìä Excel (Installer xlsxwriter)", disabled=True, width="stretch")
        except Exception as e:
            st.button("üìä Excel (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    with col3:
        # Export PDF professionnel
        try:
            from reports import ReportGenerator
            generator = ReportGenerator()
            pdf_data = generator.generate_pdf()
            st.download_button(
                "üìÑ Export PDF Pro",
                pdf_data,
                "guignolee_2025_rapport.pdf",
                "application/pdf",
                width="stretch"
            )
        except ImportError:
            st.button("üìÑ PDF (Installer reportlab)", disabled=True, width="stretch")
        except Exception as e:
            st.button("üìÑ PDF (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    # Export CSV assignations (nouveau v4.1)
    st.markdown("---")
    st.markdown("### üìã Export sp√©cialis√©s v4.1")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        # Export CSV assignations
        try:
            assignations_data = db.get_assignations_export_data()
            if assignations_data:  # Liste non vide
                csv_data = pd.DataFrame(assignations_data).to_csv(index=False, encoding='utf-8')
                st.download_button(
                    "üìã Export CSV Assignations",
                    csv_data,
                    "assignations_secteurs.csv",
                    "text/csv",
                    width="stretch",
                    help="Colonnes: secteur, rue, √©quipe, statut"
                )
            else:
                st.button("üìã Assignations (Aucune donn√©e)", disabled=True, width="stretch")
        except Exception as e:
            st.button("üìã Assignations (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")
    
    with col2:
        # Export notes
        try:
            st.download_button(
                "üìù Export Notes",
                db.export_notes_csv(),
                "rapport_notes.csv",
                "text/csv",
                width="stretch"
            )
        except Exception as e:
            st.button("üìù Notes (Erreur)", disabled=True, width="stretch")
            st.caption(f"Erreur: {e}")

def page_benevole_mes_rues():
    """Vue 'Mes rues' pour b√©n√©voles v4.1"""
    
    # R√©cup√©rer l'√©quipe du b√©n√©vole connect√©
    if not st.session_state.auth or st.session_state.auth.get("role") != "volunteer":
        st.warning("Acc√®s r√©serv√© aux b√©n√©voles connect√©s")
        return
    
    team_id = st.session_state.auth.get("team")
    if not team_id:
        st.error("√âquipe non identifi√©e")
        return
    
    st.markdown(f"### üèòÔ∏è Mes rues assign√©es - √âquipe {team_id}")
    
    try:
        # R√©cup√©rer les rues de l'√©quipe
        team_streets = db.get_team_streets(team_id)
        
        if not team_streets:  # Liste vide
            st.info("Aucune rue assign√©e √† votre √©quipe pour le moment.")
            return
        
        # Afficher les statistiques de l'√©quipe
        total_streets = len(team_streets)
        done_streets = len([s for s in team_streets if s.get('status') == 'terminee'])
        in_progress = len([s for s in team_streets if s.get('status') == 'en_cours'])
        
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Total rues", total_streets)
        with col2:
            st.metric("Termin√©es", done_streets)
        with col3:
            st.metric("En cours", in_progress)
        with col4:
            progress = (done_streets / total_streets * 100) if total_streets > 0 else 0
            st.metric("Progression", f"{progress:.1f}%")
        
        st.markdown("---")
        
        # Affichage par rue avec actions
        for street in team_streets:
            if isinstance(street, str):
                street_name = street
            else:
                street_name = street.get("name", street)
            street_name = street['street_name']
            current_status = street['status']
            notes_count = street['notes_count']
            
            with st.expander(f"üèòÔ∏è {street_name} ({street['sector']}) - {current_status.replace('_', ' ').title()}", 
                           expanded=current_status == 'en_cours'):
                
                col1, col2, col3 = st.columns([2, 1, 1])
                
                with col1:
                    st.markdown(f"**Secteur:** {street['sector']}")
                    st.markdown(f"**Statut actuel:** {current_status.replace('_', ' ').title()}")
                    if notes_count > 0:
                        st.markdown(f"**Notes existantes:** {notes_count}")
                
                with col2:
                    # Bouton "En cours"
                    if st.button(
                        "üöÄ En cours", 
                        key=f"progress_{street_name}",
                        disabled=current_status == 'en_cours',
                        width="stretch"
                    ):
                        if db.update_street_status(street_name, 'en_cours', team_id):
                            st.toast(f"‚úÖ {street_name} marqu√©e en cours", icon="üöÄ")
                            st.rerun()
                        else:
                            st.error("Erreur lors de la mise √† jour")
                
                with col3:
                    # Bouton "Termin√©e"
                    if st.button(
                        "‚úÖ Termin√©e", 
                        key=f"done_{street_name}",
                        disabled=current_status == 'terminee',
                        width="stretch"
                    ):
                        if db.update_street_status(street_name, 'terminee', team_id):
                            st.toast(f"üéâ {street_name} termin√©e!", icon="üéâ")
                            st.rerun()
                        else:
                            st.error("Erreur lors de la mise √† jour")
                
                # Section notes
                st.markdown("**Gestion des notes:**")
                
                # Afficher les notes existantes
                existing_notes = db.get_street_notes_for_team(street_name, team_id)
                if existing_notes:
                    st.markdown("*Notes existantes:*")
                    for note in existing_notes:
                        st.markdown(f"‚Ä¢ **#{list(note.values())[0] if isinstance(note, dict) else note[0]}** : {list(note.values())[1] if isinstance(note, dict) else note[1]} _{list(note.values())[2] if isinstance(note, dict) else note[2]}_")
                
                # Ajouter une nouvelle note
                with st.form(f"note_form_{street_name}"):
                    col_addr, col_note = st.columns([1, 3])
                    with col_addr:
                        address_number = st.text_input(
                            "N¬∞ civique", 
                            key=f"addr_{street_name}",
                            placeholder="123A"
                        )
                    with col_note:
                        comment = st.text_area(
                            "Commentaire", 
                            key=f"comment_{street_name}",
                            placeholder="Ex: Absent, refus, don re√ßu...",
                            max_chars=500,
                            height=80
                        )
                    
                    if st.form_submit_button("üíæ Enregistrer note"):
                        if address_number and comment:
                            if db.add_street_note(street_name, team_id, address_number, comment):
                                st.toast(f"üìù Note ajout√©e pour {street_name} #{address_number}", icon="üìù")
                                st.rerun()
                            else:
                                st.error("Erreur lors de l'enregistrement de la note")
                        else:
                            st.warning("Veuillez remplir le num√©ro et le commentaire")
                            
    except Exception as e:
        st.error(f"Erreur lors du chargement de vos rues: {e}")
        st.info("Fonctionnalit√© temporairement indisponible")

def main():
    """Point d'entr√©e principal - Version 2.0 Guignol√©e"""
    
    # CSS moderne
    inject_css()
    
    # Connexion DB
    # Initialisation de la base de donn√©es
    db.init_db()
    
    # Compatibilit√© legacy supprim√©e - utilise SQLAlchemy via src.database
    # Connexion centralis√©e via get_session() au lieu de sqlite3 direct
    
    # Cache g√©om√©trique
    @st.cache_data(ttl=None)
    def get_geo(_sig):
        data = load_geometry_cache()
        return data if data else {}
    
    sig = int(CACHE_FILE.stat().st_mtime_ns) if CACHE_FILE.exists() else 0
    geo = get_geo(sig)
    
    # Header festif
    render_header()
    
    # Navigation modernis√©e dans la sidebar
    with st.sidebar:
        # CSS pour la sidebar sans position absolue
        st.markdown("""
        <style>
        .css-1d391kg { padding-top: 1rem !important; }
        .stSidebar > div:first-child { padding-top: 1rem !important; }
        </style>
        """, unsafe_allow_html=True)
        
        # Logo en haut de la sidebar (position normale)
        logo_path = ASSETS / "logo.png"
        if logo_path.exists():
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                st.image(str(logo_path), width=150)
            st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
        else:
            # Placeholder centr√©
            st.markdown("""
            <div style="
                background: linear-gradient(135deg, #c41e3a, #165b33);
                border-radius: 15px;
                padding: 2rem;
                color: white;
                text-align: center;
                margin: 1rem 0;
                box-shadow: 0 4px 15px rgba(0,0,0,0.3);
            ">
                <div style="font-size: 2.5rem;">üéÅ</div>
                <div style="font-weight: bold; font-size: 1.2rem;">LOGO</div>
                <small>Espace r√©serv√©</small>
            </div>
            """, unsafe_allow_html=True)
            st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
        
        # Navigation
        st.markdown("### üéÑ Navigation")
        
        # Boutons de navigation stylis√©s
        if st.button("üè† Accueil", width="stretch"):
            st.session_state.page = "accueil"
            st.rerun()
        
        if st.button("üéÖ B√©n√©vole", width="stretch"):
            st.session_state.page = "benevole"
            st.rerun()
            
        if st.button("üëî Gestionnaire", width="stretch"):
            st.session_state.page = "gestionnaire"  
            st.rerun()
        
        # D√©connexion si connect√©
        if st.session_state.auth:
            st.markdown("---")
            if st.button("üö™ D√©connexion", width="stretch"):
                st.session_state.auth = None
                st.rerun()
        
        # Compteur temps r√©el
        st.markdown("---")
        stats = db.extended_stats()
        st.markdown(f"""
        <div style="text-align: center;">
            <h4>√âtat de la collecte</h4>
            <div style="font-size: 2rem; color: #FFD700;">
                {stats['done']}/{stats['total']}
            </div>
            <small>Rues compl√©t√©es</small>
        </div>
        """, unsafe_allow_html=True)
    
    # Routing pages
    page = st.session_state.get('page', 'accueil')
    
    if page == "accueil":
        page_accueil_v2(geo)
    elif page == "benevole":
        page_benevole_v2(geo)
    elif page == "gestionnaire":
        page_gestionnaire_v2(geo)
    
    # Footer festif
    st.markdown("""
    <div style="
        text-align: center;
        padding: 2rem;
        margin-top: 3rem;
        border-top: 2px solid rgba(255,215,0,0.3);
        color: #8b92a4;
    ">
        <p>
            üéÑ Guignol√©e 2025 - Le Relais de Mascouche üéÑ<br>
            <small>Ensemble, redonnons espoir | üìû 450-474-4133</small>
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Banni√®re en bas de page
    if (ASSETS / "banner.png").exists():
        st.image(str(ASSETS / "banner.png"))

if __name__ == "__main__":
    main()

# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/app.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/backup.py
# --- encoding: utf-8 | size: 5616 bytes | sha256: 1e920bbdbea93ec5bf121cae8494d89245fa8f29e07deca6dfd9d193f3dae250 | mtime: 2025-09-14T15:05:37-04:00
# --------------------------------------------------------------------------------
"""
Syst√®me de backup automatique pour GuignoMap
Sauvegarde la base de donn√©es et les caches
"""

import shutil
import sqlite3
from pathlib import Path
from datetime import datetime
import json
import zipfile

class BackupManager:
    def __init__(self, db_path):
        self.db_path = Path(db_path)
        self.backup_dir = self.db_path.parent / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        self.max_backups = 7  # Garder 7 jours de backups
        
    def create_backup(self, reason="manual"):
        """Cr√©e un backup complet avec timestamp"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"backup_{timestamp}_{reason}"
        backup_path = self.backup_dir / backup_name
        backup_path.mkdir(exist_ok=True)
        
        try:
            # Backup de la base de donn√©es
            db_backup = backup_path / "guigno_map.db"
            shutil.copy2(self.db_path, db_backup)
            
            # Backup des caches OSM
            for cache_file in ["osm_cache.json", "osm_addresses.json"]:
                cache_path = self.db_path.parent / cache_file
                if cache_path.exists():
                    shutil.copy2(cache_path, backup_path / cache_file)
            
            # Cr√©er un ZIP
            zip_path = self.backup_dir / f"{backup_name}.zip"
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                for file in backup_path.iterdir():
                    zipf.write(file, file.name)
            
            # Nettoyer le dossier temporaire
            shutil.rmtree(backup_path)
            
            # Nettoyer les vieux backups
            self._cleanup_old_backups()
            
            # Log le backup
            self._log_backup(timestamp, reason)
            
            print(f"‚úÖ Backup cr√©√© : {zip_path.name}")
            return str(zip_path)
            
        except Exception as e:
            print(f"‚ùå Erreur backup : {e}")
            if backup_path.exists():
                shutil.rmtree(backup_path)
            return None
    
    def restore_backup(self, backup_file):
        """Restaure un backup sp√©cifique"""
        backup_path = self.backup_dir / backup_file
        if not backup_path.exists():
            print(f"‚ùå Backup introuvable : {backup_file}")
            return False
            
        try:
            # Cr√©er un backup de s√©curit√© avant restauration
            self.create_backup("pre_restore")
            
            # Extraire le ZIP
            temp_dir = self.backup_dir / "temp_restore"
            with zipfile.ZipFile(backup_path, 'r') as zipf:
                zipf.extractall(temp_dir)
            
            # Restaurer les fichiers
            for file in temp_dir.iterdir():
                target = self.db_path.parent / file.name
                shutil.copy2(file, target)
            
            # Nettoyer
            shutil.rmtree(temp_dir)
            
            print(f"‚úÖ Backup restaur√© : {backup_file}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur restauration : {e}")
            return False
    
    def list_backups(self):
        """Liste tous les backups disponibles"""
        backups = []
        for file in self.backup_dir.glob("backup_*.zip"):
            stat = file.stat()
            backups.append({
                "name": file.name,
                "size": f"{stat.st_size / 1024 / 1024:.2f} MB",
                "date": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            })
        return sorted(backups, key=lambda x: x["date"], reverse=True)
    
    def _cleanup_old_backups(self):
        """Supprime les backups de plus de 7 jours"""
        backups = sorted(self.backup_dir.glob("backup_*.zip"), key=lambda x: x.stat().st_mtime)
        while len(backups) > self.max_backups:
            oldest = backups.pop(0)
            oldest.unlink()
            print(f"üóëÔ∏è Ancien backup supprim√© : {oldest.name}")
    
    def _log_backup(self, timestamp, reason):
        """Log les backups dans un fichier"""
        log_file = self.backup_dir / "backup_log.json"
        log = []
        if log_file.exists():
            with open(log_file, 'r') as f:
                log = json.load(f)
        
        log.append({
            "timestamp": timestamp,
            "reason": reason,
            "date": datetime.now().isoformat()
        })
        
        # Garder seulement les 100 derniers logs
        log = log[-100:]
        
        with open(log_file, 'w') as f:
            json.dump(log, f, indent=2)

def auto_backup_before_critical(func):
    """D√©corateur pour backup automatique avant op√©rations critiques"""
    def wrapper(*args, **kwargs):
        # Trouver la connexion DB dans les arguments
        conn = None
        for arg in args:
            if hasattr(arg, 'execute'):  # C'est une connexion SQLite
                conn = arg
                break
        
        if conn:
            try:
                # Cr√©er un backup avant l'op√©ration
                db_path = Path(__file__).parent / "guigno_map.db"
                backup_mgr = BackupManager(db_path)
                backup_mgr.create_backup(f"auto_{func.__name__}")
            except:
                pass  # Ne pas bloquer l'op√©ration si le backup √©choue
        
        return func(*args, **kwargs)
    return wrapper
# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/backup.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/db.py
# --- encoding: utf-8 | size: 33090 bytes | sha256: 4d6c2dd5e7c3f5918852835eb37d38c80ac8b07f8657fd8f1cdd7ef88d6d4790 | mtime: 2025-09-14T19:29:58-04:00
# --------------------------------------------------------------------------------
import sqlite3
import pandas as pd
import hashlib
import bcrypt
from backup import auto_backup_before_critical, BackupManager
from validators import validate_and_clean_input, InputValidator
from datetime import datetime
import json
from pathlib import Path
import os
import secrets
import string

# Sch√©ma am√©lior√© de la base de donn√©es
SCHEMA = """
-- Table des rues
CREATE TABLE IF NOT EXISTS streets (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL UNIQUE,
    sector TEXT,
    team TEXT,
    status TEXT NOT NULL DEFAULT 'a_faire' 
        CHECK (status IN ('a_faire', 'en_cours', 'terminee'))
);

-- Table des √©quipes
CREATE TABLE IF NOT EXISTS teams (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    password_hash TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    active BOOLEAN DEFAULT 1
);

-- Table des notes/commentaires PAR ADRESSE
CREATE TABLE IF NOT EXISTS notes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    street_name TEXT NOT NULL,
    team_id TEXT NOT NULL,
    address_number TEXT,
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (street_name) REFERENCES streets(name),
    FOREIGN KEY (team_id) REFERENCES teams(id)
);

-- Table d'activit√© (log)
CREATE TABLE IF NOT EXISTS activity_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    team_id TEXT,
    action TEXT NOT NULL,
    details TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Table des adresses OSM
CREATE TABLE IF NOT EXISTS addresses (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    street_name TEXT NOT NULL,
    house_number TEXT NOT NULL,
    latitude REAL,
    longitude REAL,
    osm_type TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (street_name) REFERENCES streets(name)
);

-- Index pour am√©liorer les performances
CREATE INDEX IF NOT EXISTS idx_streets_team ON streets(team);
CREATE INDEX IF NOT EXISTS idx_streets_status ON streets(status);
CREATE INDEX IF NOT EXISTS idx_notes_street ON notes(street_name);
CREATE INDEX IF NOT EXISTS idx_activity_created ON activity_log(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_addresses_street ON addresses(street_name);
CREATE INDEX IF NOT EXISTS idx_addresses_number ON addresses(house_number);
"""

def get_conn(db_path):
    """Cr√©e une connexion √† la base de donn√©es"""
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn

def init_db(conn):
    """Initialise la base de donn√©es avec le sch√©ma et les donn√©es initiales"""
    try:
        # Cr√©er les tables si elles n'existent pas
        conn.executescript(SCHEMA)
        conn.commit()
        
        # Cr√©er un compte admin par d√©faut s'il n'existe pas
        cursor = conn.execute("SELECT COUNT(*) FROM teams WHERE id = 'ADMIN'")
        if cursor.fetchone()[0] == 0:
            pwd = os.getenv("GM_ADMIN_PWD", "RELAIS2025")  # Par d√©faut RELAIS2025
            create_team(conn, 'ADMIN', 'Superviseur', pwd)
        
        # AUTO-IMPORT : Si aucune rue n'existe, importer automatiquement depuis OSM
        cursor = conn.execute("SELECT COUNT(*) FROM streets")
        if cursor.fetchone()[0] == 0:
            print("üîÑ Aucune rue trouv√©e. Import automatique depuis OpenStreetMap...")
            auto_import_streets(conn)
            
    except Exception as e:
        print(f"Erreur lors de l'initialisation de la DB: {e}")
        raise

@auto_backup_before_critical
def auto_import_streets(conn):
    """Import automatique des rues de Mascouche"""
    try:
        # Essayer d'abord avec OSM
        from osm import generate_streets_csv
        csv_data = generate_streets_csv("Mascouche")
        
        if csv_data:
            import io
            df = pd.read_csv(io.StringIO(csv_data.decode('utf-8')))
            
            if not df.empty:
                for _, row in df.iterrows():
                    conn.execute(
                        "INSERT OR IGNORE INTO streets(name, sector, team, status) VALUES (?, ?, ?, 'a_faire')",
                        (row.get("name", ""), row.get("sector", ""), row.get("team", ""))
                    )
                conn.commit()
                print(f"‚úÖ {len(df)} rues import√©es automatiquement")
                log_activity(conn, None, "AUTO_IMPORT", f"Import automatique de {len(df)} rues")
                return
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de l'import OSM: {e}")
    
    # Fallback : Donn√©es de test si OSM √©choue
    print("üì¶ Import de donn√©es de test...")
    test_streets = [
        ("Mont√©e Masson", "Centre", ""),
        ("Chemin Sainte-Marie", "Centre", ""),
        ("Boulevard de Mascouche", "Centre", ""),
        ("Rue Dupras", "Centre", ""),
        ("Rue Saint-Pierre", "Centre", ""),
        ("Rue de l'√âglise", "Centre", ""),
        ("Avenue des √ârables", "Nord", ""),
        ("Rue des Pins", "Nord", ""),
        ("Rue Gravel", "Sud", ""),
        ("Rue Forget", "Sud", ""),
    ]
    
    for name, sector, team in test_streets:
        conn.execute(
            "INSERT OR IGNORE INTO streets(name, sector, team, status) VALUES (?, ?, ?, 'a_faire')",
            (name, sector, team)
        )
    conn.commit()
    print(f"‚úÖ {len(test_streets)} rues de test import√©es")

# ---------- Fonctions pour les √©quipes ----------
def hash_password(password):
    """Hash un mot de passe avec bcrypt et salt automatique"""
    salt = bcrypt.gensalt()
    hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
    return hashed.decode('utf-8')

def create_team(conn, team_id, name, password):
    """Cr√©e une nouvelle √©quipe avec validation"""
    try:
        # Valider les entr√©es
        valid_id, clean_id = validate_and_clean_input("team_id", team_id)
        valid_name, clean_name = validate_and_clean_input("text", name)
        valid_pwd, _ = validate_and_clean_input("password", password)
        
        if not valid_id or not valid_name or not valid_pwd:
            return False
        
        conn.execute(
            "INSERT INTO teams (id, name, password_hash) VALUES (?, ?, ?)",
            (clean_id, clean_name, hash_password(password))
        )
        conn.commit()
        log_activity(conn, clean_id, "TEAM_CREATED", f"√âquipe {clean_name} cr√©√©e")
        return True
    except sqlite3.IntegrityError:
        return False

def verify_team(conn, team_id, password):
    """V√©rifie les identifiants d'une √©quipe avec bcrypt"""
    cursor = conn.execute(
        "SELECT password_hash FROM teams WHERE id = ? AND active = 1",
        (team_id,)
    )
    row = cursor.fetchone()
    if row:
        try:
            # Support ancien SHA256 pour migration
            stored_hash = row[0]
            if stored_hash.startswith('$2b$') or stored_hash.startswith('$2a$'):
                # Hash bcrypt
                return bcrypt.checkpw(password.encode('utf-8'), stored_hash.encode('utf-8'))
            else:
                # Ancien SHA256, v√©rifier et migrer
                if stored_hash == hashlib.sha256(password.encode()).hexdigest():
                    # Migrer vers bcrypt
                    new_hash = hash_password(password)
                    conn.execute("UPDATE teams SET password_hash = ? WHERE id = ?", (new_hash, team_id))
                    conn.commit()
                    return True
                return False
        except Exception as e:
            print(f"Erreur v√©rification mot de passe: {e}")
            return False
    return False

def migrate_all_passwords_to_bcrypt(conn):
    """Migration manuelle des mots de passe SHA256 vers bcrypt"""
    print("‚ö†Ô∏è Migration des mots de passe requise")
    print("Entrez les mots de passe actuels pour migration:")
    
    cursor = conn.execute("SELECT id, name FROM teams WHERE active = 1")
    teams = cursor.fetchall()
    
    for team_id, team_name in teams:
        if team_id == 'ADMIN':
            pwd = input(f"Mot de passe actuel pour {team_name} (ADMIN): ")
            if pwd:
                new_hash = hash_password(pwd)
                conn.execute("UPDATE teams SET password_hash = ? WHERE id = ?", (new_hash, team_id))
        
    conn.commit()
    print("‚úÖ Migration termin√©e")

def get_all_teams(conn):
    """R√©cup√®re toutes les √©quipes avec leurs statistiques"""
    query = """
    SELECT 
        t.id,
        t.name,
        t.created_at,
        COUNT(DISTINCT s.name) as streets_count,
        SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as done_count,
        CASE 
            WHEN COUNT(s.name) > 0 
            THEN (SUM(CASE WHEN s.status = 'terminee' THEN 1.0 ELSE 0 END) / COUNT(s.name)) * 100
            ELSE 0 
        END as progress
    FROM teams t
    LEFT JOIN streets s ON t.id = s.team
    WHERE t.active = 1 AND t.id != 'ADMIN'
    GROUP BY t.id, t.name, t.created_at
    ORDER BY t.id
    """
    return pd.read_sql_query(query, conn)

@auto_backup_before_critical
def delete_team(conn, team_id):
    """D√©sactive une √©quipe"""
    conn.execute("UPDATE teams SET active = 0 WHERE id = ?", (team_id,))
    conn.execute("UPDATE streets SET team = NULL WHERE team = ?", (team_id,))
    conn.commit()
    log_activity(conn, None, "TEAM_DELETED", f"√âquipe {team_id} supprim√©e")

def teams(conn):
    """Liste des IDs d'√©quipes actives"""
    cursor = conn.execute(
        "SELECT id FROM teams WHERE active = 1 AND id != 'ADMIN' ORDER BY id"
    )
    return [row[0] for row in cursor.fetchall()]

# ---------- Fonctions pour les rues ----------
def list_streets(conn, team=None):
    """Liste les rues, optionnellement filtr√©es par √©quipe"""
    try:
        if team:
            query = """
                SELECT 
                    s.name, 
                    COALESCE(s.sector, '') as sector, 
                    COALESCE(s.team, '') as team, 
                    COALESCE(s.status, 'a_faire') as status,
                    COUNT(n.id) as notes,
                    COUNT(DISTINCT n.address_number) as addresses_with_notes
                FROM streets s
                LEFT JOIN notes n ON s.name = n.street_name
                WHERE s.team = ?
                GROUP BY s.name, s.sector, s.team, s.status
                ORDER BY 
                    CASE s.status 
                        WHEN 'a_faire' THEN 1 
                        WHEN 'en_cours' THEN 2 
                        WHEN 'terminee' THEN 3 
                    END, 
                    s.name
            """
            df = pd.read_sql_query(query, conn, params=(team,))
        else:
            query = """
                SELECT 
                    s.name, 
                    COALESCE(s.sector, '') as sector, 
                    COALESCE(s.team, '') as team, 
                    COALESCE(s.status, 'a_faire') as status,
                    COUNT(n.id) as notes,
                    COUNT(DISTINCT n.address_number) as addresses_with_notes
                FROM streets s
                LEFT JOIN notes n ON s.name = n.street_name
                GROUP BY s.name, s.sector, s.team, s.status
                ORDER BY 
                    s.team, 
                    CASE s.status 
                        WHEN 'a_faire' THEN 1 
                        WHEN 'en_cours' THEN 2 
                        WHEN 'terminee' THEN 3 
                    END, 
                    s.name
            """
            df = pd.read_sql_query(query, conn)
        
        # S'assurer que toutes les colonnes existent
        for col in ['name', 'sector', 'team', 'status', 'notes', 'addresses_with_notes']:
            if col not in df.columns:
                df[col] = '' if col in ['sector', 'team'] else ('a_faire' if col == 'status' else 0)
        
        return df
        
    except Exception as e:
        print(f"Erreur list_streets: {e}")
        # Retourner un DataFrame vide avec la structure attendue
        return pd.DataFrame(columns=['name', 'sector', 'team', 'status', 'notes', 'addresses_with_notes'])

def get_unassigned_streets(conn):
    """R√©cup√®re les rues non assign√©es"""
    query = """
        SELECT name, sector 
        FROM streets 
        WHERE team IS NULL OR team = ''
        ORDER BY sector, name
    """
    return pd.read_sql_query(query, conn)

def assign_streets_to_team(conn, street_names, team_id):
    """Assigne plusieurs rues √† une √©quipe en une transaction"""
    try:
        for street_name in street_names:
            conn.execute(
                "UPDATE streets SET team = ? WHERE name = ?",
                (team_id, street_name)
            )
        conn.commit()
        log_activity(conn, team_id, "STREETS_ASSIGNED", f"{len(street_names)} rues assign√©es")
        return True
    except Exception as e:
        conn.rollback()
        print(f"Erreur lors de l'assignation: {e}")
        return False

def set_status(conn, name, status):
    """Met √† jour le statut d'une rue avec validation"""
    valid_name, clean_name = validate_and_clean_input("street_name", name)
    clean_status = InputValidator.validate_status(status)
    
    if not valid_name:
        print("‚ùå Nom de rue invalide")
        return False
    
    conn.execute(
        "UPDATE streets SET status = ? WHERE name = ?",
        (clean_status, clean_name)
    )
    conn.commit()
    
    cursor = conn.execute("SELECT team FROM streets WHERE name = ?", (clean_name,))
    row = cursor.fetchone()
    if row:
        log_activity(conn, row[0], f"STATUS_{clean_status.upper()}", f"Rue {clean_name}")
    return True

# ---------- Fonctions pour les notes PAR ADRESSE ----------
def add_note_for_address(conn, street_name, team_id, address_number, comment):
    """Ajoute une note pour une adresse sp√©cifique avec validation"""
    # Valider toutes les entr√©es
    valid_street, clean_street = validate_and_clean_input("street_name", street_name)
    valid_team, clean_team = validate_and_clean_input("team_id", team_id)
    valid_addr, clean_addr = validate_and_clean_input("address", address_number)
    valid_note, clean_note = validate_and_clean_input("note", comment)
    
    if not all([valid_street, valid_team, valid_addr, valid_note]):
        print("‚ùå Donn√©es invalides pour la note")
        return False
    
    conn.execute(
        """INSERT INTO notes (street_name, team_id, address_number, comment) 
           VALUES (?, ?, ?, ?)""",
        (clean_street, clean_team, clean_addr, clean_note)
    )
    
    # Met automatiquement le statut √† "en_cours" si c'√©tait "a_faire"
    conn.execute(
        """UPDATE streets 
           SET status = CASE 
               WHEN status = 'a_faire' THEN 'en_cours' 
               ELSE status 
           END
           WHERE name = ?""",
        (clean_street,)
    )
    
    conn.commit()
    log_activity(conn, clean_team, "NOTE_ADDED", f"Note ajout√©e pour {clean_addr} {clean_street}")
    return True

def get_street_addresses_with_notes(conn, street_name):
    """R√©cup√®re toutes les adresses avec notes pour une rue"""
    query = """
        SELECT 
            n.address_number,
            n.comment,
            n.created_at,
            t.name as team_name
        FROM notes n
        JOIN teams t ON n.team_id = t.id
        WHERE n.street_name = ?
        ORDER BY 
            CAST(n.address_number AS INTEGER),
            n.created_at DESC
    """
    return pd.read_sql_query(query, conn, params=(street_name,))

def get_team_notes(conn, team_id):
    """R√©cup√®re toutes les notes d'une √©quipe"""
    query = """
        SELECT 
            street_name, 
            address_number, 
            comment, 
            created_at
        FROM notes
        WHERE team_id = ?
        ORDER BY created_at DESC
        LIMIT 50
    """
    return pd.read_sql_query(query, conn, params=(team_id,))

# ---------- Fonctions de statistiques ----------
def extended_stats(conn):
    """Statistiques √©tendues avec d√©tails par adresse"""
    cursor = conn.execute("""
        SELECT 
            COUNT(DISTINCT s.name) as total,
            SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as done,
            SUM(CASE WHEN s.status = 'en_cours' THEN 1 ELSE 0 END) as partial,
            SUM(CASE WHEN s.status = 'a_faire' THEN 1 ELSE 0 END) as todo,
            COUNT(DISTINCT n.id) as total_notes,
            COUNT(DISTINCT n.address_number || n.street_name) as addresses_with_notes
        FROM streets s
        LEFT JOIN notes n ON s.name = n.street_name
    """)
    row = cursor.fetchone()
    return {
        "total": row[0] or 0,
        "done": row[1] or 0,
        "partial": row[2] or 0,
        "todo": row[3] or 0,
        "total_notes": row[4] or 0,
        "addresses_with_notes": row[5] or 0
    }

def stats_by_team(conn):
    """Statistiques par √©quipe"""
    query = """
        SELECT 
            s.team,
            COUNT(DISTINCT s.name) as total,
            SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as done,
            SUM(CASE WHEN s.status = 'en_cours' THEN 1 ELSE 0 END) as partial,
            COUNT(DISTINCT n.id) as notes,
            ROUND(
                (SUM(CASE WHEN s.status = 'terminee' THEN 1.0 ELSE 0 END) / COUNT(*)) * 100, 
                1
            ) as progress
        FROM streets s
        LEFT JOIN notes n ON s.name = n.street_name AND n.team_id = s.team
        WHERE s.team IS NOT NULL AND s.team != ''
        GROUP BY s.team
        ORDER BY progress DESC
    """
    return pd.read_sql_query(query, conn)

# ---------- Fonctions d'activit√© ----------
def recent_activity(conn, limit=10):
    """R√©cup√®re l'activit√© r√©cente"""
    query = """
        SELECT 
            datetime(created_at, 'localtime') as timestamp,
            COALESCE(team_id, 'SYSTEM') as team,
            action,
            details
        FROM activity_log
        ORDER BY created_at DESC
        LIMIT ?
    """
    return pd.read_sql_query(query, conn, params=(limit,))

# ---------- Fonctions d'export ----------
def export_to_csv(conn):
    """Exporte toutes les donn√©es en CSV"""
    query = """
        SELECT 
            s.name as rue,
            s.sector as secteur,
            s.team as equipe,
            s.status as statut,
            COUNT(DISTINCT n.id) as nombre_notes,
            COUNT(DISTINCT n.address_number) as adresses_avec_notes
        FROM streets s
        LEFT JOIN notes n ON s.name = n.street_name
        GROUP BY s.name, s.sector, s.team, s.status
        ORDER BY s.team, s.name
    """
    df = pd.read_sql_query(query, conn)
    return df.to_csv(index=False).encode('utf-8')

def export_notes_csv(conn):
    """Exporte toutes les notes en CSV avec adresses"""
    query = """
        SELECT 
            n.street_name as rue,
            n.address_number as numero,
            n.team_id as equipe,
            n.comment as commentaire,
            n.created_at as date_creation
        FROM notes n
        ORDER BY n.street_name, CAST(n.address_number AS INTEGER), n.created_at DESC
    """
    df = pd.read_sql_query(query, conn)
    return df.to_csv(index=False).encode('utf-8')

# ========================================
# NOUVELLES FONCTIONS POUR LES ADRESSES
# ========================================

@auto_backup_before_critical
def import_addresses_from_cache(conn, cache):
    """
    Importe les adresses depuis le cache OSM vers la base de donn√©es
    """
    try:
        # Vider la table existante
        conn.execute("DELETE FROM addresses")
        
        imported_count = 0
        skipped_count = 0
        
        for street_name, addresses in cache.items():
            # V√©rifier que la rue existe dans la DB
            cursor = conn.execute("SELECT COUNT(*) FROM streets WHERE name = ?", (street_name,))
            if cursor.fetchone()[0] == 0:
                # Si la rue n'existe pas, la cr√©er
                conn.execute(
                    "INSERT OR IGNORE INTO streets(name, sector, team, status) VALUES (?, '', '', 'a_faire')",
                    (street_name,)
                )
                print(f"‚ûï Rue ajout√©e: {street_name}")
            
            for addr in addresses:
                try:
                    # Validation des donn√©es
                    number = str(addr.get("number", "")).strip()
                    lat = addr.get("lat")
                    lon = addr.get("lon")
                    osm_type = addr.get("type", "unknown")
                    
                    if not number or lat is None or lon is None:
                        skipped_count += 1
                        continue
                    
                    conn.execute(
                        """INSERT INTO addresses (street_name, house_number, latitude, longitude, osm_type) 
                           VALUES (?, ?, ?, ?, ?)""",
                        (street_name, number, float(lat), float(lon), osm_type)
                    )
                    imported_count += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur import adresse {addr}: {e}")
                    skipped_count += 1
        
        conn.commit()
        log_activity(conn, None, "ADDRESSES_IMPORTED", f"{imported_count} adresses import√©es, {skipped_count} ignor√©es")
        print(f"‚úÖ {imported_count} adresses import√©es en base de donn√©es ({skipped_count} ignor√©es)")
        return imported_count
        
    except Exception as e:
        conn.rollback()
        print(f"‚ùå Erreur import adresses: {e}")
        return 0

def get_addresses_for_street(conn, street_name):
    """
    R√©cup√®re toutes les adresses d'une rue depuis la base de donn√©es
    """
    query = """
        SELECT 
            house_number,
            latitude,
            longitude,
            osm_type,
            created_at
        FROM addresses
        WHERE street_name = ?
        ORDER BY CAST(house_number AS INTEGER)
    """
    return pd.read_sql_query(query, conn, params=(street_name,))

def get_addresses_stats(conn):
    """
    R√©cup√®re les statistiques des adresses
    """
    cursor = conn.execute("""
        SELECT 
            COUNT(DISTINCT street_name) as streets_with_addresses,
            COUNT(*) as total_addresses,
            COUNT(DISTINCT CASE WHEN osm_type = 'node' THEN id END) as node_addresses,
            COUNT(DISTINCT CASE WHEN osm_type = 'way' THEN id END) as way_addresses
        FROM addresses
    """)
    row = cursor.fetchone()
    return {
        "streets_with_addresses": row[0] or 0,
        "total_addresses": row[1] or 0,
        "node_addresses": row[2] or 0,
        "way_addresses": row[3] or 0
    }

def get_backup_manager(db_path):
    """Retourne une instance du gestionnaire de backup"""
    return BackupManager(db_path)

# ================================================================================
# NOUVELLES FONCTIONS v4.1 - SUPERVISEUR ET B√âN√âVOLE
# ================================================================================

def get_unassigned_streets_count(conn):
    """Compte les rues non assign√©es √† une √©quipe"""
    try:
        cursor = conn.execute("""
            SELECT COUNT(*) FROM streets 
            WHERE team IS NULL OR team = ''
        """)
        return cursor.fetchone()[0] or 0
    except Exception as e:
        print(f"Erreur get_unassigned_streets_count: {e}")
        return 0

def get_sectors_list(conn):
    """R√©cup√®re la liste des secteurs disponibles"""
    try:
        cursor = conn.execute("""
            SELECT DISTINCT sector FROM streets 
            WHERE sector IS NOT NULL AND sector != ''
            ORDER BY sector
        """)
        return [row[0] for row in cursor.fetchall()]
    except Exception as e:
        print(f"Erreur get_sectors_list: {e}")
        return []

def get_teams_list(conn):
    """R√©cup√®re la liste des √©quipes actives"""
    try:
        cursor = conn.execute("""
            SELECT id, name FROM teams 
            WHERE active = 1 AND id != 'ADMIN'
            ORDER BY name
        """)
        return [(row[0], row[1]) for row in cursor.fetchall()]
    except Exception as e:
        print(f"Erreur get_teams_list: {e}")
        return []

@auto_backup_before_critical
def bulk_assign_sector(conn, sector, team_id):
    """Assigne toutes les rues d'un secteur √† une √©quipe"""
    try:
        # Valider les entr√©es
        valid_sector, clean_sector = validate_and_clean_input("sector", sector)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        
        if not valid_sector or not valid_team:
            raise ValueError("Secteur ou √©quipe invalide")
        
        # V√©rifier que l'√©quipe existe
        cursor = conn.execute("SELECT COUNT(*) FROM teams WHERE id = ?", (clean_team,))
        if cursor.fetchone()[0] == 0:
            raise ValueError(f"√âquipe {clean_team} inexistante")
        
        # Effectuer l'assignation
        cursor = conn.execute("""
            UPDATE streets 
            SET team = ? 
            WHERE sector = ? AND (team IS NULL OR team = '')
        """, (clean_team, clean_sector))
        
        affected_rows = cursor.rowcount
        conn.commit()
        
        # Log de l'activit√©
        log_activity(conn, clean_team, "bulk_assign", 
                    f"Assignation secteur {clean_sector}: {affected_rows} rues")
        
        return affected_rows
        
    except Exception as e:
        print(f"Erreur bulk_assign_sector: {e}")
        return 0

def get_team_streets(conn, team_id):
    """R√©cup√®re les rues assign√©es √† une √©quipe"""
    try:
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        if not valid_team:
            return pd.DataFrame()
        
        query = """
            SELECT 
                s.name as street_name,
                s.sector,
                s.status,
                COUNT(n.id) as notes_count
            FROM streets s
            LEFT JOIN notes n ON s.name = n.street_name AND n.team_id = ?
            WHERE s.team = ?
            GROUP BY s.name, s.sector, s.status
            ORDER BY s.sector, s.name
        """
        return pd.read_sql_query(query, conn, params=(clean_team, clean_team))
        
    except Exception as e:
        print(f"Erreur get_team_streets: {e}")
        return pd.DataFrame()

@auto_backup_before_critical
def update_street_status(conn, street_name, new_status, team_id):
    """Met √† jour le statut d'une rue"""
    try:
        # Valider les entr√©es
        valid_street, clean_street = validate_and_clean_input("street_name", street_name)
        valid_status, clean_status = validate_and_clean_input("status", new_status)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        
        if not all([valid_street, valid_status, valid_team]):
            raise ValueError("Param√®tres invalides")
        
        # V√©rifier que la rue est assign√©e √† cette √©quipe
        cursor = conn.execute("""
            SELECT COUNT(*) FROM streets 
            WHERE name = ? AND team = ?
        """, (clean_street, clean_team))
        
        if cursor.fetchone()[0] == 0:
            raise ValueError(f"Rue {clean_street} non assign√©e √† l'√©quipe {clean_team}")
        
        # Mettre √† jour le statut
        conn.execute("""
            UPDATE streets 
            SET status = ? 
            WHERE name = ? AND team = ?
        """, (clean_status, clean_street, clean_team))
        
        conn.commit()
        
        # Log de l'activit√©
        log_activity(conn, clean_team, "status_update", 
                    f"Rue {clean_street}: {clean_status}")
        
        return True
        
    except Exception as e:
        print(f"Erreur update_street_status: {e}")
        return False

def get_assignations_export_data(conn):
    """R√©cup√®re les donn√©es d'assignation pour export CSV"""
    try:
        query = """
            SELECT 
                COALESCE(sector, 'Non d√©fini') as secteur,
                name as rue,
                COALESCE(team, 'Non assign√©e') as equipe,
                CASE status 
                    WHEN 'a_faire' THEN '√Ä faire'
                    WHEN 'en_cours' THEN 'En cours'
                    WHEN 'terminee' THEN 'Termin√©e'
                    ELSE status 
                END as statut
            FROM streets
            ORDER BY secteur, rue
        """
        return pd.read_sql_query(query, conn)
        
    except Exception as e:
        print(f"Erreur get_assignations_export_data: {e}")
        return pd.DataFrame()

def log_activity(conn, team_id, action, details):
    """Enregistre une activit√© dans le log"""
    try:
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        valid_action, clean_action = validate_and_clean_input("text", action)
        valid_details, clean_details = validate_and_clean_input("note", details)
        
        if not all([valid_team, valid_action, valid_details]):
            print("Param√®tres de log invalides")
            return
        
        conn.execute("""
            INSERT INTO activity_log (team_id, action, details)
            VALUES (?, ?, ?)
        """, (clean_team, clean_action, clean_details))
        
        conn.commit()
        
        # Log aussi dans un fichier texte pour backup
        log_dir = Path(__file__).parent / "logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / "activity.log"
        with open(log_file, "a", encoding="utf-8") as f:
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"{timestamp} | {clean_team} | {clean_action} | {clean_details}\n")
            
    except Exception as e:
        print(f"Erreur log_activity: {e}")

def get_street_notes_for_team(conn, street_name, team_id):
    """R√©cup√®re les notes d'une rue pour une √©quipe"""
    try:
        valid_street, clean_street = validate_and_clean_input("street_name", street_name)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        
        if not all([valid_street, valid_team]):
            return []
        
        cursor = conn.execute("""
            SELECT address_number, comment, created_at
            FROM notes
            WHERE street_name = ? AND team_id = ?
            ORDER BY created_at DESC
        """, (clean_street, clean_team))
        
        return cursor.fetchall()
        
    except Exception as e:
        print(f"Erreur get_street_notes_for_team: {e}")
        return []

@auto_backup_before_critical
def add_street_note(conn, street_name, team_id, address_number, comment):
    """Ajoute une note pour une adresse sp√©cifique"""
    try:
        # Valider les entr√©es
        valid_street, clean_street = validate_and_clean_input("street_name", street_name)
        valid_team, clean_team = validate_and_clean_input("team_id", team_id)
        valid_address, clean_address = validate_and_clean_input("address", address_number)
        valid_comment, clean_comment = validate_and_clean_input("note", comment)
        
        if not all([valid_street, valid_team, valid_address, valid_comment]):
            raise ValueError("Param√®tres invalides")
        
        # V√©rifier que la rue est assign√©e √† cette √©quipe
        cursor = conn.execute("""
            SELECT COUNT(*) FROM streets 
            WHERE name = ? AND team = ?
        """, (clean_street, clean_team))
        
        if cursor.fetchone()[0] == 0:
            raise ValueError(f"Rue {clean_street} non assign√©e √† l'√©quipe {clean_team}")
        
        # Ajouter la note
        conn.execute("""
            INSERT INTO notes (street_name, team_id, address_number, comment)
            VALUES (?, ?, ?, ?)
        """, (clean_street, clean_team, clean_address, clean_comment))
        
        conn.commit()
        
        # Log de l'activit√©
        log_activity(conn, clean_team, "note_added", 
                    f"Note ajout√©e - {clean_street} #{clean_address}")
        
        return True
        
    except Exception as e:
        print(f"Erreur add_street_note: {e}")
        return False
# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/db.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/db_v5.py
# --- encoding: utf-8 | size: 118 bytes | sha256: c1632b2490df21aa1ecd40da80e9f567fb53bdcc6623d33766eac57f22bb2e91 | mtime: 2025-09-15T21:31:11-04:00
# --------------------------------------------------------------------------------
"""Compat shim: permet 'import db_v5' depuis guignomap/app.py"""
from src.database.db_v5 import *  # r√©-exporte tout
# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/db_v5.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/osm.py
# --- encoding: utf-8 | size: 19722 bytes | sha256: 9fb5cd3d65276a1fba841b7225c940d86feada9cd55ba501b6d8a05d5d801eeb | mtime: 2025-09-14T14:16:51-04:00
# --------------------------------------------------------------------------------
"""
Module OSM pour Guigno-Map
G√®re l'import et le cache des donn√©es OpenStreetMap pour Mascouche
"""

import io
import json
from pathlib import Path
import pandas as pd
import overpy

# Configuration
CACHE_FILE = Path(__file__).parent / "osm_cache.json"
ADDR_CACHE_FILE = Path(__file__).parent / "osm_addresses.json"

# Toutes les voies routi√®res nomm√©es de Mascouche
QUERY_STREETS_ALL = """
[out:json][timeout:300];
area["name"="Mascouche"]["boundary"="administrative"]->.a;
(
  way["highway"~"^(primary|secondary|tertiary|residential|service|unclassified|living_street|pedestrian|track|road|busway|footway|path)$"](area.a);
);
(._;>;);
out body;
"""
# Note: R√©cup√®re TOUS les types de voies incluant petites rues, all√©es, chemins pi√©tonniers

# Requ√™te pour les adresses
QUERY_ADDR_NODES = """
[out:json][timeout:180];
area["name"="Mascouche"]["boundary"="administrative"]->.a;
(
  node["addr:housenumber"]["addr:street"](area.a);
  way["addr:housenumber"]["addr:street"](area.a);
);
out tags center;
"""

def generate_streets_csv(city="Mascouche"):
    """
    G√©n√®re un CSV avec les noms des rues principales de la ville
    Filtre automatiquement les rues priv√©es et les petites ruelles
    """
    try:
        api = overpy.Overpass()
        result = api.query(QUERY_STREETS_ALL)
        
        streets = []
        for way in result.ways:
            name = way.tags.get("name")
            if not name:
                continue
            g = getattr(way, "geometry", None)
            # garder si on a une vraie g√©om√©trie (>= 2 points)
            if isinstance(g, list) and len(g) >= 2:
                streets.append(name)

        streets = sorted(set(streets))
        
        # Assigner automatiquement des secteurs bas√©s sur les patterns de noms
        sectors = []
        for street in streets:
            if any(word in street.lower() for word in ["mont√©e", "chemin", "boulevard"]):
                sectors.append("Principal")
            elif any(word in street.lower() for word in ["avenue", "place", "croissant"]):
                sectors.append("R√©sidentiel")
            elif "rue" in street.lower():
                sectors.append("Centre")
            else:
                sectors.append("")
        
        df = pd.DataFrame({
            "name": streets,
            "sector": sectors,
            "team": [""] * len(streets)
        })
        
        buf = io.StringIO()
        df.to_csv(buf, index=False)
        print(f"‚úÖ CSV g√©n√©r√© avec {len(streets)} rues principales")
        return buf.getvalue().encode("utf-8")
        
    except Exception as e:
        print(f"‚ùå Erreur OSM: {e}")
        # Retourner des donn√©es de test en cas d'erreur
        return create_fallback_csv()

def build_geometry_cache():
    """
    Construit le cache des g√©om√©tries pour TOUTES les voies de Mascouche
    Force la r√©solution compl√®te des nodes
    """
    try:
        print("üîÑ R√©cup√©ration compl√®te de toutes les voies de Mascouche...")
        
        # IMPORTANT: Configurer l'API pour r√©soudre automatiquement les nodes manquants
        api = overpy.Overpass()
        
        # Requ√™te am√©lior√©e qui force le retour des coordonn√©es
        query = """
        [out:json][timeout:300];
        area["name"="Mascouche"]["boundary"="administrative"]->.a;
        (
          way["highway"]["name"](area.a);
          way["highway"]["ref"](area.a);
        );
        (._;>;);
        out body;
        """
        
        print("üì° Connexion √† OpenStreetMap (cela peut prendre 30-60 secondes)...")
        result = api.query(query)
        
        geo = {}
        stats = {"total": 0, "avec_geo": 0, "sans_geo": 0}
        
        # Construire un dictionnaire des nodes pour acc√®s rapide
        nodes_dict = {}
        if hasattr(result, 'nodes'):
            for node in result.nodes:
                if hasattr(node, 'id') and hasattr(node, 'lat') and hasattr(node, 'lon'):
                    nodes_dict[node.id] = (float(node.lat), float(node.lon))
        
        print(f"üìç {len(nodes_dict)} nodes r√©cup√©r√©s")
        
        ways = result.ways if hasattr(result, 'ways') else []
        print(f"üìä {len(ways)} voies trouv√©es dans OpenStreetMap")
        
        for way in ways:
            try:
                # R√©cup√©rer le nom ou ref
                if not hasattr(way, 'tags'):
                    continue
                    
                name = way.tags.get("name")
                if not name:
                    ref = way.tags.get("ref")
                    if ref:
                        name = f"Autoroute {ref}"
                    else:
                        continue
                
                stats["total"] += 1
                coords = []
                
                # R√©cup√©rer les IDs des nodes
                if hasattr(way, 'nd_ids'):
                    # Si on a les IDs des nodes, les r√©soudre
                    for node_id in way.nd_ids:
                        if node_id in nodes_dict:
                            lat, lon = nodes_dict[node_id]
                            coords.append([lat, lon])
                elif hasattr(way, 'nodes'):
                    # Si on a directement les nodes
                    for node in way.nodes:
                        if hasattr(node, 'lat') and hasattr(node, 'lon'):
                            coords.append([float(node.lat), float(node.lon)])
                        elif hasattr(node, 'id') and node.id in nodes_dict:
                            lat, lon = nodes_dict[node.id]
                            coords.append([lat, lon])
                
                if len(coords) >= 2:
                    if name not in geo:
                        geo[name] = []
                    geo[name].append(coords)
                    stats["avec_geo"] += 1
                else:
                    stats["sans_geo"] += 1
                    
            except Exception as e:
                continue
        
        print(f"‚úÖ R√©sultat: {stats['avec_geo']} voies avec g√©om√©trie sur {stats['total']} trouv√©es")
        
        # Si on a r√©cup√©r√© des donn√©es, sauvegarder
        if geo:
            CACHE_FILE.write_text(json.dumps(geo, indent=2), encoding="utf-8")
            print(f"üíæ Cache cr√©√©: {len(geo)} voies sauvegard√©es dans osm_cache.json")
            
            # Importer aussi automatiquement dans la DB
            try:
                from pathlib import Path
                import sys
                sys.path.append(str(Path(__file__).parent))
                import db
                
                db_path = Path(__file__).parent / "guigno_map.db"
                conn = db.get_conn(db_path)
                
                # Ajouter les rues manquantes √† la DB
                for street_name in geo.keys():
                    cursor = conn.execute("SELECT COUNT(*) FROM streets WHERE name = ?", (street_name,))
                    if cursor.fetchone()[0] == 0:
                        conn.execute(
                            "INSERT INTO streets(name, sector, team, status) VALUES (?, '', '', 'a_faire')",
                            (street_name,)
                        )
                conn.commit()
                print(f"‚úÖ Rues import√©es dans la base de donn√©es")
            except Exception as e:
                print(f"‚ö†Ô∏è Import DB: {e}")
            
            return geo
        
        # Si aucune donn√©e, utiliser un fallback √©tendu
        print("‚ö†Ô∏è Aucune donn√©e OSM, utilisation du fallback local")
        return get_extended_fallback()
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return get_extended_fallback()

def get_fallback_geometry():
    """Fallback avec les principales voies de Mascouche"""
    return {
        "Autoroute 25": [[[45.70, -73.65], [45.78, -73.58]]],
        "Autoroute 640": [[[45.76, -73.70], [45.76, -73.55]]],
        "Mont√©e Masson": [[[45.730, -73.620], [45.765, -73.580]]],
        "Chemin Sainte-Marie": [[[45.735, -73.615], [45.755, -73.595]]],
        "Boulevard de Mascouche": [[[45.740, -73.610], [45.752, -73.590]]],
        "Chemin des Anglais": [[[45.74, -73.65], [45.75, -73.64]]],
        "Chemin Gascon": [[[45.75, -73.62], [45.76, -73.60]]],
        "Chemin Pincourt": [[[45.72, -73.64], [45.73, -73.63]]],
        "Chemin Newton": [[[45.73, -73.58], [45.74, -73.57]]],
        "Chemin Saint-Henri": [[[45.71, -73.61], [45.72, -73.60]]],
        "Chemin Saint-Pierre": [[[45.74, -73.59], [45.75, -73.58]]],
        "Avenue de la Gare": [[[45.745, -73.601], [45.748, -73.598]]],
        "Rue Dupras": [[[45.745, -73.602], [45.748, -73.599]]],
        "Rue Saint-Pierre": [[[45.746, -73.604], [45.749, -73.600]]],
        "Rue de l'√âglise": [[[45.747, -73.601], [45.750, -73.599]]],
        "Avenue des √ârables": [[[45.755, -73.605], [45.758, -73.600]]],
        "Rue des Pins": [[[45.756, -73.603], [45.759, -73.598]]],
        "Rue Brien": [[[45.738, -73.605], [45.741, -73.600]]],
        "Rue Boh√©mier": [[[45.742, -73.607], [45.745, -73.604]]]
    }

def get_extended_fallback():
    """Fallback √©tendu avec les principales voies de Mascouche"""
    fallback = {
        # Autoroutes
        "Autoroute 25": [[[45.70, -73.65], [45.72, -73.63], [45.74, -73.61], [45.76, -73.59], [45.78, -73.58]]],
        "Autoroute 640": [[[45.76, -73.70], [45.76, -73.65], [45.76, -73.60], [45.76, -73.55]]],
        
        # Chemins principaux
        "Mont√©e Masson": [[[45.730, -73.620], [45.740, -73.610], [45.750, -73.600], [45.765, -73.580]]],
        "Chemin Sainte-Marie": [[[45.735, -73.615], [45.745, -73.605], [45.755, -73.595]]],
        "Boulevard de Mascouche": [[[45.740, -73.610], [45.747, -73.600], [45.752, -73.590]]],
        "Chemin des Anglais": [[[45.74, -73.65], [45.745, -73.645], [45.75, -73.64]]],
        "Chemin Gascon": [[[45.75, -73.62], [45.755, -73.615], [45.76, -73.60]]],
        "Chemin Pincourt": [[[45.72, -73.64], [45.725, -73.635], [45.73, -73.63]]],
        "Chemin Newton": [[[45.73, -73.58], [45.735, -73.575], [45.74, -73.57]]],
        "Chemin Saint-Henri": [[[45.71, -73.61], [45.715, -73.605], [45.72, -73.60]]],
        "Chemin Saint-Pierre": [[[45.74, -73.59], [45.745, -73.585], [45.75, -73.58]]],
        
        # Avenues
        "Avenue de la Gare": [[[45.745, -73.601], [45.747, -73.599], [45.748, -73.598]]],
        "Avenue Bourque": [[[45.742, -73.603], [45.744, -73.601], [45.746, -73.599]]],
        "Avenue Cr√©peau": [[[45.743, -73.602], [45.745, -73.600], [45.747, -73.598]]],
        "Avenue Garden": [[[45.751, -73.606], [45.753, -73.604], [45.755, -73.602]]],
        "Avenue de l'Esplanade": [[[45.748, -73.605], [45.750, -73.603], [45.752, -73.601]]],
        
        # Rues du centre
        "Rue Dupras": [[[45.745, -73.602], [45.747, -73.600], [45.748, -73.599]]],
        "Rue Saint-Pierre": [[[45.746, -73.604], [45.748, -73.602], [45.749, -73.600]]],
        "Rue de l'√âglise": [[[45.747, -73.601], [45.749, -73.599], [45.750, -73.598]]],
        "Rue Brien": [[[45.738, -73.605], [45.740, -73.603], [45.741, -73.600]]],
        "Rue Boh√©mier": [[[45.742, -73.607], [45.744, -73.605], [45.745, -73.604]]],
        
        # Rues r√©sidentielles
        "Rue des Pins": [[[45.756, -73.603], [45.758, -73.601], [45.759, -73.598]]],
        "Avenue des √ârables": [[[45.755, -73.605], [45.757, -73.603], [45.758, -73.600]]],
        "Rue Gravel": [[[45.738, -73.605], [45.740, -73.603], [45.741, -73.600]]]
    }
    
    # Sauvegarder le fallback
    CACHE_FILE.write_text(json.dumps(fallback, indent=2), encoding="utf-8")
    print(f"üíæ Fallback sauvegard√© avec {len(fallback)} voies principales")
    
    return fallback

def load_geometry_cache():
    """
    Charge le cache de g√©om√©tries depuis le fichier JSON
    Cr√©e un cache de base si le fichier n'existe pas
    """
    if not CACHE_FILE.exists():
        print("‚ö†Ô∏è Cache non trouv√©, construction en cours...")
        return build_geometry_cache()  # build_geometry_cache() g√®re d√©j√† le fallback en m√©moire
    
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
            print(f"‚úÖ Cache charg√©: {len(cache)} rues")
            return cache
    except Exception as e:
        print(f"‚ùå Erreur chargement cache: {e}")
        # Ne pas √©crire de fallback sur disque ! Utiliser build_geometry_cache() qui g√®re le fallback en m√©moire
        return build_geometry_cache()

def create_fallback_csv():
    """
    Cr√©e un CSV de fallback avec quelques rues principales de Mascouche
    Utilis√© si l'API OSM est indisponible
    """
    fallback_streets = [
        ("Mont√©e Masson", "Principal"),
        ("Chemin Sainte-Marie", "Principal"),
        ("Boulevard de Mascouche", "Principal"),
        ("Chemin des Anglais", "Principal"),
        ("Rue Dupras", "Centre"),
        ("Rue Saint-Pierre", "Centre"),
        ("Rue de l'√âglise", "Centre"),
        ("Avenue des √ârables", "R√©sidentiel"),
        ("Rue des Pins", "R√©sidentiel"),
        ("Avenue Garden", "R√©sidentiel"),
    ]
    
    df = pd.DataFrame(fallback_streets, columns=["name", "sector"])
    df["team"] = ""
    
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    print("‚ö†Ô∏è Mode fallback: 10 rues de test")
    return buf.getvalue().encode("utf-8")

def create_fallback_cache():
    """
    Cr√©e un cache minimal pour tests
    """
    fallback_geo = {
        "Mont√©e Masson": [[[45.730, -73.620], [45.750, -73.600], [45.765, -73.580]]],
        "Chemin Sainte-Marie": [[[45.735, -73.615], [45.748, -73.602], [45.755, -73.595]]],
        "Boulevard de Mascouche": [[[45.740, -73.610], [45.747, -73.600], [45.752, -73.590]]],
        "Rue Dupras": [[[45.745, -73.602], [45.748, -73.599]]],
        "Rue Saint-Pierre": [[[45.746, -73.604], [45.749, -73.600]]],
        "Rue de l'√âglise": [[[45.747, -73.601], [45.750, -73.599]]],
        "Avenue des √ârables": [[[45.755, -73.605], [45.758, -73.600]]],
        "Rue des Pins": [[[45.756, -73.603], [45.759, -73.598]]],
        "Avenue Garden": [[[45.753, -73.606], [45.756, -73.601]]],
        "Rue Gravel": [[[45.738, -73.605], [45.741, -73.600]]]
    }
    
    CACHE_FILE.write_text(json.dumps(fallback_geo, indent=2), encoding="utf-8")
    print("‚ö†Ô∏è Cache fallback cr√©√© avec 10 rues")

# Fonction utilitaire pour tests
def test_osm_connection():
    """
    Teste la connexion √† l'API Overpass
    """
    try:
        api = overpy.Overpass()
        # Requ√™te minimale pour tester
        result = api.query('[out:json];node(45.7475,-73.6005,45.7476,-73.6004);out;')
        print("‚úÖ Connexion OSM OK")
        return True
    except:
        print("‚ùå Connexion OSM √©chou√©e")
        return False

# ========================================
# NOUVELLES FONCTIONS POUR LES ADRESSES
# ========================================

def build_addresses_cache():
    """
    Construit le cache des adresses OSM pour Mascouche
    R√©cup√®re addr:housenumber + addr:street depuis OSM
    """
    try:
        api = overpy.Overpass()
        result = api.query(QUERY_ADDR_NODES)
        
        addresses = {}
        
        # Traiter les nodes avec adresses
        for node in result.nodes:
            house_number = node.tags.get("addr:housenumber")
            street_name = node.tags.get("addr:street")
            
            if house_number and street_name:
                if street_name not in addresses:
                    addresses[street_name] = []
                addresses[street_name].append({
                    "number": str(house_number),  # Forcer en string
                    "lat": float(node.lat),
                    "lon": float(node.lon),
                    "type": "node"
                })
        
        # Traiter les ways avec adresses
        for way in result.ways:
            num = way.tags.get("addr:housenumber")
            street = way.tags.get("addr:street")
            if not num or not street:
                continue
            
            # R√©cup√©rer le centre du way
            lat = getattr(way, "center_lat", None)
            lon = getattr(way, "center_lon", None)
            
            # Fallback si center_lat/lon non disponibles
            if lat is None or lon is None:
                nodes = getattr(way, "nodes", []) or []
                if nodes:
                    try:
                        valid_lats = []
                        valid_lons = []
                        for n in nodes:
                            if hasattr(n, 'lat') and hasattr(n, 'lon'):
                                if n.lat is not None and n.lon is not None:
                                    valid_lats.append(float(n.lat))
                                    valid_lons.append(float(n.lon))
                        if valid_lats and valid_lons:
                            lat = sum(valid_lats) / len(valid_lats)
                            lon = sum(valid_lons) / len(valid_lons)
                    except Exception as e:
                        print(f"Erreur calcul centre pour way: {e}")
                        continue
            
            if lat is not None and lon is not None:
                addresses.setdefault(street, []).append({
                    "number": str(num),
                    "lat": float(lat),
                    "lon": float(lon),
                    "type": "way"
                })
        
        # Trier les adresses par num√©ro pour chaque rue
        for street_name in addresses:
            try:
                # Tri num√©rique intelligent
                addresses[street_name].sort(
                    key=lambda x: (
                        int(''.join(filter(str.isdigit, x["number"]))) 
                        if any(c.isdigit() for c in x["number"]) 
                        else float('inf')
                    )
                )
            except:
                # Si le tri √©choue, garder l'ordre original
                pass
        
        # Sauvegarder le cache
        ADDR_CACHE_FILE.write_text(json.dumps(addresses, indent=2), encoding="utf-8")
        total_addresses = sum(len(addrs) for addrs in addresses.values())
        print(f"‚úÖ Cache adresses cr√©√©: {len(addresses)} rues, {total_addresses} adresses")
        return addresses
        
    except Exception as e:
        print(f"‚ùå Erreur construction cache adresses: {e}")
        # Cr√©er un cache vide en cas d'erreur
        ADDR_CACHE_FILE.write_text(json.dumps({}), encoding="utf-8")
        return {}

def load_addresses_cache():
    """
    Charge le cache d'adresses depuis le fichier JSON
    """
    if not ADDR_CACHE_FILE.exists():
        print("‚ö†Ô∏è Cache adresses non trouv√©")
        return {}
    
    try:
        with open(ADDR_CACHE_FILE, 'r', encoding='utf-8') as f:
            cache = json.load(f)
            total_addresses = sum(len(addrs) for addrs in cache.values())
            print(f"‚úÖ Cache adresses charg√©: {len(cache)} rues, {total_addresses} adresses")
            return cache
    except Exception as e:
        print(f"‚ùå Erreur chargement cache adresses: {e}")
        return {}
# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/osm.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/reports.py
# --- encoding: utf-8 | size: 10201 bytes | sha256: 7c584c2d9a96e886a2f9b4906f39424927f0e03baa87f5f4af2209e365d247a5 | mtime: 2025-09-15T21:31:11-04:00
# --------------------------------------------------------------------------------
"""
G√©n√©rateur de rapports Excel et PDF pour GuignoMap
"""

from pathlib import Path
from datetime import datetime
import pandas as pd
from reportlab.lib import colors
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.enums import TA_CENTER, TA_RIGHT
import xlsxwriter
from io import BytesIO

# Mapping des statuts pour l'affichage (√©vite imports circulaires)
STATUS_TO_LABEL = {"a_faire": "√Ä faire", "en_cours": "En cours", "terminee": "Termin√©e"}

class ReportGenerator:
    def __init__(self):
        self.styles = getSampleStyleSheet()
        self._setup_custom_styles()
    
    def _setup_custom_styles(self):
        """D√©finit les styles personnalis√©s pour PDF"""
        self.styles.add(ParagraphStyle(
            name='CustomTitle',
            parent=self.styles['Heading1'],
            fontSize=24,
            textColor=colors.HexColor('#8B0000'),
            spaceAfter=30,
            alignment=TA_CENTER
        ))
        
        self.styles.add(ParagraphStyle(
            name='SectionTitle',
            parent=self.styles['Heading2'],
            fontSize=16,
            textColor=colors.HexColor('#8B0000'),
            spaceAfter=12,
            spaceBefore=12
        ))
    
    def generate_excel(self):
        """G√©n√®re un rapport Excel professionnel"""
        output = BytesIO()
        workbook = xlsxwriter.Workbook(output, {'remove_timezone': True})
        
        # Styles Excel
        header_format = workbook.add_format({
            'bold': True,
            'bg_color': '#8B0000',
            'font_color': 'white',
            'align': 'center',
            'valign': 'vcenter',
            'border': 1
        })
        
        cell_format = workbook.add_format({
            'align': 'center',
            'valign': 'vcenter',
            'border': 1
        })
        
        status_formats = {
            'terminee': workbook.add_format({'bg_color': '#90EE90', 'border': 1}),
            'en_cours': workbook.add_format({'bg_color': '#FFE4B5', 'border': 1}),
            'a_faire': workbook.add_format({'bg_color': '#FFB6C1', 'border': 1})
        }
        
        # Feuille 1 : R√©sum√©
        summary_sheet = workbook.add_worksheet('R√©sum√© Guignol√©e 2025')
        summary_sheet.set_column(0, 4, 20)  # A:E
        
        # Titre
        title_format = workbook.add_format({
            'bold': True,
            'font_size': 20,
            'font_color': '#8B0000',
            'align': 'center'
        })
        summary_sheet.merge_range(0, 0, 0, 4, 'GUIGNOL√âE 2025 - RELAIS DE MASCOUCHE', title_format)  # A1:E1
        summary_sheet.merge_range(1, 0, 1, 4, f'Rapport g√©n√©r√© le {datetime.now().strftime("%d/%m/%Y √† %H:%M")}', cell_format)  # A2:E2
        
        # Stats globales
        from db_v5 import extended_stats
        stats = extended_stats()
        
        row = 4
        summary_sheet.write(row, 0, 'STATISTIQUES GLOBALES', header_format)
        summary_sheet.merge_range(row, 1, row, 4, '', header_format)  # B{row+1}:E{row+1}
        
        row += 2
        summary_data = [
            ['Total des rues', stats['total']],
            ['Rues termin√©es', stats['done']],
            ['Rues en cours', stats.get('partial', 0)],
            ['Rues √† faire', stats.get('todo', 0)],
            ['Progression globale', f"{(stats['done']/stats['total']*100) if stats['total'] > 0 else 0:.1f}%"],
            ['Total des notes', stats.get('total_notes', 0)],
            ['Adresses avec notes', stats.get('addresses_with_notes', 0)]
        ]
        
        for label, value in summary_data:
            summary_sheet.write(row, 0, label, cell_format)
            summary_sheet.write(row, 1, value, cell_format)
            row += 1
        
        # Feuille 2 : D√©tail des rues
        streets_sheet = workbook.add_worksheet('D√©tail des rues')
        streets_sheet.set_column(0, 0, 30)
        streets_sheet.set_column(1, 4, 15)
        
        # Headers
        headers = ['Rue', 'Secteur', '√âquipe', 'Statut', 'Notes']
        for col, header in enumerate(headers):
            streets_sheet.write(0, col, header, header_format)
        
        # Donn√©es
        from db_v5 import list_streets
        df = list_streets()
        
        for idx, row_data in enumerate(df.iterrows(), 1):
            _, row = row_data
            streets_sheet.write(idx, 0, row.get('name', ''), cell_format)
            streets_sheet.write(idx, 1, row.get('sector', ''), cell_format)
            streets_sheet.write(idx, 2, row.get('team', ''), cell_format)
            
            status = row.get('status', 'a_faire')
            format_to_use = status_formats.get(status, cell_format)
            status_label = STATUS_TO_LABEL.get(status, "√Ä faire")
            streets_sheet.write(idx, 3, status_label, format_to_use)
            
            streets_sheet.write(idx, 4, row.get('notes', 0), cell_format)
        
        # Feuille 3 : Performance des √©quipes
        teams_sheet = workbook.add_worksheet('Performance √©quipes')
        teams_sheet.set_column(0, 5, 15)
        
        from db_v5 import stats_by_team
        teams_data = stats_by_team()
        
        if teams_data:
            headers = ['√âquipe', 'Total rues', 'Termin√©es', 'En cours', 'Notes', 'Progression %']
            for col, header in enumerate(headers):
                teams_sheet.write(0, col, header, header_format)
            
            for idx, row in enumerate(teams_data, 1):
                total = row.get('total_streets', 0)
                completed = row.get('completed', 0)
                in_progress = row.get('in_progress', 0)
                todo = row.get('todo', 0)
                progress = (completed / total * 100) if total > 0 else 0
                teams_sheet.write(idx, 0, row.get('name', ''), cell_format)
                teams_sheet.write(idx, 1, total, cell_format)
                teams_sheet.write(idx, 2, completed, cell_format)
                teams_sheet.write(idx, 3, in_progress, cell_format)
                teams_sheet.write(idx, 4, todo, cell_format)
                teams_sheet.write(idx, 5, f"{progress:.1f}%", cell_format)
        
        workbook.close()
        output.seek(0)
        return output.getvalue()
    
    def generate_pdf(self):
        """G√©n√®re un rapport PDF professionnel"""
        output = BytesIO()
        doc = SimpleDocTemplate(output, pagesize=A4)
        story = []
        
        # Page de titre
        story.append(Paragraph("GUIGNOL√âE 2025", self.styles['CustomTitle']))
        story.append(Paragraph("Le Relais de Mascouche", self.styles['Title']))
        story.append(Spacer(1, 0.2*inch))
        story.append(Paragraph(f"Rapport g√©n√©r√© le {datetime.now().strftime('%d/%m/%Y √† %H:%M')}", self.styles['Normal']))
        story.append(PageBreak())
        
        # R√©sum√©
        story.append(Paragraph("R√©sum√© de la collecte", self.styles['SectionTitle']))
        
        from db_v5 import extended_stats
        stats = extended_stats()
        
        summary_data = [
            ['Statistique', 'Valeur'],
            ['Total des rues', str(stats['total'])],
            ['Rues termin√©es', str(stats['done'])],
            ['Rues en cours', str(stats.get('partial', 0))],
            ['Rues √† faire', str(stats.get('todo', 0))],
            ['Progression', f"{(stats['done']/stats['total']*100) if stats['total'] > 0 else 0:.1f}%"],
            ['Total notes', str(stats.get('total_notes', 0))]
        ]
        
        summary_table = Table(summary_data, colWidths=[3*inch, 2*inch])
        summary_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#8B0000')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 14),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        story.append(summary_table)
        story.append(PageBreak())
        
        # Performance des √©quipes
        story.append(Paragraph("Performance des √©quipes", self.styles['SectionTitle']))
        
        from db_v5 import stats_by_team
        teams_data_list = stats_by_team()
        
        if teams_data_list:
            teams_data = [['√âquipe', 'Total', 'Termin√©es', 'En cours', 'Progression']]
            for row in teams_data_list:
                total = row.get('total_streets', 0)
                completed = row.get('completed', 0)
                in_progress = row.get('in_progress', 0)
                progress = (completed / total * 100) if total > 0 else 0
                teams_data.append([
                    row.get('name', ''),
                    str(total),
                    str(completed),
                    str(in_progress),
                    f"{progress:.1f}%"
                ])
            
            teams_table = Table(teams_data, colWidths=[2*inch, 1*inch, 1*inch, 1*inch, 1.5*inch])
            teams_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#8B0000')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('BACKGROUND', (0, 1), (-1, -1), colors.lightgrey),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            story.append(teams_table)
        
        doc.build(story)
        output.seek(0)
        return output.getvalue()
# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/reports.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: guignomap/validators.py
# --- encoding: utf-8 | size: 4834 bytes | sha256: e76c3c4d3862dc6d6b4e04a090472bb6923dd8fdb0da4b04218868b697a0d0a6 | mtime: 2025-09-14T19:39:26-04:00
# --------------------------------------------------------------------------------
"""
Validateurs et sanitizers pour GuignoMap
Protection contre injections et validation des formats
"""

import re
import html
from typing import Optional, Tuple

class InputValidator:
    """Classe de validation et sanitization des entr√©es"""
    
    @staticmethod
    def sanitize_text(text: str, max_length: int = 255) -> str:
        """Nettoie et limite un texte"""
        if not text:
            return ""
        # Supprimer les caract√®res de contr√¥le
        text = "".join(char for char in text if ord(char) >= 32 or char == '\n')
        # √âchapper le HTML
        text = html.escape(text)
        # Limiter la longueur
        return text[:max_length].strip()
    
    @staticmethod
    def sanitize_street_name(name: str) -> str:
        """Valide et nettoie un nom de rue"""
        if not name:
            return ""
        # Garder seulement lettres, chiffres, espaces, tirets, apostrophes, accents
        name = re.sub(r'[^a-zA-Z√Ä-√ø0-9\s\-\'\.]', '', name)
        return name[:100].strip()
    
    @staticmethod
    def sanitize_team_id(team_id: str) -> str:
        """Valide un ID d'√©quipe"""
        if not team_id:
            return ""
        # Format: LETTRES + CHIFFRES seulement, max 20 caract√®res
        team_id = re.sub(r'[^A-Z0-9]', '', team_id.upper())
        return team_id[:20]
    
    @staticmethod
    def sanitize_address_number(number: str) -> str:
        """Valide un num√©ro civique"""
        if not number:
            return ""
        # Garder chiffres et lettres (ex: 123A)
        number = re.sub(r'[^0-9A-Za-z\-]', '', number)
        return number[:10]
    
    @staticmethod
    def validate_password(password: str) -> Tuple[bool, str]:
        """Valide la force d'un mot de passe - minimum 4 caract√®res"""
        if password is None:
            return False, "Mot de passe requis"
        if len(password) < 4:
            return False, "Minimum 4 caract√®res"
        if len(password) > 128:
            return False, "Maximum 128 caract√®res"
        return True, "OK"
    
    @staticmethod
    def validate_sector(sector: str) -> str:
        """Valide un secteur"""
        valid_sectors = ['Principal', 'Centre', 'Nord', 'Sud', 'Est', 'Ouest', 'R√©sidentiel', '']
        if sector not in valid_sectors:
            return ''
        return sector
    
    @staticmethod
    def validate_status(status: str) -> str:
        """Valide un statut de rue"""
        valid_statuses = ['a_faire', 'en_cours', 'terminee']
        if status not in valid_statuses:
            return 'a_faire'
        return status
    
    @staticmethod
    def sanitize_note(note: str) -> str:
        """Nettoie une note/commentaire"""
        if not note:
            return ""
        # Supprimer caract√®res dangereux mais garder ponctuation basique
        note = re.sub(r'[<>\"\'`;]', '', note)
        return note[:500].strip()
    
    @staticmethod
    def is_sql_safe(text: str) -> bool:
        """V√©rifie qu'un texte ne contient pas de patterns SQL dangereux"""
        if not text:
            return True
        dangerous_patterns = [
            r'\bDROP\b', r'\bDELETE\b', r'\bINSERT\b', r'\bUPDATE\b',
            r'\bEXEC\b', r'\bEXECUTE\b', r'--', r'/\*', r'\*/', r';'
        ]
        text_upper = text.upper()
        for pattern in dangerous_patterns:
            if re.search(pattern, text_upper):
                return False
        return True

def validate_and_clean_input(input_type: str, value: str) -> Tuple[bool, str]:
    """Fonction principale de validation"""
    validator = InputValidator()
    
    if input_type == "team_id":
        clean = validator.sanitize_team_id(value)
        return bool(clean), clean
    
    elif input_type == "street_name":
        clean = validator.sanitize_street_name(value)
        if not validator.is_sql_safe(clean):
            return False, ""
        return bool(clean), clean
    
    elif input_type == "address":
        clean = validator.sanitize_address_number(value)
        return bool(clean), clean
    
    elif input_type == "note":
        clean = validator.sanitize_note(value)
        if not validator.is_sql_safe(clean):
            return False, ""
        return bool(clean), clean
    
    elif input_type == "sector":
        clean = validator.validate_sector(value)
        return True, clean
    
    elif input_type == "status":
        clean = validator.validate_status(value)
        return True, clean
    
    elif input_type == "password":
        valid, msg = validator.validate_password(value)
        return valid, value if valid else ""
    
    else:
        clean = validator.sanitize_text(value)
        return bool(clean), clean
# --------------------------------------------------------------------------------
# <<< END FILE: guignomap/validators.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/export_repo_audit.py
# --- encoding: utf-8 | size: 8182 bytes | sha256: c0af0706872933ae10a7e4c8110013717ef742684a41c1d8fe8d988bba331fd1 | mtime: 2025-09-15T18:06:45-04:00
# --------------------------------------------------------------------------------
# coding: utf-8
"""
Export d‚Äôaudit GuignoMap ‚Üí exports/export_audit_YYYYMMDD_HHMMSS.txt

Objectifs:
- Inclure 100% du contenu de TOUS les fichiers .py (src/, tests/, scripts/, guignomap/‚Ä¶)
- Inclure fichiers importants: config (toml/yaml/ini/json/sql), alembic.ini, migrations, requirements*, pyproject.toml, Dockerfile, .gitignore, README.md
- Exclure: backups, exports, caches, pycache, venv, .git, binaires, secrets (.streamlit/secrets.toml)
- Ajouter une section ENV avec: version Python, chemin ex√©cutable, plateforme, LISTE COMPLETE des paquets install√©s
Sortie: UTF-8 (LF), sans BOM
"""

from __future__ import annotations
from pathlib import Path
from datetime import datetime
import sys, platform, subprocess

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "exports"
OUTDIR.mkdir(parents=True, exist_ok=True)
OUTFILE = OUTDIR / f"export_audit_{datetime.now():%Y%m%d_%H%M%S}.txt"

# Dossiers √† exclure totalement
EXCLUDE_DIRS = {
    ".git", ".github", ".venv", "venv", "env", ".vscode", "__pycache__", ".pytest_cache", ".mypy_cache",
    "node_modules", "storage_local", "backups", "exports", "logs", ".idea", ".DS_Store"
}

# Fichiers exactement exclus (secrets etc.)
EXCLUDE_FILES = {
    "secrets.toml",  # ne jamais sortir les secrets
    ".python-version"
}

# Extensions autoris√©es pour les fichiers non-.py, consid√©r√©s ‚Äúimportants √† auditer‚Äù
ALLOW_NONPY_EXTS = {
    ".toml", ".ini", ".cfg", ".conf", ".yml", ".yaml", ".json", ".sql",
    ".md", ".txt", ".ps1", ".bat", ".sh", ".dockerignore"
}

# Fichiers/chemins ‚Äúimportants‚Äù accept√©s m√™me sans extension (ou sp√©cifiques)
ALLOW_SPECIAL_PATHS = {
    "alembic.ini",
    "Dockerfile",
    ".gitignore",
    ".env.example",
    ".streamlit/config.toml",
    "requirements.txt",
    "requirements-dev.txt",
    "pyproject.toml",
}

# Pr√©fixes utiles (whitelist de zones pertinentes)
ALLOW_PREFIXES = [
    "src/",
    "tests/",
    "scripts/",
    "guignomap/",
    "src/database/migrations/",
]

# Ne PAS limiter la taille des .py (on les veut ENTIEREMENT)
MAX_NONPY_SIZE = 200 * 1024  # 200KB pour les non-.py (pour √©viter blobs inutiles)

def is_excluded_path(p: Path) -> bool:
    for part in p.parts:
        if part in EXCLUDE_DIRS:
            return True
    return False

def is_allowed_file(p: Path) -> bool:
    if p.name in EXCLUDE_FILES:
        return False

    rel = p.relative_to(ROOT).as_posix()

    # .py ‚Üí toujours inclus s'il est dans une zone pertinente
    if p.suffix.lower() == ".py":
        if any(rel == pre or rel.startswith(pre) for pre in ALLOW_PREFIXES):
            return True
        return False

    # Fichiers sp√©ciaux explicitement autoris√©s
    if rel in ALLOW_SPECIAL_PATHS:
        return True

    # Fichiers non-.py: extension autoris√©e ET dans une zone pertinente
    if p.suffix.lower() in ALLOW_NONPY_EXTS:
        if any(rel == pre or rel.startswith(pre) for pre in ALLOW_PREFIXES):
            return True

    return False

def read_text_utf8(p: Path) -> str:
    try:
        txt = p.read_text(encoding="utf-8", errors="replace")
    except Exception as e:
        txt = f"<<ERREUR LECTURE {p}: {e}>>"
    return txt.replace("\r\n", "\n").replace("\r", "\n")

def collect_files() -> list[Path]:
    files: list[Path] = []
    for p in ROOT.rglob("*"):
        if not p.is_file():
            continue
        if is_excluded_path(p):
            continue
        if not is_allowed_file(p):
            continue

        # Taille: .py = aucune limite; autres = borne raisonnable
        if p.suffix.lower() != ".py":
            try:
                if p.stat().st_size > MAX_NONPY_SIZE:
                    continue
            except Exception:
                continue

        files.append(p)

    files.sort(key=lambda x: x.relative_to(ROOT).as_posix().lower())
    return files

def get_installed_packages() -> list[str]:
    """Retourne une liste 'Nom==Version' tri√©e, en √©vitant les attributs priv√©s."""
    # 1) Tentative avec importlib.metadata (propre)
    try:
        from importlib.metadata import distributions
        pkgs = []
        for dist in distributions():
            meta = getattr(dist, "metadata", None)
            # meta est un email.message.Message (supporte .get)
            name = None
            if meta:
                name = meta.get("Name")
            ver = getattr(dist, "version", None)
            if name and ver:
                pkgs.append(f"{name}=={ver}")
        if pkgs:
            return sorted(pkgs, key=lambda s: s.lower())
    except Exception:
        pass

    # 2) Fallback avec pkg_resources (setuptools)
    try:
        import pkg_resources  # type: ignore
        pkgs = [f"{d.project_name}=={d.version}" for d in pkg_resources.working_set]
        if pkgs:
            return sorted(set(pkgs), key=lambda s: s.lower())
    except Exception:
        pass

    # 3) Dernier recours: pip freeze (synchronis√© avec l‚Äôenvironnement courant)
    try:
        out = subprocess.check_output([sys.executable, "-m", "pip", "freeze"], text=True, encoding="utf-8")
        pkgs = [line.strip() for line in out.splitlines() if line.strip()]
        if pkgs:
            return sorted(pkgs, key=lambda s: s.lower())
    except Exception as e:
        return [f"<<ERREUR INVENTAIRE DEPENDANCES: {e}>>"]

    return []

def env_section() -> str:
    lines: list[str] = []
    lines.append("## ENVIRONNEMENT")
    lines.append(f"- Python : {sys.version.splitlines()[0]}")
    lines.append(f"- Ex√©cutable : {sys.executable}")
    lines.append(f"- Plateforme : {platform.platform()}")

    # versions utiles si pr√©sentes
    try:
        import streamlit as _st
        lines.append(f"- streamlit : {_st.__version__}")
    except Exception:
        pass
    try:
        import sqlalchemy as _sa
        lines.append(f"- sqlalchemy : {_sa.__version__}")
    except Exception:
        pass
    try:
        import pandas as _pd
        lines.append(f"- pandas : {_pd.__version__}")
    except Exception:
        pass
    try:
        import boto3 as _b3
        lines.append(f"- boto3 : {_b3.__version__}")
    except Exception:
        pass
    try:
        import passlib as _pl  # noqa
        lines.append("- passlib : pr√©sent")
    except Exception:
        pass

    lines.append("")
    lines.append("### D√©pendances install√©es (inventaire)")
    pkgs = get_installed_packages() or ["<<Aucune d√©pendance d√©tect√©e>>"]
    lines.extend(pkgs)
    lines.append("")
    return "\n".join(lines)

def main() -> int:
    files = collect_files()

    header = [
        "# GuignoMap ‚Äî Export d‚Äôaudit COMPLET (code et config utiles)",
        f"# Date : {datetime.now():%Y-%m-%d %H:%M:%S}",
        f"# Racine : {ROOT}",
        "# Contenu : 100% des .py (zones pertinentes) + fichiers de config/migrations essentiels",
        "# Exclus : backups, exports, caches, venv, .git, binaires, secrets (.streamlit/secrets.toml)",
        ""
    ]

    with OUTFILE.open("w", encoding="utf-8", newline="\n") as out:
        out.write("\n".join(header) + "\n")
        out.write(env_section() + "\n")

        out.write("## INDEX DES FICHIERS INCLUS\n")
        for p in files:
            out.write(f"- {p.relative_to(ROOT).as_posix()}\n")

        out.write("\n## CONTENU DES FICHIERS\n")
        for p in files:
            rel = p.relative_to(ROOT).as_posix()
            out.write(f"\n---8<--- {rel} BEGIN ---\n")
            out.write("```" + (p.suffix.lower().lstrip(".") or "txt") + "\n")
            out.write(read_text_utf8(p))
            out.write("\n```\n")
            out.write(f"---8<--- {rel} END ---\n")

        out.write("\n## NOTE\n- Secrets exclus par conception (ex: .streamlit/secrets.toml)\n")
        out.write("- Tous les .py des zones pertinentes sont inclus en int√©gralit√©.\n")

    print(f"‚úÖ Export d‚Äôaudit √©crit : {OUTFILE}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

# --------------------------------------------------------------------------------
# <<< END FILE: scripts/export_repo_audit.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/export_repo_min.py
# --- encoding: utf-8 | size: 0 bytes | sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 | mtime: 2025-09-15T18:01:24-04:00
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# <<< END FILE: scripts/export_repo_min.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/export_repo_snapshot.py
# --- encoding: utf-8 | size: 4180 bytes | sha256: 0c48e13e656b7ddc4e66c13f771f311eb37c8d886bfce9f698955a169ebbc747 | mtime: 2025-09-15T17:58:50-04:00
# --------------------------------------------------------------------------------
# coding: utf-8
"""
Export complet du code GuignoMap ‚Üí exports/export_full_YYYYMMDD_HHMMSS.txt
- UTF-8 (sans BOM), normalise les fins de lignes.
- Inclut le contenu des fichiers code/texte utiles.
- Exclut .git, .venv, caches, binaires, gros fichiers.
- Ne lit jamais .streamlit/secrets.toml (s√©curit√©).
"""

from __future__ import annotations
import sys, os, io, time
from pathlib import Path
from datetime import datetime

ROOT = Path(__file__).resolve().parents[1]
OUTDIR = ROOT / "exports"
OUTDIR.mkdir(parents=True, exist_ok=True)
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
OUTFILE = OUTDIR / f"export_full_{ts}.txt"

# Dossiers √† exclure
EXCLUDE_DIRS = {
    ".git", ".github", ".venv", ".vscode", "__pycache__", ".mypy_cache", ".pytest_cache",
    "node_modules", "dist", "build", "storage_local", ".idea"
}

# Fichiers √† exclure (par nom exact)
EXCLUDE_FILES = {
    "secrets.toml",  # ne jamais exposer des secrets
}

# Extensions √† inclure (code/texte)
INCLUDE_EXTS = {
    ".py", ".txt", ".md", ".ps1", ".bat", ".toml", ".ini", ".cfg",
    ".yml", ".yaml", ".json", ".sql"
}

# Extensions √† exclure d‚Äôoffice (binaires/pond√©reux)
BINARY_EXTS = {
    ".db", ".sqlite", ".sqlite3", ".pkl", ".zip", ".7z", ".rar", ".exe", ".dll",
    ".png", ".jpg", ".jpeg", ".gif", ".ico", ".pdf", ".parquet"
}

# Taille max (Ko) par fichier pour √©viter les √©normes blobs
SIZE_LIMIT_KB = 300  # ajuste si besoin

def should_skip(path: Path) -> bool:
    # Exclure par dossier
    for part in path.parts:
        if part in EXCLUDE_DIRS:
            return True
    # Exclure par extension binaire
    if path.suffix.lower() in BINARY_EXTS:
        return True
    # Exclure fichier secrets explicites
    if path.name in EXCLUDE_FILES:
        return True
    # Filtre d‚Äôextensions
    if path.suffix and path.suffix.lower() not in INCLUDE_EXTS:
        return True
    # Limite de taille
    try:
        if path.stat().st_size > SIZE_LIMIT_KB * 1024:
            return True
    except Exception:
        return True
    return False

def read_text_safely(path: Path) -> str:
    # Toujours lire en UTF-8 avec remplacement pour √©viter les caract√®res corrompus
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            txt = f.read()
    except Exception as e:
        txt = f"<<ERREUR LECTURE {path}: {e}>>"
    # Normaliser fins de lignes
    return txt.replace("\r\n", "\n").replace("\r", "\n")

def main() -> int:
    files: list[Path] = []
    for p in ROOT.rglob("*"):
        if not p.is_file():
            continue
        if should_skip(p):
            continue
        files.append(p)

    files.sort(key=lambda x: str(x).lower())

    header = f"""# GuignoMap - Export de code COMPLET
# Date : {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
# Racine : {ROOT}
# Fichiers inclus : {len(files)}
# Encodage : UTF-8 (sans BOM)
# R√®gles : exclusions .git/.venv/binaires/gros fichiers, NO secrets.toml
"""

    with open(OUTFILE, "w", encoding="utf-8", newline="\n") as out:
        out.write(header + "\n")
        out.write("## INDEX\n")
        for p in files:
            rel = p.relative_to(ROOT)
            out.write(f"- {rel.as_posix()}\n")

        out.write("\n## CONTENU DES FICHIERS\n")
        for p in files:
            rel = p.relative_to(ROOT)
            out.write("\n")
            out.write(f"---8<--- {rel.as_posix()} BEGIN ---\n")
            out.write("```" + f"{p.suffix.lower().lstrip('.') or 'txt'}" + "\n")
            out.write(read_text_safely(p))
            out.write("\n```\n")
            out.write(f"---8<--- {rel.as_posix()} END ---\n")

        out.write("\n## STATISTIQUES\n")
        total_bytes = sum((p.stat().st_size for p in files), 0)
        out.write(f"- Total fichiers export√©s : {len(files)}\n")
        out.write(f"- Poids cumul√© (approx) : {total_bytes/1024:.1f} Ko\n")
        out.write(f"- Limite par fichier : {SIZE_LIMIT_KB} Ko\n")

    print(f"‚úÖ Export √©crit : {OUTFILE}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())

# --------------------------------------------------------------------------------
# <<< END FILE: scripts/export_repo_snapshot.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/fix_app_types.py
# --- encoding: utf-8 | size: 3549 bytes | sha256: 5df5219e43124484182cfb28ab22b91573836f5b42ce7b53baa3b79e9525d9dd | mtime: 2025-09-15T17:26:29-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Script de migration automatique des types DataFrame vers List[Dict] dans app.py
Corrige les incompatibilit√©s entre l'ancien db.py et le nouveau db_v5.py
"""

import re
from pathlib import Path

def fix_app_py():
    """Corrige automatiquement les incompatibilit√©s de types dans app.py"""
    
    app_path = Path("guignomap/app.py")
    content = app_path.read_text(encoding='utf-8')
    
    # Corrections par regex
    corrections = [
        # 1. .empty sur listes -> len() == 0
        (r'if not (\w+)\.empty:', r'if \1:  # Liste non vide'),
        (r'if (\w+)\.empty:', r'if not \1:  # Liste vide'),
        
        # 2. .iterrows() -> enumerate() ou iteration directe
        (r'for _, (\w+) in (\w+)\.iterrows\(\):', r'for \1 in \2:'),
        (r'for (\w+), (\w+) in (\w+)\.iterrows\(\):', r'for \1, \2 in enumerate(\3):'),
        
        # 3. DataFrame['column'] -> list access pour unassigned
        (r"unassigned\['name'\]\.tolist\(\)", r"unassigned"),
        
        # 4. to_csv() sur listes -> DataFrame conversion
        (r'(\w+)\.to_csv\(([^)]+)\)', r'pd.DataFrame(\1).to_csv(\2)'),
        
        # 5. team_streets filtering (fonction get_team_streets retourne List[str])
        (r"team_streets\[team_streets\['status'\] == '(\w+)'\]", r"[s for s in team_streets if hasattr(s, 'status') and s.status == '\1']"),
        
        # 6. Acc√®s par index sur dictionnaires (notes)
        (r"note\[(\d+)\]", r"list(note.values())[\1] if isinstance(note, dict) else note[\1]"),
    ]
    
    for pattern, replacement in corrections:
        content = re.sub(pattern, replacement, content)
    
    # Corrections manuelles sp√©cifiques
    
    # Fix pour get_team_streets qui doit retourner les donn√©es compl√®tes, pas juste les noms
    content = content.replace(
        'def get_team_streets(team_id: str) -> List[str]:',
        'def get_team_streets(team_id: str) -> List[Dict[str, Any]]:'
    )
    
    # Fix pour l'utilisation de team_streets dans l'interface √©quipe
    content = re.sub(
        r'if team_streets\.empty:',
        'if not team_streets:',
        content
    )
    
    content = re.sub(
        r'done_streets = len\(team_streets\[team_streets\[\'status\'\] == \'terminee\'\]\)',
        'done_streets = len([s for s in team_streets if isinstance(s, dict) and s.get("status") == "terminee"])',
        content
    )
    
    content = re.sub(
        r'in_progress = len\(team_streets\[team_streets\[\'status\'\] == \'en_cours\'\]\)',
        'in_progress = len([s for s in team_streets if isinstance(s, dict) and s.get("status") == "en_cours"])',
        content
    )
    
    # Fix pour l'iteration sur team_streets
    content = re.sub(
        r'for street in team_streets:',
        'for street in team_streets:\n            if isinstance(street, str):\n                street_name = street\n            else:\n                street_name = street.get("name", street)',
        content
    )
    
    # Fix pour les notes dans l'affichage
    content = re.sub(
        r'st\.markdown\(f"‚Ä¢ \*\*#{note\[0\]}\*\* : {note\[1\]} _{note\[2\]}_"\)',
        'st.markdown(f"‚Ä¢ **#{note.get(\'address_number\', \'?\')}** : {note.get(\'comment\', \'\')} _{note.get(\'created_at\', \'\')}_ ")',
        content
    )
    
    # Sauvegarder
    app_path.write_text(content, encoding='utf-8')
    print("‚úÖ app.py corrig√© automatiquement")

if __name__ == "__main__":
    fix_app_py()
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/fix_app_types.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/fix_specific.py
# --- encoding: utf-8 | size: 1856 bytes | sha256: a9c610bf5d916e3aa29063491589d8ff3cd7bc32456f2906a6ae1077901ae47d | mtime: 2025-09-15T17:26:29-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Script de correction fine pour les types de retour dans app.py
"""

import re
from pathlib import Path

def fix_specific_issues():
    """Corrige les probl√®mes sp√©cifiques identifi√©s"""
    
    app_path = Path("guignomap/app.py")
    content = app_path.read_text(encoding='utf-8')
    
    # 1. Revert les corrections sur DataFrames (list_streets retourne bien un DataFrame)
    content = re.sub(
        r'if df_all:  # Liste non vide',
        'if not df_all.empty:',
        content
    )
    
    content = re.sub(
        r'if not df_team:  # Liste vide',
        'if df_team.empty:',
        content
    )
    
    # 2. Fix team_streets access patterns
    content = re.sub(
        r'done_streets = len\(\[s for s in team_streets if hasattr\(s, \'status\'\) and s\.status == \'terminee\'\]\)',
        'done_streets = len([s for s in team_streets if s.get("status") == "terminee"])',
        content
    )
    
    content = re.sub(
        r'in_progress = len\(\[s for s in team_streets if hasattr\(s, \'status\'\) and s\.status == \'en_cours\'\]\)',
        'in_progress = len([s for s in team_streets if s.get("status") == "en_cours"])',
        content
    )
    
    # 3. Fix row access in iterrows (certains endroits ont encore des DataFrames)
    content = re.sub(
        r"for row in df_team:\s*street = row\['name'\]\s*status = row\['status'\]\s*notes_count = row\.get\('notes', 0\)",
        """for _, row in df_team.iterrows():
            street = row['name']
            status = row['status'] 
            notes_count = row.get('notes', 0)""",
        content, flags=re.MULTILINE
    )
    
    app_path.write_text(content, encoding='utf-8')
    print("‚úÖ Corrections sp√©cifiques appliqu√©es")

if __name__ == "__main__":
    fix_specific_issues()
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/fix_specific.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/generate_audit_export.py
# --- encoding: utf-8 | size: 8675 bytes | sha256: 44f4627935aa28ba2d16e7ac5f25dab55bf9f5ddab6db55643d03478981af357 | mtime: 2025-09-16T10:37:52-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Script de g√©n√©ration d'export complet pour audit GuignoMap
G√©n√®re un fichier export_audit_complet.txt avec tous les fichiers demand√©s
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import subprocess

# Configuration
OUTPUT_FILE = "export_audit_complet.txt"
PROJECT_ROOT = Path(__file__).parent.parent  # Remonter au niveau du projet

# Liste des fichiers demand√©s pour l'audit
FICHIERS_AUDIT = [
    # Configuration
    ".streamlit/config.toml",
    
    # Styles
    "guignomap/assets/styles.css",
    
    # Application principale
    "guignomap/app.py",
    "guignomap/__init__.py",
    "guignomap/backup.py",  # corrig√© de "nackup.py"
    "guignomap/db_v5.py",
    "guignomap/db.py",
    "guignomap/osm.py",
    "guignomap/reports.py",
    "guignomap/validators.py",
    
    # Scripts
    "scripts/fix_app_types.py",
    "scripts/fix_specific.py",
    "scripts/migrate_password_hashes.py",
    "scripts/migrate_sqlite_to_postgres.py",
    "scripts/sanity_db_pandas.py",
    "scripts/smoke_create_map.py",
    
    # Source
    "src/auth/passwords.py",
    "src/database/migrations/env.py",
    "src/database/connection.py",
    "src/database/db_v5.py",  # corrig√© de "dv_v5.py"
    "src/database/models.py",
    "src/storage/__init__.py",
    "src/storage/cloud.py",
    "src/storage/local.py",
    "src/utils/__init__.py",
    "src/utils/adapters.py",
    "src/__init__.py",
    "src/config.py",
    
    # Tools
    "tools/quick_sanity.py",
    
    # Configuration syst√®me
    ".gitignore",
    "requirements_freeze.txt",
    "requirements.txt",
    "streamlit_app.py",
]

def get_system_info():
    """R√©cup√®re les informations syst√®me"""
    try:
        python_version = subprocess.check_output([sys.executable, "--version"], text=True).strip()
        pip_list = subprocess.check_output([sys.executable, "-m", "pip", "list"], text=True)
        git_commit = subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
        git_branch = subprocess.check_output(["git", "branch", "--show-current"], text=True).strip()
        
        return {
            "python_version": python_version,
            "pip_list": pip_list,
            "git_commit": git_commit,
            "git_branch": git_branch,
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    except Exception as e:
        return {"error": str(e)}

def read_file_safe(file_path):
    """Lit un fichier de mani√®re s√©curis√©e"""
    try:
        full_path = PROJECT_ROOT / file_path
        if not full_path.exists():
            return f"‚ùå FICHIER NON TROUV√â: {file_path}"
        
        with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
        
        # D√©terminer l'extension pour le formatage
        suffix = full_path.suffix.lower()
        if suffix in ['.py']:
            lang = 'python'
        elif suffix in ['.css']:
            lang = 'css'
        elif suffix in ['.toml']:
            lang = 'toml'
        elif suffix in ['.json']:
            lang = 'json'
        elif suffix in ['.txt', '.md']:
            lang = 'text'
        else:
            lang = 'text'
        
        lines_count = len(content.splitlines())
        size_kb = len(content.encode('utf-8')) / 1024
        
        return f"""## {file_path} ({lines_count} lignes, {size_kb:.1f}KB)
```{lang}
{content}
```
"""
    except Exception as e:
        return f"‚ùå ERREUR lecture {file_path}: {str(e)}\n"

def generate_tree():
    """G√©n√®re un arbre optimis√© du projet"""
    tree_items = []
    
    for item in FICHIERS_AUDIT:
        path = Path(item)
        tree_items.append(f"‚îú‚îÄ‚îÄ {item}")
    
    return "\n".join(tree_items)

def main():
    """Fonction principale"""
    print(f"üöÄ G√©n√©ration de l'export d'audit pour GuignoMap...")
    
    # Obtenir les infos syst√®me
    sys_info = get_system_info()
    
    # Commencer le fichier
    content = f"""# ================================================================================
# GUIGNOMAP - EXPORT COMPLET POUR AUDIT
# Date de g√©n√©ration: {sys_info.get('date', 'Inconnue')}
# Python: {sys_info.get('python_version', 'Inconnu')}
# Git: {sys_info.get('git_branch', 'main')} ({sys_info.get('git_commit', 'unknown')[:8]})
# ================================================================================

# ================================================================================
# ARBRE DU PROJET (Fichiers d'audit)
# ================================================================================

GuignoMap/
{generate_tree()}

# ================================================================================
# INFORMATIONS SYST√àME
# ================================================================================

{sys_info.get('pip_list', 'Packages non disponibles')}

# ================================================================================
# FICHIERS COMPLETS
# ================================================================================

"""
    
    # Lire tous les fichiers
    print(f"üìÅ Lecture de {len(FICHIERS_AUDIT)} fichiers...")
    
    fichiers_lus = 0
    fichiers_manquants = []
    
    for fichier in FICHIERS_AUDIT:
        print(f"  üìÑ {fichier}")
        file_content = read_file_safe(fichier)
        content += file_content + "\n"
        
        if "‚ùå" in file_content:
            fichiers_manquants.append(fichier)
        else:
            fichiers_lus += 1
    
    # Ajouter un r√©sum√© final
    content += f"""
# ================================================================================
# R√âSUM√â DE L'EXPORT
# ================================================================================

‚úÖ Fichiers lus avec succ√®s: {fichiers_lus}/{len(FICHIERS_AUDIT)}
‚ùå Fichiers manquants: {len(fichiers_manquants)}

"""
    
    if fichiers_manquants:
        content += "FICHIERS MANQUANTS:\n"
        for fichier in fichiers_manquants:
            content += f"  - {fichier}\n"
        content += "\n"
    
    # Ajouter les fichiers JSON recommand√©s
    json_files = [
        "guignomap/osm_addresses.json",
        "guignomap/osm_cache.json",
        "guignomap/backups/backup_log.json"
    ]
    
    content += "# ================================================================================\n"
    content += "# FICHIERS JSON ADDITIONNELS (Recommand√©s pour audit)\n"
    content += "# ================================================================================\n\n"
    
    for json_file in json_files:
        content += read_file_safe(json_file) + "\n"
    
    content += f"""
# ================================================================================
# RECOMMANDATIONS FINALES POUR L'AUDIT
# ================================================================================

üîç POINTS DE V√âRIFICATION PRIORITAIRES:
1. S√©curit√©: V√©rifier src/auth/passwords.py et guignomap/validators.py
2. Base de donn√©es: Examiner src/database/ et guignomap/db_v5.py
3. Configuration: Valider src/config.py et .streamlit/config.toml
4. Int√©grations: Contr√¥ler guignomap/osm.py et src/storage/
5. Interface: R√©viser guignomap/app.py (2000+ lignes)

‚ö†Ô∏è FICHIERS SENSIBLES:
- src/auth/passwords.py (authentification Argon2)
- src/config.py (gestion des secrets)
- guignomap/validators.py (validation anti-XSS)
- src/database/connection.py (pool PostgreSQL)

üìä M√âTRIQUES:
- Codebase: ~10,000 lignes Python
- D√©pendances: 50+ packages
- Architecture: Streamlit + PostgreSQL + Folium
- D√©ploiement: Streamlit Cloud compatible

===============================================================================
FIN DU RAPPORT D'EXPORT POUR AUDIT - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
===============================================================================
"""
    
    # √âcrire le fichier final
    output_path = PROJECT_ROOT / OUTPUT_FILE
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"‚úÖ Export g√©n√©r√©: {output_path}")
    print(f"üìä Fichiers trait√©s: {fichiers_lus}/{len(FICHIERS_AUDIT)}")
    print(f"üìÑ Taille: {len(content.encode('utf-8')) / 1024:.1f}KB")
    
    if fichiers_manquants:
        print(f"‚ö†Ô∏è  {len(fichiers_manquants)} fichiers manquants (voir d√©tails dans l'export)")

if __name__ == "__main__":
    main()
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/generate_audit_export.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/generate_audit_optimise.py
# --- encoding: utf-8 | size: 14796 bytes | sha256: 1a01fa907b10b57a43f1358ed71e005981168a09ebb21855690ddb8cdac7b7ff | mtime: 2025-09-16T11:08:45-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Script de g√©n√©ration d'export optimis√© pour audit GuignoMap
Version all√©g√©e - SANS backups, __pycache__, logs, etc.
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import subprocess

# Configuration
OUTPUT_FILE = "export_audit_optimise.txt"
PROJECT_ROOT = Path(__file__).parent.parent

# Liste OPTIMIS√âE des fichiers essentiels pour l'audit
FICHIERS_AUDIT_ESSENTIELS = [
    # Configuration principale
    ".streamlit/config.toml",
    ".streamlit/secrets.toml.example",  # Template secrets
    "streamlit_app.py",
    "requirements.txt",
    "requirements_freeze.txt",
    ".gitignore",
    "runtime.txt",                      # Version Python Streamlit Cloud
    "alembic.ini",                      # Configuration Alembic
    
    # Application principale (SEULEMENT les fichiers sources)
    "guignomap/__init__.py",
    "guignomap/app.py",
    "guignomap/db_v5.py",
    "guignomap/db.py",                  # Legacy DB (pour migration)
    "guignomap/backup.py",              # Syst√®me de sauvegarde
    "guignomap/osm.py",
    "guignomap/validators.py", 
    "guignomap/reports.py",
    "guignomap/assets/styles.css",
    
    # Architecture source
    "src/__init__.py",
    "src/config.py",
    "src/auth/passwords.py",
    "src/database/connection.py",
    "src/database/models.py",
    "src/database/db_v5.py",            # V√©rifier si diff√©rent de guignomap/db_v5.py
    "src/storage/__init__.py",          # Init module storage
    "src/storage/cloud.py",
    "src/storage/local.py",
    "src/utils/__init__.py",            # Init module utils
    "src/utils/adapters.py",
    
    # Scripts utilitaires essentiels
    "scripts/sanity_db_pandas.py",
    "scripts/smoke_create_map.py",
    "scripts/migrate_password_hashes.py",    # Migration Argon2
    "scripts/migrate_sqlite_to_postgres.py", # Migration DB
    "tools/quick_sanity.py",
    
    # Migration Alembic
    "src/database/migrations/env.py",
    "src/database/migrations/script.py.mako",  # Template migration
    
    # Tests essentiels
    "tests/manual/test_db_connection.py",
    "tests/manual/test_db_simple.py",
    
    # Configuration d√©veloppement
    ".vscode/tasks.json",               # T√¢ches automatis√©es
    ".devcontainer/devcontainer.json",  # Configuration Dev Container
]

# Fichiers √† EXCLURE explicitement 
FICHIERS_A_EXCLURE = [
    "__pycache__",
    ".venv",
    "backups",
    "logs", 
    "*.log",
    "*.pyc",
    "*.pyo",
    "*.db",
    "*.sqlite",
    "*.zip",
    "node_modules",
    ".git",
    "exports/export_*.txt",  # √âviter les anciens exports
]

def get_system_info_minimal():
    """R√©cup√®re les informations syst√®me essentielles"""
    try:
        python_version = subprocess.check_output([sys.executable, "--version"], text=True).strip()
        
        # Packages essentiels seulement
        essential_packages = [
            "streamlit", "pandas", "folium", "sqlalchemy", "psycopg2-binary", 
            "passlib", "boto3", "reportlab", "xlsxwriter", "overpy"
        ]
        
        pip_output = subprocess.check_output([sys.executable, "-m", "pip", "list"], text=True)
        essential_pip = []
        for line in pip_output.split('\n'):
            for pkg in essential_packages:
                if line.lower().startswith(pkg.lower()):
                    essential_pip.append(line)
                    break
        
        git_commit = subprocess.check_output(["git", "rev-parse", "HEAD"], text=True).strip()
        git_branch = subprocess.check_output(["git", "branch", "--show-current"], text=True).strip()
        
        return {
            "python_version": python_version,
            "essential_packages": "\n".join(essential_pip),
            "git_commit": git_commit,
            "git_branch": git_branch,
            "date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }
    except Exception as e:
        return {"error": str(e)}

def read_file_safe(file_path):
    """Lit un fichier de mani√®re s√©curis√©e avec limitation de taille"""
    try:
        full_path = PROJECT_ROOT / file_path
        if not full_path.exists():
            return None
        
        # Limiter la taille des fichiers lus (max 500KB)
        size = full_path.stat().st_size
        if size > 500 * 1024:  # 500KB max
            return f"## {file_path} (FICHIER TROP VOLUMINEUX - {size/1024:.1f}KB)\n‚ùå Fichier non inclus (> 500KB)\n\n"
        
        # Essayer plusieurs encodages pour √©viter les caract√®res corrompus
        encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']
        content = None
        
        for encoding in encodings:
            try:
                with open(full_path, 'r', encoding=encoding) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            # En dernier recours, mode binaire et d√©codage manuel
            with open(full_path, 'rb') as f:
                raw_content = f.read()
                content = raw_content.decode('utf-8', errors='ignore')
        
        # D√©terminer l'extension pour le formatage
        suffix = full_path.suffix.lower()
        if suffix in ['.py']:
            lang = 'python'
        elif suffix in ['.css']:
            lang = 'css'
        elif suffix in ['.toml']:
            lang = 'toml'
        elif suffix in ['.json']:
            lang = 'json'
        elif suffix in ['.txt', '.md']:
            lang = 'text'
        else:
            lang = 'text'
        
        lines_count = len(content.splitlines())
        size_kb = len(content.encode('utf-8')) / 1024
        
        return f"""## {file_path} ({lines_count} lignes, {size_kb:.1f}KB)
```{lang}
{content}
```

"""
    except Exception as e:
        return f"‚ùå ERREUR lecture {file_path}: {str(e)}\n"

def main():
    """Fonction principale optimis√©e"""
    print(f"üöÄ G√©n√©ration de l'export d'audit OPTIMIS√â pour GuignoMap...")
    
    # Obtenir les infos syst√®me
    sys_info = get_system_info_minimal()
    
    # Commencer le fichier
    content = f"""# ================================================================================
# GUIGNOMAP - EXPORT OPTIMIS√â POUR AUDIT
# Date: {sys_info.get('date', 'Inconnue')}
# Python: {sys_info.get('python_version', 'Inconnu')}
# Git: {sys_info.get('git_branch', 'main')} ({sys_info.get('git_commit', 'unknown')[:8]})
# ================================================================================

# ================================================================================
# PACKAGES ESSENTIELS
# ================================================================================

{sys_info.get('essential_packages', 'Packages non disponibles')}

# ================================================================================
# STRUCTURE PROJET (Fichiers essentiels seulement)
# ================================================================================

GuignoMap/
‚îú‚îÄ‚îÄ .devcontainer/devcontainer.json     # üîß Configuration Dev Container  
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îú‚îÄ‚îÄ config.toml                     # ‚≠ê Configuration Streamlit
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml.example            # üìã Template secrets
‚îú‚îÄ‚îÄ .vscode/tasks.json                  # üîß T√¢ches automatis√©es VS Code
‚îú‚îÄ‚îÄ streamlit_app.py                    # ‚≠ê Point d'entr√©e Cloud
‚îú‚îÄ‚îÄ requirements.txt                    # üì¶ D√©pendances production
‚îú‚îÄ‚îÄ requirements_freeze.txt             # üì¶ Versions exactes
‚îú‚îÄ‚îÄ runtime.txt                         # üêç Version Python (Cloud)
‚îú‚îÄ‚îÄ alembic.ini                         # üîß Configuration Alembic
‚îú‚îÄ‚îÄ .gitignore                          # üö´ Exclusions Git
‚îÇ
‚îú‚îÄ‚îÄ guignomap/                          # üéØ Application principale
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ app.py                          # ‚≠ê Interface Streamlit (2000+ lignes)
‚îÇ   ‚îú‚îÄ‚îÄ db_v5.py                        # ‚≠ê Base de donn√©es SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ db.py                           # üîÑ Legacy DB (migration)
‚îÇ   ‚îú‚îÄ‚îÄ backup.py                       # üíæ Syst√®me de sauvegarde
‚îÇ   ‚îú‚îÄ‚îÄ osm.py                          # ‚≠ê Int√©gration OpenStreetMap
‚îÇ   ‚îú‚îÄ‚îÄ validators.py                   # üõ°Ô∏è Validation s√©curis√©e
‚îÇ   ‚îú‚îÄ‚îÄ reports.py                      # üìä G√©n√©ration rapports
‚îÇ   ‚îî‚îÄ‚îÄ assets/styles.css               # üé® Styles CSS
‚îÇ
‚îú‚îÄ‚îÄ src/                                # üèóÔ∏è Architecture modulaire
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                     # Init module principal
‚îÇ   ‚îú‚îÄ‚îÄ config.py                       # ‚≠ê Configuration centralis√©e
‚îÇ   ‚îú‚îÄ‚îÄ auth/passwords.py               # üõ°Ô∏è Authentification Argon2
‚îÇ   ‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection.py               # ‚≠ê Connexions PostgreSQL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                   # ‚≠ê Mod√®les SQLAlchemy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db_v5.py                    # üîÑ Op√©rations DB (duplicate?)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ migrations/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ env.py                  # Configuration Alembic
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ script.py.mako          # Template migration
‚îÇ   ‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                 # Init module storage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cloud.py                    # ‚òÅÔ∏è Stockage S3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ local.py                    # üíª Stockage local
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py                 # Init module utils
‚îÇ       ‚îî‚îÄ‚îÄ adapters.py                 # üîÑ Adaptateurs donn√©es
‚îÇ
‚îú‚îÄ‚îÄ scripts/                            # üîß Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ sanity_db_pandas.py             # üîç V√©rification DB
‚îÇ   ‚îú‚îÄ‚îÄ smoke_create_map.py             # üó∫Ô∏è Test carte
‚îÇ   ‚îú‚îÄ‚îÄ migrate_password_hashes.py      # üîê Migration Argon2
‚îÇ   ‚îî‚îÄ‚îÄ migrate_sqlite_to_postgres.py   # üîÑ Migration PostgreSQL
‚îÇ
‚îú‚îÄ‚îÄ tests/manual/                       # üß™ Tests manuels
‚îÇ   ‚îú‚îÄ‚îÄ test_db_connection.py           # Test connexion DB
‚îÇ   ‚îî‚îÄ‚îÄ test_db_simple.py               # Test simple DB
‚îÇ
‚îî‚îÄ‚îÄ tools/
    ‚îî‚îÄ‚îÄ quick_sanity.py                 # ‚ö° V√©rification rapide

# ================================================================================
# FICHIERS SOURCES COMPLETS
# ================================================================================

"""
    
    # Lire tous les fichiers essentiels
    print(f"üìÅ Lecture de {len(FICHIERS_AUDIT_ESSENTIELS)} fichiers essentiels...")
    
    fichiers_lus = 0
    fichiers_manquants = []
    taille_totale = 0
    
    for fichier in FICHIERS_AUDIT_ESSENTIELS:
        print(f"  üìÑ {fichier}")
        file_content = read_file_safe(fichier)
        
        if file_content is None:
            fichiers_manquants.append(fichier)
            continue
        elif "‚ùå" in file_content and "ERREUR" in file_content:
            fichiers_manquants.append(fichier)
            content += file_content
        else:
            content += file_content
            fichiers_lus += 1
            taille_totale += len(file_content.encode('utf-8'))
    
    # Ajouter JSON essentiels (s'ils existent et sont raisonnables)
    json_essentiels = ["guignomap/osm_addresses.json"]  # Seulement celui-ci
    
    content += """# ================================================================================
# DONN√âES JSON ESSENTIELLES
# ================================================================================

"""
    
    for json_file in json_essentiels:
        json_content = read_file_safe(json_file)
        if json_content and "‚ùå" not in json_content:
            content += json_content
    
    # R√©sum√© final optimis√©
    content += f"""
# ================================================================================
# R√âSUM√â DE L'EXPORT OPTIMIS√â
# ================================================================================

‚úÖ Fichiers inclus: {fichiers_lus}/{len(FICHIERS_AUDIT_ESSENTIELS)}
üìä Taille totale: {taille_totale/1024:.1f}KB
üö´ Exclusions: __pycache__, backups, logs, .venv, *.pyc, *.db

"""
    
    if fichiers_manquants:
        content += f"‚ö†Ô∏è Fichiers non trouv√©s ({len(fichiers_manquants)}):\n"
        for fichier in fichiers_manquants:
            content += f"  - {fichier}\n"
        content += "\n"
    
    content += f"""
# ================================================================================
# POINTS CL√âS POUR L'AUDIT
# ================================================================================

üîç S√âCURIT√â (Priorit√© 1):
- src/auth/passwords.py          # Hachage Argon2
- guignomap/validators.py        # Anti-XSS/injection
- src/config.py                  # Gestion secrets

üìä BASE DE DONN√âES (Priorit√© 2):  
- guignomap/db_v5.py            # ORM SQLAlchemy
- src/database/connection.py     # Pool PostgreSQL
- src/database/models.py         # Sch√©ma DB

üé® INTERFACE (Priorit√© 3):
- guignomap/app.py              # Interface Streamlit
- guignomap/osm.py              # Cartes OpenStreetMap
- streamlit_app.py              # Point d'entr√©e

‚öôÔ∏è CONFIGURATION:
- .streamlit/config.toml        # Th√®me et config
- requirements.txt              # D√©pendances

ARCHITECTURE: Streamlit + PostgreSQL + Folium + S3
D√âPLOIEMENT: Compatible Streamlit Cloud
VERSION: v3.0 Production

===============================================================================
FIN EXPORT OPTIMIS√â - {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
===============================================================================
"""
    
    # √âcrire le fichier final
    output_path = PROJECT_ROOT / OUTPUT_FILE
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    final_size = len(content.encode('utf-8')) / 1024
    print(f"‚úÖ Export optimis√© g√©n√©r√©: {output_path}")
    print(f"üìä Fichiers inclus: {fichiers_lus}/{len(FICHIERS_AUDIT_ESSENTIELS)}")
    print(f"üìÑ Taille finale: {final_size:.1f}KB")
    print(f"üö´ Exclusions: __pycache__, backups, logs, .venv")
    
    if fichiers_manquants:
        print(f"‚ö†Ô∏è  {len(fichiers_manquants)} fichiers non trouv√©s")

if __name__ == "__main__":
    main()
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/generate_audit_optimise.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/generate_export_txt.py
# --- encoding: utf-8 | size: 9571 bytes | sha256: e29946e3f2cc9ccf7a0c6bc61d08936bb13dce4d36a34eaab37f4d98a025208d | mtime: 2025-09-16T11:17:22-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
# scripts/generate_export_txt.py
from __future__ import annotations
import argparse
import datetime as dt
import hashlib
import os
import platform
import re
import subprocess
import sys
from pathlib import Path
from typing import Iterable

# --- R√©glages par d√©faut -------------------------------------------------------

EXCLUDE_DIRS = {
    ".git", ".venv", "__pycache__", "backups", "exports", "logs",
}
EXCLUDE_GLOBS = ["*.pyc", "*.pyo", "*.pyd", "*.log", "*.db", "*.sqlite"]
INCLUDE_EXTS = {".py"}  # On n'exporte en entier que les .py (conforme √† ta demande)

# ------------------------------------------------------------------------------

def repo_root_from_this_script() -> Path:
    p = Path(__file__).resolve()
    # si le script est dans .../scripts/ on remonte d'un cran
    return p.parents[1] if p.parent.name == "scripts" else p.parent

def is_wsl() -> bool:
    try:
        return "microsoft" in platform.release().lower() or "WSL" in platform.version()
    except Exception:
        return False

def run(cmd: list[str]) -> tuple[int, str]:
    try:
        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, text=True)
        return 0, out.strip()
    except subprocess.CalledProcessError as e:
        return e.returncode, e.output.strip()
    except FileNotFoundError:
        return 127, ""

def sha256_bytes(b: bytes) -> str:
    return hashlib.sha256(b).hexdigest()

def should_skip(path: Path, root: Path) -> bool:
    # dossiers exclus (par nom de segment)
    for part in path.relative_to(root).parts[:-1]:
        if part in EXCLUDE_DIRS:
            return True
    # patterns fichier exclus
    for pat in EXCLUDE_GLOBS:
        if path.match(pat):
            return True
    return False

def iter_files(root: Path, include_exts: set[str]) -> Iterable[Path]:
    for p in root.rglob("*"):
        if not p.is_file():
            continue
        if should_skip(p, root):
            continue
        if p.suffix.lower() in include_exts:
            yield p

def read_text_any(p: Path) -> tuple[str, str]:
    # retourne (texte, encodage_utilis√©)
    for enc in ("utf-8", "utf-8-sig", "latin-1"):
        try:
            t = p.read_text(encoding=enc, errors="strict")
            return t, enc
        except Exception:
            continue
    # fallback : remplacer erreurs
    return p.read_text(encoding="utf-8", errors="replace"), "utf-8(replace)"

def gather_env_block(root: Path) -> str:
    now = dt.datetime.now().astimezone()
    py_ver = sys.version.split()[0]
    pip_rc, pip_ver = run([sys.executable, "-m", "pip", "--version"])
    pip_info = pip_ver if pip_rc == 0 else "(pip non disponible)"
    plat = platform.platform()
    machine = platform.machine()
    wsl_flag = "oui" if is_wsl() else "non"

    # Info git (si dispo)
    git_lines = []
    rc, branch = run(["git", "-C", str(root), "rev-parse", "--abbrev-ref", "HEAD"])
    if rc == 0:
        git_lines.append(f"Branche courante : {branch}")
    rc, commit = run(["git", "-C", str(root), "rev-parse", "--short", "HEAD"])
    if rc == 0:
        git_lines.append(f"Commit : {commit}")
    rc, status = run(["git", "-C", str(root), "status", "--porcelain=2", "-b"])
    if rc == 0 and status:
        git_lines.append("Status :")
        git_lines.append(status)

    env = []
    env.append("# ================================================================================")
    env.append("# ENVIRONNEMENT & VERSIONS")
    env.append("# ================================================================================")
    env.append(f"Horodatage local : {now.isoformat(timespec='seconds')}")
    env.append(f"Racine projet    : {root}")
    env.append(f"Python           : {py_ver}")
    env.append(f"Pip              : {pip_info}")
    env.append(f"OS/Platform      : {plat}  ({machine})")
    env.append(f"WSL              : {wsl_flag}")
    if git_lines:
        env.append("Git              :")
        env.extend(f"  {line}" for line in git_lines)
    return "\n".join(env) + "\n\n"

def gather_dependencies_block() -> str:
    rc, freeze = run([sys.executable, "-m", "pip", "freeze"])
    header = []
    header.append("# ================================================================================")
    header.append("# D√âPENDANCES INSTALL√âES (pip freeze)")
    header.append("# ================================================================================")
    if rc == 0:
        body = freeze if freeze.strip() else "(aucune sortie)"
    else:
        body = "(Impossible d'ex√©cuter 'pip freeze')"
    return "\n".join(header) + "\n" + body + "\n\n"

def file_header(path: Path, root: Path, enc: str, content_bytes: bytes) -> str:
    rel = path.relative_to(root).as_posix()
    stat = path.stat()
    mtime = dt.datetime.fromtimestamp(stat.st_mtime).astimezone().isoformat(timespec="seconds")
    size = stat.st_size
    digest = sha256_bytes(content_bytes)
    return (
        "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
        f"# >>> BEGIN FILE: {rel}\n"
        f"# --- encoding: {enc} | size: {size} bytes | sha256: {digest} | mtime: {mtime}\n"
        "# --------------------------------------------------------------------------------\n"
    )

def file_footer(path: Path, root: Path) -> str:
    rel = path.relative_to(root).as_posix()
    return (
        "\n# --------------------------------------------------------------------------------\n"
        f"# <<< END FILE: {rel}\n"
        "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\n"
    )

def gather_sources_block(root: Path, include_exts: set[str]) -> str:
    header = []
    header.append("# ================================================================================")
    header.append("# SOURCES PYTHON COMPLETS (fichiers .py)")
    header.append("#   Exclusions: __pycache__, .venv, backups/, exports/, .git, logs/, *.pyc, *.log, *.db, *.sqlite")
    header.append("# ================================================================================")
    out = "\n".join(header) + "\n\n"
    files = sorted(iter_files(root, include_exts), key=lambda p: p.as_posix().lower())
    out_parts = [out]
    count = 0
    for p in files:
        text, enc = read_text_any(p)
        content_bytes = text.encode("utf-8", errors="replace")
        out_parts.append(file_header(p, root, enc, content_bytes))
        # normaliser les fins de ligne pour la lisibilit√©
        normalized = re.sub(r"\r\n?", "\n", text)
        out_parts.append(normalized)
        out_parts.append(file_footer(p, root))
        count += 1

    summary = (
        "# ================================================================================"
        "\n# R√âSUM√â SOURCES"
        f"\n#   Fichiers .py inclus : {count}"
        "\n# ================================================================================"
        "\n\n"
    )
    out_parts.append(summary)
    return "".join(out_parts)

def tail_placeholder_for_tree() -> str:
    return (
        "# ================================================================================"
        "\n# √Ä COLLER : ARBRE DU PROJET (TREE)"
        "\n# -------------------------------------------------------------------------------"
        "\n# Colle ici exactement l'arbre que tu as g√©n√©r√© (avec les exclusions)."
        "\n# ================================================================================"
        "\n"
    )

def main():
    parser = argparse.ArgumentParser(description="G√©n√®re un export.txt d'audit (env + deps + sources .py).")
    parser.add_argument("--root", type=str, default=None, help="Racine du projet (par d√©faut: auto)")
    parser.add_argument("--outfile", type=str, default=None, help="Chemin du fichier export (par d√©faut: racine/GuignoMap_code_export_YYYYMMDD_HHMMSS_audit.txt)")
    args = parser.parse_args()

    # D√©terminer la racine
    if args.root:
        root = Path(args.root).resolve()
    else:
        # si lanc√© depuis scripts/, remonte d'un niveau; sinon cwd
        try:
            root = repo_root_from_this_script()
        except Exception:
            root = Path.cwd().resolve()

    if not root.exists():
        print(f"[ERREUR] Racine introuvable: {root}", file=sys.stderr)
        sys.exit(2)

    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    default_name = f"GuignoMap_code_export_{ts}_audit.txt"
    outfile = Path(args.outfile).resolve() if args.outfile else (root / default_name)

    # Construire le contenu
    parts = []
    parts.append("# ================================================================================\n")
    parts.append("# EXPORT AUDIT - GUIGNOMAP\n")
    parts.append("# Contenu: ENVIRONNEMENT, D√âPENDANCES (pip freeze), SOURCES .py COMPLETS\n")
    parts.append("# ================================================================================\n\n")
    parts.append(gather_env_block(root))
    parts.append(gather_dependencies_block())
    parts.append(gather_sources_block(root, INCLUDE_EXTS))
    parts.append(tail_placeholder_for_tree())

    text = "".join(parts)

    # √âcriture (UTF-8, sans BOM)
    outfile.write_text(text, encoding="utf-8", errors="strict")

    print(f"[OK] Export cr√©√© : {outfile}")

if __name__ == "__main__":
    main()

# --------------------------------------------------------------------------------
# <<< END FILE: scripts/generate_export_txt.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/migrate_password_hashes.py
# --- encoding: utf-8 | size: 9957 bytes | sha256: d1e05eab697df11c9759c24ee6338ac099f1d8939eddcf69dbce74089b7bfadd | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
"""
Script de migration des hashes de mots de passe bcrypt ‚Üí Argon2
Migration paresseuse : les anciens hashes bcrypt sont migr√©s lors de la prochaine connexion
"""
import sys
import sqlite3
from pathlib import Path
from datetime import datetime

# Ajouter le r√©pertoire parent au PYTHONPATH pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from src.auth.passwords import get_password_hash_info, is_bcrypt_hash, is_argon2_hash


def get_sqlite_connection():
    """Connexion √† la base SQLite existante"""
    sqlite_path = Path(__file__).parent.parent / "guignomap" / "guigno_map.db"
    if not sqlite_path.exists():
        print(f"‚ùå Base SQLite non trouv√©e: {sqlite_path}")
        return None
    
    conn = sqlite3.connect(str(sqlite_path))
    conn.row_factory = sqlite3.Row
    return conn


def analyze_password_hashes():
    """
    Analyse des hashes de mots de passe dans la base
    Identifie les √©quipes avec des hashes bcrypt qui n√©cessitent une migration
    """
    conn = get_sqlite_connection()
    if not conn:
        return
    
    try:
        print("üîç Analyse des hashes de mots de passe...")
        print("=" * 50)
        
        # R√©cup√©rer toutes les √©quipes
        cursor = conn.execute("SELECT id, name, password_hash, created_at FROM teams ORDER BY id")
        teams = cursor.fetchall()
        
        if not teams:
            print("‚ÑπÔ∏è Aucune √©quipe trouv√©e dans la base")
            return
        
        bcrypt_count = 0
        argon2_count = 0
        unknown_count = 0
        
        print(f"{'√âquipe':<15} {'Algorithme':<10} {'Statut':<20} {'Cr√©√© le'}")
        print("-" * 65)
        
        for team in teams:
            team_id = team['id']
            name = team['name']
            hash_value = team['password_hash']
            created_at = team['created_at']
            
            # Analyser le hash
            hash_info = get_password_hash_info(hash_value)
            algorithm = hash_info['algorithm']
            needs_update = hash_info['needs_update']
            
            if algorithm == 'bcrypt':
                bcrypt_count += 1
                status = "üîÑ √Ä migrer"
            elif algorithm == 'argon2':
                argon2_count += 1
                status = "‚úÖ Moderne" if not needs_update else "üîÑ √Ä mettre √† jour"
            else:
                unknown_count += 1
                status = "‚ùì Inconnu"
            
            print(f"{team_id:<15} {algorithm:<10} {status:<20} {created_at or 'N/A'}")
        
        print("-" * 65)
        print(f"\nüìä R√©sum√© de l'analyse:")
        print(f"   ‚Ä¢ Hashes bcrypt (√† migrer) : {bcrypt_count}")
        print(f"   ‚Ä¢ Hashes Argon2 (modernes) : {argon2_count}")
        print(f"   ‚Ä¢ Hashes inconnus          : {unknown_count}")
        print(f"   ‚Ä¢ Total √©quipes            : {len(teams)}")
        
        if bcrypt_count > 0:
            print(f"\nüí° Migration n√©cessaire:")
            print(f"   Les {bcrypt_count} √©quipe(s) avec bcrypt seront automatiquement")
            print(f"   migr√©es vers Argon2 lors de leur prochaine connexion r√©ussie.")
            print(f"   Aucune action manuelle n'est requise.")
        else:
            print(f"\nüéâ Toutes les √©quipes utilisent d√©j√† Argon2 !")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
    finally:
        conn.close()


def generate_migration_report():
    """
    G√©n√®re un rapport d√©taill√© de migration
    """
    conn = get_sqlite_connection()
    if not conn:
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = Path(__file__).parent.parent / f"password_migration_report_{timestamp}.txt"
    
    try:
        with open(report_path, 'w', encoding='utf-8') as report:
            report.write(f"Rapport de migration des mots de passe - {datetime.now()}\n")
            report.write("=" * 70 + "\n\n")
            
            # R√©cup√©rer toutes les √©quipes
            cursor = conn.execute("SELECT id, name, password_hash, created_at FROM teams ORDER BY id")
            teams = cursor.fetchall()
            
            teams_to_migrate = []
            
            for team in teams:
                team_id = team['id']
                name = team['name']
                hash_value = team['password_hash']
                created_at = team['created_at']
                
                hash_info = get_password_hash_info(hash_value)
                
                report.write(f"√âquipe: {team_id} ({name})\n")
                report.write(f"  Cr√©√©e: {created_at or 'Date inconnue'}\n")
                report.write(f"  Algorithme: {hash_info['algorithm']}\n")
                report.write(f"  N√©cessite mise √† jour: {hash_info['needs_update']}\n")
                
                if 'passlib_scheme' in hash_info:
                    report.write(f"  Sch√©ma passlib: {hash_info['passlib_scheme']}\n")
                
                if hash_info['algorithm'] == 'bcrypt':
                    teams_to_migrate.append(team_id)
                    report.write(f"  üîÑ MIGRATION REQUISE lors de la prochaine connexion\n")
                elif hash_info['algorithm'] == 'argon2':
                    report.write(f"  ‚úÖ Hash moderne\n")
                else:
                    report.write(f"  ‚ö†Ô∏è Hash de type inconnu\n")
                
                report.write("\n")
            
            report.write(f"R√âSUM√â DE MIGRATION\n")
            report.write("=" * 30 + "\n")
            report.write(f"√âquipes √† migrer: {len(teams_to_migrate)}\n")
            if teams_to_migrate:
                report.write(f"IDs concern√©s: {', '.join(teams_to_migrate)}\n")
            report.write(f"Total √©quipes: {len(teams)}\n\n")
            
            report.write("PROC√âDURE DE MIGRATION\n")
            report.write("=" * 30 + "\n")
            report.write("1. La migration est automatique et transparente\n")
            report.write("2. Elle se d√©clenche lors de la prochaine connexion r√©ussie\n")
            report.write("3. L'ancien hash bcrypt est remplac√© par un nouveau hash Argon2\n")
            report.write("4. Le mot de passe de l'utilisateur reste inchang√©\n")
            report.write("5. Aucune action manuelle n'est requise\n\n")
            
            report.write("POLITIQUE DE MOT DE PASSE\n")
            report.write("=" * 30 + "\n")
            report.write("‚Ä¢ Minimum 4 caract√®res (politique UI v4.1 conserv√©e)\n")
            report.write("‚Ä¢ Confirmation requise lors de la cr√©ation (UI)\n")
            report.write("‚Ä¢ Algorithme Argon2 pour nouveaux comptes\n")
            report.write("‚Ä¢ Compatibilit√© bcrypt maintenue\n")
        
        print(f"üìÑ Rapport g√©n√©r√©: {report_path}")
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©ration rapport: {e}")
    finally:
        conn.close()


def test_migration_functions():
    """
    Test des fonctions de migration avec des donn√©es d'exemple
    """
    print("üß™ Test des fonctions de migration...")
    
    try:
        from src.auth.passwords import create_test_hashes, verify_password, migrate_password_if_needed
        
        # Cr√©er des hashes de test
        test_password = "test123"
        hashes = create_test_hashes(test_password)
        
        print(f"\nüîë Mot de passe de test: {test_password}")
        print(f"Hash Argon2: {hashes['argon2'][:50]}...")
        print(f"Hash bcrypt: {hashes['bcrypt_legacy'][:50]}...")
        
        # Tester la v√©rification
        print(f"\n‚úÖ Tests de v√©rification:")
        
        # Test Argon2
        ok, needs_rehash = verify_password(test_password, hashes['argon2'])
        print(f"Argon2: OK={ok}, Rehash={needs_rehash}")
        
        # Test bcrypt
        ok, needs_rehash = verify_password(test_password, hashes['bcrypt_legacy'])
        print(f"bcrypt: OK={ok}, Rehash={needs_rehash}")
        
        # Test migration
        print(f"\nüîÑ Test de migration:")
        migrated, new_hash = migrate_password_if_needed(test_password, hashes['bcrypt_legacy'])
        print(f"Migration effectu√©e: {migrated}")
        if migrated:
            print(f"Nouveau hash: {new_hash[:50]}...")
        
        print(f"‚úÖ Tests termin√©s avec succ√®s")
        
    except Exception as e:
        print(f"‚ùå Erreur durant les tests: {e}")


def main():
    """Point d'entr√©e principal du script"""
    print("üîê Script de migration des mots de passe GuignoMap v5.0")
    print("bcrypt ‚Üí Argon2 avec migration paresseuse")
    print("=" * 60)
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "analyze":
            analyze_password_hashes()
        elif command == "report":
            generate_migration_report()
        elif command == "test":
            test_migration_functions()
        else:
            print(f"‚ùå Commande inconnue: {command}")
            print_usage()
    else:
        # Par d√©faut, faire l'analyse
        analyze_password_hashes()


def print_usage():
    """Affiche l'aide d'utilisation"""
    print("\nUtilisation:")
    print("  python scripts/migrate_password_hashes.py [commande]")
    print("\nCommandes disponibles:")
    print("  analyze  - Analyser les hashes actuels (d√©faut)")
    print("  report   - G√©n√©rer un rapport d√©taill√©")
    print("  test     - Tester les fonctions de migration")
    print("\nExemples:")
    print("  python scripts/migrate_password_hashes.py analyze")
    print("  python scripts/migrate_password_hashes.py report")


if __name__ == "__main__":
    main()
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/migrate_password_hashes.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/migrate_sqlite_to_postgres.py
# --- encoding: utf-8 | size: 9753 bytes | sha256: dca94e0cc37c3ce2b60e84ab6872c4320ece7c17f88daf80aa8d2a30a6ccaab6 | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
"""
Script de migration SQLite ‚Üí PostgreSQL pour GuignoMap v5.0
Copie toutes les donn√©es existantes de SQLite vers PostgreSQL
"""
import sqlite3
import sys
import os
from pathlib import Path

# Ajouter le r√©pertoire parent au PYTHONPATH pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from src.database.connection import get_engine, execute_transaction
from src.database.models import Base, Street, Team, Note, ActivityLog, Address
from sqlalchemy.orm import sessionmaker
from datetime import datetime
import pandas as pd


def get_sqlite_connection():
    """Connexion en lecture seule √† la base SQLite existante"""
    sqlite_path = Path(__file__).parent.parent / "guignomap" / "guigno_map.db"
    if not sqlite_path.exists():
        print(f"‚ùå Base SQLite non trouv√©e: {sqlite_path}")
        return None
    
    conn = sqlite3.connect(str(sqlite_path))
    conn.row_factory = sqlite3.Row
    return conn


def create_postgres_tables():
    """Cr√©er les tables PostgreSQL via Alembic/SQLAlchemy"""
    try:
        engine = get_engine()
        Base.metadata.create_all(engine)
        print("‚úÖ Tables PostgreSQL cr√©√©es")
        return True
    except Exception as e:
        print(f"‚ùå Erreur cr√©ation tables PostgreSQL: {e}")
        return False


def copy_teams(sqlite_conn, postgres_session):
    """Copier les √©quipes SQLite ‚Üí PostgreSQL"""
    try:
        # Lire depuis SQLite
        teams_data = pd.read_sql_query("""
            SELECT id, name, password_hash, created_at, active 
            FROM teams 
            ORDER BY created_at
        """, sqlite_conn)
        
        if teams_data.empty:
            print("‚ÑπÔ∏è Aucune √©quipe √† migrer")
            return 0
        
        # Ins√©rer dans PostgreSQL
        count = 0
        for _, row in teams_data.iterrows():
            team = Team(
                id=row['id'],
                name=row['name'],
                password_hash=row['password_hash'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow(),
                active=bool(row['active'])
            )
            postgres_session.merge(team)  # merge pour √©viter les doublons
            count += 1
        
        postgres_session.commit()
        print(f"‚úÖ {count} √©quipes migr√©es")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"‚ùå Erreur migration √©quipes: {e}")
        return 0


def copy_streets(sqlite_conn, postgres_session):
    """Copier les rues SQLite ‚Üí PostgreSQL"""
    try:
        # Lire depuis SQLite
        streets_data = pd.read_sql_query("""
            SELECT id, name, sector, team, status 
            FROM streets 
            ORDER BY id
        """, sqlite_conn)
        
        if streets_data.empty:
            print("‚ÑπÔ∏è Aucune rue √† migrer")
            return 0
        
        # Ins√©rer dans PostgreSQL
        count = 0
        for _, row in streets_data.iterrows():
            street = Street(
                id=row['id'] if row['id'] else None,
                name=row['name'],
                sector=row['sector'],
                team=row['team'],
                status=row['status'] or 'a_faire'
            )
            postgres_session.merge(street)
            count += 1
        
        postgres_session.commit()
        print(f"‚úÖ {count} rues migr√©es")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"‚ùå Erreur migration rues: {e}")
        return 0


def copy_notes(sqlite_conn, postgres_session):
    """Copier les notes SQLite ‚Üí PostgreSQL"""
    try:
        # Lire depuis SQLite
        notes_data = pd.read_sql_query("""
            SELECT id, street_name, team_id, address_number, comment, created_at 
            FROM notes 
            ORDER BY created_at
        """, sqlite_conn)
        
        if notes_data.empty:
            print("‚ÑπÔ∏è Aucune note √† migrer")
            return 0
        
        # Ins√©rer dans PostgreSQL
        count = 0
        for _, row in notes_data.iterrows():
            note = Note(
                id=row['id'] if row['id'] else None,
                street_name=row['street_name'],
                team_id=row['team_id'],
                address_number=row['address_number'],
                comment=row['comment'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow()
            )
            postgres_session.merge(note)
            count += 1
        
        postgres_session.commit()
        print(f"‚úÖ {count} notes migr√©es")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"‚ùå Erreur migration notes: {e}")
        return 0


def copy_activity_logs(sqlite_conn, postgres_session):
    """Copier les logs d'activit√© SQLite ‚Üí PostgreSQL"""
    try:
        # V√©rifier si la table existe
        cursor = sqlite_conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='activity_log'
        """)
        if not cursor.fetchone():
            print("‚ÑπÔ∏è Table activity_log non pr√©sente dans SQLite")
            return 0
        
        # Lire depuis SQLite
        logs_data = pd.read_sql_query("""
            SELECT id, team_id, action, details, created_at 
            FROM activity_log 
            ORDER BY created_at
        """, sqlite_conn)
        
        if logs_data.empty:
            print("‚ÑπÔ∏è Aucun log d'activit√© √† migrer")
            return 0
        
        # Ins√©rer dans PostgreSQL
        count = 0
        for _, row in logs_data.iterrows():
            log = ActivityLog(
                id=row['id'] if row['id'] else None,
                team_id=row['team_id'],
                action=row['action'],
                details=row['details'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow()
            )
            postgres_session.merge(log)
            count += 1
        
        postgres_session.commit()
        print(f"‚úÖ {count} logs d'activit√© migr√©s")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"‚ùå Erreur migration logs: {e}")
        return 0


def copy_addresses(sqlite_conn, postgres_session):
    """Copier les adresses OSM SQLite ‚Üí PostgreSQL"""
    try:
        # V√©rifier si la table existe
        cursor = sqlite_conn.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name='addresses'
        """)
        if not cursor.fetchone():
            print("‚ÑπÔ∏è Table addresses non pr√©sente dans SQLite")
            return 0
        
        # Lire depuis SQLite
        addresses_data = pd.read_sql_query("""
            SELECT id, street_name, house_number, latitude, longitude, osm_type, created_at 
            FROM addresses 
            ORDER BY created_at
        """, sqlite_conn)
        
        if addresses_data.empty:
            print("‚ÑπÔ∏è Aucune adresse √† migrer")
            return 0
        
        # Ins√©rer dans PostgreSQL
        count = 0
        for _, row in addresses_data.iterrows():
            address = Address(
                id=row['id'] if row['id'] else None,
                street_name=row['street_name'],
                house_number=row['house_number'],
                latitude=row['latitude'],
                longitude=row['longitude'],
                osm_type=row['osm_type'],
                created_at=pd.to_datetime(row['created_at']) if row['created_at'] else datetime.utcnow()
            )
            postgres_session.merge(address)
            count += 1
        
        postgres_session.commit()
        print(f"‚úÖ {count} adresses migr√©es")
        return count
        
    except Exception as e:
        postgres_session.rollback()
        print(f"‚ùå Erreur migration adresses: {e}")
        return 0


def main():
    """Migration compl√®te SQLite ‚Üí PostgreSQL"""
    print("üîÑ D√©but migration SQLite ‚Üí PostgreSQL...")
    
    # Connexions
    sqlite_conn = get_sqlite_connection()
    if not sqlite_conn:
        return False
    
    try:
        # Cr√©er les tables PostgreSQL
        if not create_postgres_tables():
            return False
        
        # Session PostgreSQL
        engine = get_engine()
        Session = sessionmaker(bind=engine)
        postgres_session = Session()
        
        # Migration par table
        total_migrated = 0
        total_migrated += copy_teams(sqlite_conn, postgres_session)
        total_migrated += copy_streets(sqlite_conn, postgres_session)
        total_migrated += copy_notes(sqlite_conn, postgres_session)
        total_migrated += copy_activity_logs(sqlite_conn, postgres_session)
        total_migrated += copy_addresses(sqlite_conn, postgres_session)
        
        postgres_session.close()
        sqlite_conn.close()
        
        print(f"üéâ Migration termin√©e ! {total_migrated} enregistrements migr√©s")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur g√©n√©rale migration: {e}")
        if 'postgres_session' in locals():
            postgres_session.close()
        sqlite_conn.close()
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/migrate_sqlite_to_postgres.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/sanity_db_pandas.py
# --- encoding: utf-8 | size: 751 bytes | sha256: 196c060c60bdb07a4c39f43b5cfbccfbf961b09faacdc844b9689e578e6d5296 | mtime: 2025-09-15T18:24:27-04:00
# --------------------------------------------------------------------------------
from sqlalchemy import text
import pandas as pd
import sys, pathlib

# 1) Injecte la racine du repo dans sys.path
ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

# 2) Import de get_session() depuis src.database.connection
try:
    from src.database.connection import get_session
except Exception as e:
    raise SystemExit(f"[ERREUR] Import get_session introuvable depuis src.database.connection: {e}")

# 3) Test SELECT 1 + version pandas
try:
    with get_session() as s:
        print("DB OK:", s.execute(text("SELECT 1")).scalar_one())
except Exception as e:
    raise SystemExit(f"[ERREUR] DB SELECT 1 a √©chou√©: {e}")

print("pandas OK:", pd.__version__)
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/sanity_db_pandas.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: scripts/smoke_create_map.py
# --- encoding: utf-8 | size: 1397 bytes | sha256: 1066478ac296ab246aa52b3d3b9bbf2007c684038b2cc881b068beaecfa7b933 | mtime: 2025-09-15T21:44:48-04:00
# --------------------------------------------------------------------------------
import sys, pathlib, importlib
import pandas as pd
from importlib.util import spec_from_file_location, module_from_spec

ROOT = pathlib.Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

mod = None
# Tentatives d'import par paquet (apr√®s ajout __init__.py)
for candidate in ("guignomap.app", "src.app"):
    try:
        mod = importlib.import_module(candidate)
        break
    except Exception:
        continue

# Fallback: import direct par chemin
app_path = ROOT / "guignomap" / "app.py"
if mod is None and app_path.exists():
    spec = spec_from_file_location("app_fallback", str(app_path))
    if spec is not None and spec.loader is not None:
        m = module_from_spec(spec)
        spec.loader.exec_module(m)
        mod = m

if mod is None or not hasattr(mod, "create_map"):
    raise SystemExit("[ERREUR] create_map introuvable (guignomap.app/src.app ou fallback fichier)")

# DataFrame minimal conforme √† create_map(df, geo)
df = pd.DataFrame([
    {"name": "Rue Test A", "status": "a_faire", "team": "", "notes": "0"},
    {"name": "Rue Test B", "status": "terminee", "team": "EQUIPE1", "notes": "2"},
])

try:
    mod.create_map(df, {})  # geo vide: on valide la boucle DataFrame
    print("create_map(df) OK")
except Exception as e:
    print("create_map(df) FAILED:", e)
    sys.exit(1)
# --------------------------------------------------------------------------------
# <<< END FILE: scripts/smoke_create_map.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/__init__.py
# --- encoding: utf-8 | size: 0 bytes | sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 | mtime: 2025-09-15T18:21:06-04:00
# --------------------------------------------------------------------------------

# --------------------------------------------------------------------------------
# <<< END FILE: src/__init__.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/auth/passwords.py
# --- encoding: utf-8 | size: 6273 bytes | sha256: 88ba1592d5d90e1bb544e2ebf81ebd8545e24e21b112962d1248d02379a3ff5b | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
"""
Gestion des mots de passe avec Argon2 pour GuignoMap v5.0
Migration compatible depuis bcrypt + politique UI inchang√©e (min 4 + confirmation)
"""
from passlib.context import CryptContext
from typing import Tuple
import bcrypt


# Configuration passlib avec Argon2 comme algorithme principal
# Garde bcrypt pour la compatibilit√© ascendante (lecture uniquement)
pwd_context = CryptContext(
    schemes=["argon2", "bcrypt"],
    deprecated="auto",  # Marque bcrypt comme obsol√®te
    argon2__rounds=2,   # Param√®tres Argon2 pour performance/s√©curit√© √©quilibr√©e
    argon2__memory_cost=65536,  # 64 MB
    argon2__parallelism=1,
    argon2__hash_len=32
)


def hash_password(password: str) -> str:
    """
    Hash un mot de passe avec Argon2
    
    Args:
        password: Mot de passe en texte clair
        
    Returns:
        Hash Argon2 du mot de passe
    """
    return pwd_context.hash(password)


def verify_password(password: str, hashed: str) -> Tuple[bool, bool]:
    """
    V√©rifie un mot de passe contre son hash
    Supporte la migration automatique bcrypt ‚Üí Argon2
    
    Args:
        password: Mot de passe en texte clair
        hashed: Hash stock√© (bcrypt ou Argon2)
        
    Returns:
        Tuple (verification_ok, needs_rehash)
        - verification_ok: True si le mot de passe est correct
        - needs_rehash: True si le hash doit √™tre mis √† jour (migration paresseuse)
    """
    try:
        # V√©rification avec passlib (supporte bcrypt et Argon2)
        verification_ok = pwd_context.verify(password, hashed)
        
        if verification_ok:
            # V√©rifier si une mise √† jour du hash est n√©cessaire
            needs_rehash = pwd_context.needs_update(hashed)
            return True, needs_rehash
        else:
            return False, False
            
    except Exception as e:
        print(f"Erreur v√©rification mot de passe: {e}")
        return False, False


def is_bcrypt_hash(hashed: str) -> bool:
    """
    D√©termine si un hash est au format bcrypt
    
    Args:
        hashed: Hash √† v√©rifier
        
    Returns:
        True si c'est un hash bcrypt
    """
    return hashed.startswith('$2b$') or hashed.startswith('$2a$') or hashed.startswith('$2y$')


def is_argon2_hash(hashed: str) -> bool:
    """
    D√©termine si un hash est au format Argon2
    
    Args:
        hashed: Hash √† v√©rifier
        
    Returns:
        True si c'est un hash Argon2
    """
    return hashed.startswith('$argon2')


def migrate_password_if_needed(password: str, old_hash: str) -> Tuple[bool, str]:
    """
    Migration paresseuse d'un mot de passe bcrypt vers Argon2
    Appel√© lors d'une connexion r√©ussie
    
    Args:
        password: Mot de passe en texte clair (fourni lors de la connexion)
        old_hash: Hash actuel stock√©
        
    Returns:
        Tuple (migrated, new_hash)
        - migrated: True si une migration a eu lieu
        - new_hash: Nouveau hash Argon2 (ou old_hash si pas de migration)
    """
    verification_ok, needs_rehash = verify_password(password, old_hash)
    
    if verification_ok and needs_rehash:
        # Migration n√©cessaire : re-hasher avec Argon2
        new_hash = hash_password(password)
        print(f"üîÑ Migration hash bcrypt ‚Üí Argon2")
        return True, new_hash
    
    return False, old_hash


def validate_password_policy(password: str) -> Tuple[bool, str]:
    """
    Validation de la politique de mot de passe
    IMPORTANT: Garder la politique UI v4.1 (min 4 caract√®res + confirmation)
    
    Args:
        password: Mot de passe √† valider
        
    Returns:
        Tuple (valid, error_message)
    """
    if not password:
        return False, "Le mot de passe est requis"
    
    if len(password) < 4:
        return False, "Le mot de passe doit contenir au moins 4 caract√®res"
    
    # Note: La confirmation est g√©r√©e c√¥t√© UI, pas ici
    return True, ""


def get_password_hash_info(hashed: str) -> dict:
    """
    Informations sur un hash de mot de passe
    Utile pour diagnostics et migration
    
    Args:
        hashed: Hash √† analyser
        
    Returns:
        Dictionnaire avec les informations du hash
    """
    info = {
        'algorithm': 'unknown',
        'needs_update': False,
        'is_bcrypt': is_bcrypt_hash(hashed),
        'is_argon2': is_argon2_hash(hashed)
    }
    
    try:
        if is_bcrypt_hash(hashed):
            info['algorithm'] = 'bcrypt'
            info['needs_update'] = True  # Tous les bcrypt doivent migrer
        elif is_argon2_hash(hashed):
            info['algorithm'] = 'argon2'
            info['needs_update'] = pwd_context.needs_update(hashed)
        
        # Informations suppl√©mentaires via passlib
        hash_info = pwd_context.identify(hashed)
        if hash_info:
            info['passlib_scheme'] = hash_info
            
    except Exception as e:
        info['error'] = str(e)
    
    return info


# Fonctions de compatibilit√© avec l'ancien syst√®me bcrypt
def legacy_verify_bcrypt(password: str, bcrypt_hash: str) -> bool:
    """
    V√©rification directe bcrypt pour r√©trocompatibilit√©
    Utilis√© uniquement si passlib √©choue
    
    Args:
        password: Mot de passe en texte clair
        bcrypt_hash: Hash bcrypt √† v√©rifier
        
    Returns:
        True si le mot de passe correspond
    """
    try:
        return bcrypt.checkpw(password.encode('utf-8'), bcrypt_hash.encode('utf-8'))
    except Exception as e:
        print(f"Erreur v√©rification bcrypt legacy: {e}")
        return False


def create_test_hashes(password: str = "test123") -> dict:
    """
    Utilitaire pour cr√©er des hashes de test
    Aide au d√©veloppement et aux tests
    
    Args:
        password: Mot de passe de test
        
    Returns:
        Dictionnaire avec les diff√©rents hashes
    """
    return {
        'password': password,
        'argon2': hash_password(password),
        'bcrypt_legacy': bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
    }
# --------------------------------------------------------------------------------
# <<< END FILE: src/auth/passwords.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/config.py
# --- encoding: utf-8 | size: 1759 bytes | sha256: 6b09a8dfb12135988e8db389c7a7b6416f908f87118af6840128fcce5351d7b4 | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
"""
Configuration centralis√©e pour GuignoMap v5.0
Acc√®s aux secrets Streamlit et param√®tres applicatifs
"""
import streamlit as st
import os


def get_database_url():
    """R√©cup√®re l'URL de la base de donn√©es depuis les secrets"""
    try:
        return st.secrets["database"]["url"]
    except (KeyError, AttributeError):
        # Fallback pour d√©veloppement local ou tests
        return os.getenv("DATABASE_URL", "sqlite:///guigno_map.db")


def get_database_pool_config():
    """Configuration du pool de connexions PostgreSQL"""
    try:
        return {
            "pool_size": st.secrets["database"].get("pool_size", 5),
            "max_overflow": st.secrets["database"].get("max_overflow", 10)
        }
    except (KeyError, AttributeError):
        return {"pool_size": 5, "max_overflow": 10}


def get_s3_config():
    """Configuration S3 pour le stockage cloud"""
    try:
        return {
            "bucket": st.secrets["storage"]["s3_bucket"],
            "region": st.secrets["storage"]["s3_region"],
            "access_key": st.secrets["storage"]["s3_access_key"],
            "secret_key": st.secrets["storage"]["s3_secret_key"]
        }
    except (KeyError, AttributeError):
        return {
            "bucket": os.getenv("S3_BUCKET", "guignomap-dev"),
            "region": os.getenv("S3_REGION", "us-east-1"),
            "access_key": os.getenv("S3_ACCESS_KEY", ""),
            "secret_key": os.getenv("S3_SECRET_KEY", "")
        }


def get_cdn_base_url():
    """URL de base CDN pour les assets (optionnel)"""
    try:
        return st.secrets["storage"].get("cdn_base_url", "")
    except (KeyError, AttributeError):
        return os.getenv("CDN_BASE_URL", "")
# --------------------------------------------------------------------------------
# <<< END FILE: src/config.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/database/connection.py
# --- encoding: utf-8 | size: 4570 bytes | sha256: 6079490811be90cff876cac564602a78faf7e09b56f7fdd9632e4f740d505efb | mtime: 2025-09-15T21:50:52-04:00
# --------------------------------------------------------------------------------
"""
Connexion PostgreSQL avec SQLAlchemy pour GuignoMap v5.0
Engine + QueuePool + cache Streamlit + retry logic
"""
import time
import functools
import streamlit as st
from sqlalchemy import create_engine, text
from sqlalchemy.pool import QueuePool
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import SQLAlchemyError
from src.config import get_database_url, get_database_pool_config


def db_retry(max_retries=3, backoff_factor=1):
    """
    D√©corateur retry exponentiel pour op√©rations DB critiques
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except SQLAlchemyError as e:
                    last_exception = e
                    if attempt < max_retries - 1:
                        wait_time = backoff_factor * (2 ** attempt)
                        print(f"Retry DB operation {func.__name__} in {wait_time}s (attempt {attempt + 1}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        print(f"Max retries reached for {func.__name__}")
                        break
            
            # Si aucune exception captur√©e, lever une erreur g√©n√©rique
            if last_exception is not None:
                raise RuntimeError(f"√âchec d'initialisation DB: {last_exception}") from last_exception
            else:
                raise RuntimeError(f"√âchec d'initialisation DB pour {func.__name__}: raison inconnue")
        return wrapper
    return decorator


@st.cache_resource
def get_engine():
    """
    Engine PostgreSQL avec cache Streamlit et configuration pool
    Conform√©ment au plan v5.0
    """
    database_url = get_database_url()
    pool_config = get_database_pool_config()
    
    # Configuration PostgreSQL avec QueuePool
    engine = create_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=pool_config["pool_size"],
        max_overflow=pool_config["max_overflow"],
        pool_pre_ping=True,
        pool_recycle=300,
        echo=False  # Set to True for SQL debugging
    )
    
    return engine


def get_session():
    """Fabrique de session SQLAlchemy"""
    engine = get_engine()
    Session = sessionmaker(bind=engine)
    return Session()


@db_retry(max_retries=3)
def test_connection():
    """Test de connexion √† la base PostgreSQL"""
    try:
        engine = get_engine()
        with engine.connect() as conn:
            result = conn.execute(text("SELECT 1 as test"))
            row = result.fetchone()
            return row is not None and row[0] == 1
    except Exception as e:
        print(f"Erreur test connexion DB: {e}")
        return False


@db_retry(max_retries=3)
def execute_query(query, params=None):
    """
    Ex√©cution de requ√™te avec retry automatique
    Pour transition progressive vers SQLAlchemy
    """
    engine = get_engine()
    with engine.connect() as conn:
        if params:
            return conn.execute(text(query), params)
        else:
            return conn.execute(text(query))


@db_retry(max_retries=3)  
def execute_transaction(queries_and_params):
    """
    Ex√©cution de transaction multi-requ√™tes avec retry
    queries_and_params: liste de tuples (query, params)
    """
    engine = get_engine()
    with engine.begin() as conn:
        results = []
        for query, params in queries_and_params:
            if params:
                result = conn.execute(text(query), params)
            else:
                result = conn.execute(text(query))
            results.append(result)
        return results


# Wrapper facultatif pour compatibilit√© avec l'ancien code/patrons
def get_session_factory():
    """Retourne un callable qui ouvre une session (usage: with Session() as s: ...)."""
    def _open():
        return get_session().__enter__()  # pour compat avec 'with Session() as s'
    # pour permettre 'with Session() as s', on expose un objet qui supporte __call__ et __enter__/__exit__
    class _Factory:
        def __call__(self):
            return get_session().__enter__()
        def __enter__(self):
            return get_session().__enter__()
        def __exit__(self, exc_type, exc, tb):
            return get_session().__exit__(exc_type, exc, tb)
    return _Factory()
# --------------------------------------------------------------------------------
# <<< END FILE: src/database/connection.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/database/db_v5.py
# --- encoding: utf-8 | size: 29250 bytes | sha256: 166c04aa5a2775273561a423e4eb664f54837bf161dd57ec2cac5975d277d50c | mtime: 2025-09-15T21:50:52-04:00
# --------------------------------------------------------------------------------
"""
GuignoMap v5.0 - Database operation            admin_exists = session.execute(
                text("SELECT COUNT(*) FROM teams WHERE id = 'ADMIN'")
            ).scalar() or 0
            
            if admin_exists > 0:h SQLAlchemy
Migration from raw sqlite3 to SQLAlchemy + PostgreSQL support
"""
import os
import pandas as pd
import hashlib
import bcrypt
from datetime import datetime
import json
from pathlib import Path
import secrets
import string
from typing import Optional, List, Dict, Any

from sqlalchemy import text, and_, or_
from src.database.connection import get_session, db_retry
from src.database.models import Street, Team, Note, ActivityLog, Address
from guignomap.backup import auto_backup_before_critical, BackupManager
from guignomap.validators import validate_and_clean_input, InputValidator


# =============================================================================
# CONFIGURATION & CONSTANTES
# =============================================================================

# Sch√©ma de migration - utilis√© pour v√©rifier les tables existantes
REQUIRED_TABLES = ['streets', 'teams', 'notes', 'activity_log', 'addresses']


# =============================================================================
# FONCTIONS DE CONNEXION ET INITIALISATION
# =============================================================================

@db_retry(max_retries=3)
def init_db():
    """Initialise la base de donn√©es avec les donn√©es initiales"""
    try:
        with get_session() as session:
            # V√©rifier si admin existe
            admin_exists = session.execute(
                text("SELECT COUNT(*) FROM teams WHERE id = 'ADMIN'")
            ).scalar()
            
            if admin_exists == 0:
                pwd = os.getenv("GM_ADMIN_PWD", "RELAIS2025")
                create_team('ADMIN', 'Superviseur', pwd)
            
            # Auto-import des rues si vide
            streets_count = session.execute(
                text("SELECT COUNT(*) FROM streets")
            ).scalar()
            
            if streets_count == 0:
                print("üîÑ Aucune rue trouv√©e. Import automatique depuis OpenStreetMap...")
                auto_import_streets()
                
    except Exception as e:
        print(f"‚ùå Erreur init_db: {e}")
        raise


def auto_import_streets():
    """Import automatique des rues depuis OSM cache"""
    try:
        from guignomap.osm import load_geometry_cache
        
        with get_session() as session:
            cache = load_geometry_cache()
            if not cache:
                print("‚ö†Ô∏è Aucun cache OSM trouv√©. Utilisez 'Construire carte' dans l'admin.")
                return
            
            imported = 0
            for street_name in cache.keys():
                if street_name and street_name.strip():
                    # V√©rifier si existe d√©j√†
                    exists = session.execute(
                        text("SELECT COUNT(*) FROM streets WHERE name = :name"),
                        {"name": street_name.strip()}
                    ).scalar()
                    
                    if exists == 0:
                        session.execute(text("""
                            INSERT INTO streets (name, status) 
                            VALUES (:name, 'a_faire')
                        """), {"name": street_name.strip()})
                        imported += 1
            
            session.commit()
            print(f"‚úÖ {imported} rues import√©es depuis OSM")
            
    except Exception as e:
        print(f"‚ùå Erreur auto_import_streets: {e}")


# =============================================================================
# GESTION DES √âQUIPES ET AUTHENTIFICATION
# =============================================================================

def hash_password(password: str) -> str:
    """Hash un mot de passe avec bcrypt"""
    return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')


@db_retry(max_retries=2)
def create_team(team_id: str, name: str, password: str) -> bool:
    """Cr√©e une nouvelle √©quipe"""
    try:
        with get_session() as session:
            # V√©rifier si l'√©quipe existe d√©j√†
            exists = session.execute(
                text("SELECT COUNT(*) FROM teams WHERE id = :id"),
                {"id": team_id}
            ).scalar() or 0
            
            if exists > 0:
                return False
            
            password_hash = hash_password(password)
            session.execute(text("""
                INSERT INTO teams (id, name, password_hash, created_at, active)
                VALUES (:id, :name, :hash, CURRENT_TIMESTAMP, 1)
            """), {
                "id": team_id,
                "name": name, 
                "hash": password_hash
            })
            session.commit()
            
            # Log de l'activit√©
            log_activity(session, team_id, 'create_team', f"√âquipe '{name}' cr√©√©e")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Erreur create_team: {e}")
        return False


@db_retry(max_retries=2)
def verify_team(team_id: str, password: str) -> bool:
    """V√©rifie les identifiants d'une √©quipe"""
    try:
        with get_session() as session:
            result = session.execute(
                text("SELECT password_hash FROM teams WHERE id = :id AND active = 1"),
                {"id": team_id}
            ).first()
            
            if not result:
                return False
            
            stored_hash = result[0]
            
            # Support bcrypt (nouveau) et MD5 legacy (migration)
            if stored_hash.startswith('$2b$'):
                # bcrypt
                return bcrypt.checkpw(password.encode('utf-8'), stored_hash.encode('utf-8'))
            else:
                # MD5 legacy - migrer automatiquement
                if hashlib.md5(password.encode()).hexdigest() == stored_hash:
                    # Migrer vers bcrypt
                    new_hash = hash_password(password)
                    session.execute(
                        text("UPDATE teams SET password_hash = :hash WHERE id = :id"),
                        {"hash": new_hash, "id": team_id}
                    )
                    session.commit()
                    return True
                return False
                
    except Exception as e:
        print(f"‚ùå Erreur verify_team: {e}")
        return False


def get_all_teams() -> List[Dict[str, Any]]:
    """R√©cup√®re toutes les √©quipes actives"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT id, name, created_at, 
                       (SELECT COUNT(*) FROM streets WHERE team = teams.id) as assigned_streets
                FROM teams 
                WHERE active = 1 
                ORDER BY name
            """))
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur get_all_teams: {e}")
        return []


def teams() -> List[str]:
    """R√©cup√®re la liste des IDs d'√©quipes actives"""
    try:
        with get_session() as session:
            result = session.execute(
                text("SELECT id FROM teams WHERE active = 1 ORDER BY name")
            )
            return [row[0] for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur teams: {e}")
        return []


@auto_backup_before_critical
def delete_team(team_id: str) -> bool:
    """Supprime une √©quipe (soft delete)"""
    try:
        with get_session() as session:
            session.execute(
                text("UPDATE teams SET active = 0 WHERE id = :id"),
                {"id": team_id}
            )
            session.commit()
            return True
            
    except Exception as e:
        print(f"‚ùå Erreur delete_team: {e}")
        return False


# =============================================================================
# GESTION DES RUES ET STATUTS  
# =============================================================================

def list_streets(team: Optional[str] = None) -> pd.DataFrame:
    """Liste les rues avec filtrage optionnel par √©quipe"""
    try:
        with get_session() as session:
            if team:
                query = text("""
                    SELECT id, name, sector, team, status
                    FROM streets 
                    WHERE team = :team
                    ORDER BY name
                """)
                result = session.execute(query, {"team": team})
            else:
                query = text("""
                    SELECT id, name, sector, team, status
                    FROM streets 
                    ORDER BY name
                """)
                result = session.execute(query)
            
            # Convertir en DataFrame
            rows = [dict(row._mapping) for row in result]
            return pd.DataFrame(rows) if rows else pd.DataFrame(columns=['id', 'name', 'sector', 'team', 'status'])
            
    except Exception as e:
        print(f"‚ùå Erreur list_streets: {e}")
        return pd.DataFrame(columns=['id', 'name', 'sector', 'team', 'status'])


def get_unassigned_streets() -> List[str]:
    """R√©cup√®re les rues non assign√©es √† une √©quipe"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT name FROM streets 
                WHERE team IS NULL OR team = ''
                ORDER BY name
            """))
            return [row[0] for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur get_unassigned_streets: {e}")
        return []


@auto_backup_before_critical
def assign_streets_to_team(street_names: List[str], team_id: str) -> int:
    """Assigne plusieurs rues √† une √©quipe"""
    try:
        with get_session() as session:
            count = 0
            for street_name in street_names:
                # V√©rifier si la rue existe et n'est pas assign√©e
                existing = session.execute(text("""
                    SELECT COUNT(*) FROM streets 
                    WHERE name = :name AND (team IS NULL OR team = '')
                """), {"name": street_name}).scalar() or 0
                
                if existing > 0:
                    session.execute(text("""
                        UPDATE streets 
                        SET team = :team 
                        WHERE name = :name AND (team IS NULL OR team = '')
                    """), {"team": team_id, "name": street_name})
                    count += 1
            
            session.commit()
            
            # Log de l'activit√©
            if count > 0:
                log_activity(session, team_id, 'assign_streets', 
                           f"{count} rues assign√©es √† l'√©quipe")
            
            return count
            
    except Exception as e:
        print(f"‚ùå Erreur assign_streets_to_team: {e}")
        return 0


@auto_backup_before_critical
def set_status(name: str, status: str) -> bool:
    """Met √† jour le statut d'une rue"""
    try:
        # Validation du statut
        valid_statuses = ['a_faire', 'en_cours', 'terminee']
        if status not in valid_statuses:
            return False
        
        with get_session() as session:
            # V√©rifier si la rue existe
            exists = session.execute(
                text("SELECT COUNT(*) FROM streets WHERE name = :name"),
                {"name": name}
            ).scalar() or 0
            
            if exists > 0:
                session.execute(text("""
                    UPDATE streets 
                    SET status = :status 
                    WHERE name = :name
                """), {"status": status, "name": name})
                
                session.commit()
                
                # Log de l'activit√©
                team = session.execute(
                    text("SELECT team FROM streets WHERE name = :name"),
                    {"name": name}
                ).scalar()
                
                if team:
                    log_activity(session, team, 'status_change', 
                               f"Rue '{name}' -> {status}")
                
                return True
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur set_status: {e}")
        return False


# =============================================================================
# GESTION DES NOTES ET ADRESSES
# =============================================================================

@auto_backup_before_critical
def add_note_for_address(street_name: str, team_id: str, address_number: str, comment: str) -> bool:
    """Ajoute une note pour une adresse sp√©cifique"""
    try:
        # Validation et nettoyage
        _, comment = validate_and_clean_input("comment", comment)
        _, address_number = validate_and_clean_input("address_number", address_number)
        
        with get_session() as session:
            session.execute(text("""
                INSERT INTO notes (street_name, team_id, address_number, comment, created_at)
                VALUES (:street, :team, :addr, :comment, CURRENT_TIMESTAMP)
            """), {
                "street": street_name,
                "team": team_id,
                "addr": address_number,
                "comment": comment
            })
            session.commit()
            
            # Log de l'activit√©
            log_activity(session, team_id, 'add_note', 
                       f"Note ajout√©e: {street_name} #{address_number}")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Erreur add_note_for_address: {e}")
        return False


def get_street_addresses_with_notes(street_name: str) -> List[Dict[str, Any]]:
    """R√©cup√®re les adresses avec notes pour une rue"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT address_number, comment, team_id, created_at
                FROM notes 
                WHERE street_name = :street
                ORDER BY CAST(address_number AS INTEGER), created_at DESC
            """), {"street": street_name})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur get_street_addresses_with_notes: {e}")
        return []


def get_team_notes(team_id: str) -> List[Dict[str, Any]]:
    """R√©cup√®re toutes les notes d'une √©quipe"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT street_name, address_number, comment, created_at
                FROM notes 
                WHERE team_id = :team
                ORDER BY created_at DESC
            """), {"team": team_id})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur get_team_notes: {e}")
        return []


# =============================================================================
# STATISTIQUES ET RAPPORTS
# =============================================================================

def extended_stats() -> Dict[str, Any]:
    """Statistiques √©tendues de l'application"""
    try:
        with get_session() as session:
            # Stats de base
            total_streets = session.execute(text("SELECT COUNT(*) FROM streets")).scalar() or 0
            assigned_streets = session.execute(text("SELECT COUNT(*) FROM streets WHERE team IS NOT NULL AND team != ''")).scalar() or 0
            completed_streets = session.execute(text("SELECT COUNT(*) FROM streets WHERE status = 'terminee'")).scalar() or 0
            in_progress_streets = session.execute(text("SELECT COUNT(*) FROM streets WHERE status = 'en_cours'")).scalar() or 0
            
            # Stats par statut
            status_counts = session.execute(text("""
                SELECT status, COUNT(*) as count
                FROM streets 
                GROUP BY status
            """))
            status_data = {row[0]: row[1] for row in status_counts}
            
            return {
                'total_streets': total_streets,
                'assigned_streets': assigned_streets,
                'unassigned_streets': total_streets - assigned_streets,
                'completed_streets': completed_streets,
                'in_progress_streets': in_progress_streets,
                'todo_streets': total_streets - completed_streets - in_progress_streets,
                'completion_rate': (completed_streets / total_streets * 100) if total_streets > 0 else 0,
                'status_breakdown': status_data
            }
            
    except Exception as e:
        print(f"‚ùå Erreur extended_stats: {e}")
        return {}


def stats_by_team() -> List[Dict[str, Any]]:
    """Statistiques par √©quipe"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT 
                    t.id,
                    t.name,
                    COUNT(s.id) as total_streets,
                    SUM(CASE WHEN s.status = 'terminee' THEN 1 ELSE 0 END) as completed,
                    SUM(CASE WHEN s.status = 'en_cours' THEN 1 ELSE 0 END) as in_progress,
                    SUM(CASE WHEN s.status = 'a_faire' THEN 1 ELSE 0 END) as todo
                FROM teams t
                LEFT JOIN streets s ON s.team = t.id
                WHERE t.active = 1
                GROUP BY t.id, t.name
                ORDER BY t.name
            """))
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur stats_by_team: {e}")
        return []


def recent_activity(limit: int = 10) -> List[Dict[str, Any]]:
    """Activit√© r√©cente dans l'application"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT team_id, action, details, created_at
                FROM activity_log 
                ORDER BY created_at DESC 
                LIMIT :limit
            """), {"limit": limit})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur recent_activity: {e}")
        return []


def export_to_csv() -> str:
    """Exporte les donn√©es vers CSV"""
    try:
        from datetime import datetime
        
        df = list_streets()
        if df.empty:
            return ""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        export_dir = Path(__file__).parent.parent / "exports"
        export_dir.mkdir(exist_ok=True)
        
        filename = f"guignomap_export_{timestamp}.csv"
        filepath = export_dir / filename
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        return str(filepath)
        
    except Exception as e:
        print(f"‚ùå Erreur export_to_csv: {e}")
        return ""


# =============================================================================
# LOG D'ACTIVIT√â
# =============================================================================

def log_activity(session, team_id: str, action: str, details: str):
    """Log une activit√© dans la base de donn√©es"""
    try:
        session.execute(text("""
            INSERT INTO activity_log (team_id, action, details, created_at)
            VALUES (:team, :action, :details, CURRENT_TIMESTAMP)
        """), {
            "team": team_id,
            "action": action,
            "details": details
        })
        # Note: commit fait par la fonction appelante
        
    except Exception as e:
        print(f"‚ùå Erreur log_activity: {e}")


# =============================================================================
# FONCTIONS MANQUANTES POUR COMPATIBILIT√â APP.PY
# =============================================================================

def get_team_streets(team_id: str) -> List[Dict[str, Any]]:
    """R√©cup√®re les rues assign√©es √† une √©quipe avec tous les d√©tails"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT id, name, sector, team, status
                FROM streets 
                WHERE team = :team 
                ORDER BY name
            """), {"team": team_id})
            return [dict(row._mapping) for row in result]
    except Exception as e:
        print(f"‚ùå Erreur get_team_streets: {e}")
        return []


def get_unassigned_streets_count() -> int:
    """Compte les rues non assign√©es"""
    try:
        with get_session() as session:
            count = session.execute(text("""
                SELECT COUNT(*) FROM streets 
                WHERE team IS NULL OR team = ''
            """)).scalar() or 0
            return count
    except Exception as e:
        print(f"‚ùå Erreur get_unassigned_streets_count: {e}")
        return 0


def get_sectors_list() -> List[str]:
    """R√©cup√®re la liste des secteurs"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT DISTINCT sector FROM streets 
                WHERE sector IS NOT NULL AND sector != ''
                ORDER BY sector
            """))
            return [row[0] for row in result]
    except Exception as e:
        print(f"‚ùå Erreur get_sectors_list: {e}")
        return []


def get_teams_list() -> List[str]:
    """R√©cup√®re la liste des √©quipes (alias pour teams())"""
    return teams()


def bulk_assign_sector(sector: str, team_id: str) -> int:
    """Assigne toutes les rues d'un secteur √† une √©quipe"""
    try:
        with get_session() as session:
            # R√©cup√©rer les rues non assign√©es du secteur
            result = session.execute(text("""
                SELECT name FROM streets 
                WHERE sector = :sector AND (team IS NULL OR team = '')
            """), {"sector": sector})
            
            street_names = [row[0] for row in result]
            
            if street_names:
                return assign_streets_to_team(street_names, team_id)
            return 0
            
    except Exception as e:
        print(f"‚ùå Erreur bulk_assign_sector: {e}")
        return 0


def get_assignations_export_data() -> List[Dict[str, Any]]:
    """Donn√©es pour export des assignations"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT s.name as street_name, s.sector, s.team, s.status,
                       t.name as team_name
                FROM streets s
                LEFT JOIN teams t ON s.team = t.id
                ORDER BY s.name
            """))
            return [dict(row._mapping) for row in result]
    except Exception as e:
        print(f"‚ùå Erreur get_assignations_export_data: {e}")
        return []


def export_notes_csv() -> str:
    """Exporte les notes vers CSV"""
    try:
        from datetime import datetime
        
        with get_session() as session:
            result = session.execute(text("""
                SELECT n.street_name, n.team_id, n.address_number, 
                       n.comment, n.created_at,
                       t.name as team_name
                FROM notes n
                LEFT JOIN teams t ON n.team_id = t.id
                ORDER BY n.created_at DESC
            """))
            
            if not result:
                return ""
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            export_dir = Path(__file__).parent.parent / "exports"
            export_dir.mkdir(exist_ok=True)
            
            filename = f"notes_export_{timestamp}.csv"
            filepath = export_dir / filename
            
            import pandas as pd
            df = pd.DataFrame([dict(row._mapping) for row in result])
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            
            return str(filepath)
            
    except Exception as e:
        print(f"‚ùå Erreur export_notes_csv: {e}")
        return ""


def import_addresses_from_cache(addr_cache: Dict) -> int:
    """Importe les adresses depuis le cache OSM"""
    try:
        with get_session() as session:
            imported = 0
            
            for street_name, addresses in addr_cache.items():
                if isinstance(addresses, list):
                    for addr in addresses:
                        # Ins√©rer l'adresse si elle n'existe pas
                        exists = session.execute(text("""
                            SELECT COUNT(*) FROM addresses 
                            WHERE street_name = :street AND house_number = :num
                        """), {"street": street_name, "num": str(addr)}).scalar() or 0
                        
                        if exists == 0:
                            session.execute(text("""
                                INSERT INTO addresses (street_name, house_number)
                                VALUES (:street, :num)
                            """), {"street": street_name, "num": str(addr)})
                            imported += 1
            
            session.commit()
            return imported
            
    except Exception as e:
        print(f"‚ùå Erreur import_addresses_from_cache: {e}")
        return 0


def update_street_status(street_name: str, status: str, team_id: str) -> bool:
    """Met √† jour le statut d'une rue (alias pour set_status)"""
    return set_status(street_name, status)


def get_street_notes_for_team(street_name: str, team_id: str) -> List[Dict[str, Any]]:
    """R√©cup√®re les notes d'une rue pour une √©quipe sp√©cifique"""
    try:
        with get_session() as session:
            result = session.execute(text("""
                SELECT address_number, comment, created_at
                FROM notes 
                WHERE street_name = :street AND team_id = :team
                ORDER BY created_at DESC
            """), {"street": street_name, "team": team_id})
            
            return [dict(row._mapping) for row in result]
            
    except Exception as e:
        print(f"‚ùå Erreur get_street_notes_for_team: {e}")
        return []


def add_street_note(street_name: str, team_id: str, address_number: str, comment: str) -> bool:
    """Ajoute une note pour une rue (alias pour add_note_for_address)"""
    return add_note_for_address(street_name, team_id, address_number, comment)


# =============================================================================
# COMPATIBILIT√â LEGACY
# =============================================================================

def get_backup_manager(db_path=None):
    """Compatibilit√© avec backup.py - retourne le BackupManager"""
    # Pour l'instant, utilise encore l'ancien syst√®me de backup
    # TODO: Migrer le backup vers SQLAlchemy dans Phase 2
    if db_path is None:
        db_path = Path(__file__).parent / "guigno_map.db"
    return BackupManager(db_path)


# =============================================================================
# MIGRATION PASSWORD LEGACY
# =============================================================================

def migrate_all_passwords_to_bcrypt():
    """Migre tous les mots de passe MD5 vers bcrypt"""
    try:
        with get_session() as session:
            # R√©cup√©rer toutes les √©quipes avec hash MD5
            result = session.execute(text("""
                SELECT id, password_hash 
                FROM teams 
                WHERE password_hash NOT LIKE '$2b$%' AND active = 1
            """))
            
            migrated = 0
            for row in result:
                team_id, old_hash = row
                print(f"‚ö†Ô∏è √âquipe {team_id} a un hash MD5 legacy")
                print("La migration automatique se fera lors de la prochaine connexion")
                # Note: la migration se fait automatiquement dans verify_team()
                
            print(f"‚úÖ {migrated} mots de passe √† migrer d√©tect√©s")
            
    except Exception as e:
        print(f"‚ùå Erreur migrate_all_passwords_to_bcrypt: {e}")
# --------------------------------------------------------------------------------
# <<< END FILE: src/database/db_v5.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/database/migrations/env.py
# --- encoding: utf-8 | size: 2463 bytes | sha256: 3a40c9639e531c6618cf52333f5482eb183e127d73472a83b44c78bccb6141a4 | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
from logging.config import fileConfig

from sqlalchemy import engine_from_config
from sqlalchemy import pool

from alembic import context

# Import our models and config
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__)))))

from src.database.models import Base
from src.config import get_database_url

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
target_metadata = Base.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = get_database_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Use our custom database URL instead of config
    configuration = config.get_section(config.config_ini_section, {})
    configuration["sqlalchemy.url"] = get_database_url()
    
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection, target_metadata=target_metadata
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

# --------------------------------------------------------------------------------
# <<< END FILE: src/database/migrations/env.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/database/models.py
# --- encoding: utf-8 | size: 2414 bytes | sha256: ff9206ad0810ed7f6db5f79e2e3649e92f141063aac13d3408df4b57cc3b4d3c | mtime: 2025-09-15T17:15:56-04:00
# --------------------------------------------------------------------------------
"""
Mod√®les SQLAlchemy pour GuignoMap v5.0
Bas√©s sur le sch√©ma SQLite existant pour compatibilit√©
"""
from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, ForeignKey, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()


class Street(Base):
    __tablename__ = 'streets'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    name = Column(Text, nullable=False, unique=True)
    sector = Column(Text)
    team = Column(Text)
    status = Column(Text, nullable=False, default='a_faire')
    
    # Relations
    notes = relationship("Note", back_populates="street", cascade="all, delete-orphan")
    addresses = relationship("Address", back_populates="street", cascade="all, delete-orphan")


class Team(Base):
    __tablename__ = 'teams'
    
    id = Column(Text, primary_key=True)
    name = Column(Text, nullable=False)
    password_hash = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    active = Column(Boolean, default=True)


class Note(Base):
    __tablename__ = 'notes'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    team_id = Column(Text, ForeignKey('teams.id'), nullable=False)
    address_number = Column(Text)
    comment = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="notes")


class ActivityLog(Base):
    __tablename__ = 'activity_log'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    team_id = Column(Text)
    action = Column(Text, nullable=False)
    details = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)


class Address(Base):
    __tablename__ = 'addresses'
    
    id = Column(Integer, primary_key=True, autoincrement=True)
    street_name = Column(Text, ForeignKey('streets.name'), nullable=False)
    house_number = Column(Text, nullable=False)
    latitude = Column(Float)
    longitude = Column(Float)
    osm_type = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    
    # Relations
    street = relationship("Street", back_populates="addresses")
# --------------------------------------------------------------------------------
# <<< END FILE: src/database/models.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/storage/__init__.py
# --- encoding: utf-8 | size: 4378 bytes | sha256: e7090dbdd5d38bd076cd5834928880b2e730d463b3ba6990a99f459e46ebe717 | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
"""
Adapter de stockage pour GuignoMap v5.0
S√©lection automatique entre cloud S3 et local selon configuration
"""
import os
from typing import Optional, Dict, Any
from pathlib import Path

try:
    from src.storage.cloud import (
        upload_osm_cache as cloud_upload_osm_cache,
        download_osm_cache as cloud_download_osm_cache,
        upload_backup_to_cloud as cloud_upload_backup,
        list_cloud_backups as cloud_list_backups,
        download_backup_from_cloud as cloud_download_backup
    )
    CLOUD_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è Stockage cloud non disponible: {e}")
    CLOUD_AVAILABLE = False

from src.storage.local import (
    upload_osm_cache as local_upload_osm_cache,
    download_osm_cache as local_download_osm_cache,
    upload_backup_to_cloud as local_upload_backup,
    list_cloud_backups as local_list_backups,
    download_backup_from_cloud as local_download_backup
)


def is_cloud_storage_enabled() -> bool:
    """
    D√©termine si le stockage cloud est activ√©
    V√©rifie la pr√©sence des secrets S3 et la disponibilit√© des libs
    """
    if not CLOUD_AVAILABLE:
        return False
    
    try:
        from src.config import get_s3_config
        config = get_s3_config()
        
        # V√©rifier que les cl√©s essentielles sont pr√©sentes et non vides
        required_keys = ['bucket', 'access_key', 'secret_key']
        for key in required_keys:
            if not config.get(key) or config[key] in ['', 'xxx']:
                return False
        
        return True
    except Exception:
        return False


def get_storage_backend() -> str:
    """Retourne 'cloud' ou 'local' selon la configuration"""
    return 'cloud' if is_cloud_storage_enabled() else 'local'


# API unifi√©e pour le stockage
def upload_osm_cache(cache_data: Dict[Any, Any]) -> bool:
    """Upload du cache OSM (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("üì° Upload cache OSM vers S3...")
        return cloud_upload_osm_cache(cache_data)
    else:
        print("üíæ Sauvegarde cache OSM en local...")
        return local_upload_osm_cache(cache_data)


def download_osm_cache() -> Optional[Dict[Any, Any]]:
    """Download du cache OSM (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("üì° T√©l√©chargement cache OSM depuis S3...")
        return cloud_download_osm_cache()
    else:
        print("üíæ Lecture cache OSM local...")
        return local_download_osm_cache()


def upload_backup(backup_path: Path) -> bool:
    """Upload d'un backup (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("üì° Upload backup vers S3...")
        return cloud_upload_backup(backup_path)
    else:
        print("üíæ Copie backup en local...")
        return local_upload_backup(backup_path)


def list_backups() -> list:
    """Liste des backups disponibles (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("üì° Liste backups S3...")
        return cloud_list_backups()
    else:
        print("üíæ Liste backups locaux...")
        return local_list_backups()


def download_backup(backup_key: str, local_path: Path) -> bool:
    """Download d'un backup (cloud ou local selon config)"""
    if is_cloud_storage_enabled():
        print("üì° T√©l√©chargement backup depuis S3...")
        return cloud_download_backup(backup_key, local_path)
    else:
        print("üíæ Copie backup depuis local...")
        return local_download_backup(backup_key, local_path)


def get_storage_info() -> Dict[str, Any]:
    """Informations sur le backend de stockage actuel"""
    backend = get_storage_backend()
    info = {
        'backend': backend,
        'cloud_available': CLOUD_AVAILABLE,
        'cloud_enabled': is_cloud_storage_enabled()
    }
    
    if backend == 'cloud':
        try:
            from src.config import get_s3_config
            config = get_s3_config()
            info.update({
                'bucket': config.get('bucket', ''),
                'region': config.get('region', ''),
                'cdn_enabled': bool(config.get('cdn_base_url', ''))
            })
        except:
            pass
    
    return info
# --------------------------------------------------------------------------------
# <<< END FILE: src/storage/__init__.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/storage/cloud.py
# --- encoding: utf-8 | size: 9314 bytes | sha256: 92c7f4a598f9a21b94278f01d4fd75c640190aecab3b535e13e12bfc6cb76f3c | mtime: 2025-09-15T21:50:52-04:00
# --------------------------------------------------------------------------------
"""
Stockage cloud S3 pour GuignoMap v5.0
Client boto3 pour osm_cache.json et backups
"""
import boto3
import json
import io
import os
import streamlit as st
from typing import Optional, Dict, Any, BinaryIO
from pathlib import Path
from datetime import datetime
from src.config import get_s3_config, get_cdn_base_url


class S3StorageClient:
    """Client S3 pour g√©rer osm_cache.json et backups"""
    
    def __init__(self):
        self.config = get_s3_config()
        self.cdn_base_url = get_cdn_base_url()
        self._client = None
    
    @property
    def client(self):
        """Client S3 avec lazy loading et cache Streamlit"""
        if self._client is None:
            try:
                self._client = boto3.client(
                    's3',
                    region_name=self.config['region'],
                    aws_access_key_id=self.config['access_key'],
                    aws_secret_access_key=self.config['secret_key']
                )
            except Exception as e:
                print(f"Erreur initialisation client S3: {e}")
                raise
        return self._client
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """
        Upload d'un fichier JSON vers S3
        """
        try:
            json_content = json.dumps(data, ensure_ascii=False, indent=2)
            json_bytes = json_content.encode('utf-8')
            
            extra_args: Dict[str, Any] = {
                'ContentType': 'application/json',
                'ContentEncoding': 'utf-8'
            }
            
            if metadata:
                extra_args['Metadata'] = metadata
            
            self.client.put_object(
                Bucket=self.config['bucket'],
                Key=key,
                Body=json_bytes,
                **extra_args
            )
            
            print(f"‚úÖ JSON upload√© vers S3: {key}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur upload JSON S3 {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """
        Download d'un fichier JSON depuis S3
        """
        try:
            response = self.client.get_object(
                Bucket=self.config['bucket'],
                Key=key
            )
            
            content = response['Body'].read().decode('utf-8')
            data = json.loads(content)
            
            print(f"‚úÖ JSON t√©l√©charg√© depuis S3: {key}")
            return data
            
        except self.client.exceptions.NoSuchKey:
            print(f"‚ÑπÔ∏è Fichier JSON S3 non trouv√©: {key}")
            return None
        except Exception as e:
            print(f"‚ùå Erreur download JSON S3 {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """
        Upload d'un fichier backup vers S3
        """
        try:
            if not backup_file_path.exists():
                print(f"‚ùå Fichier backup non trouv√©: {backup_file_path}")
                return False
            
            # G√©n√©rer la cl√© S3 si non fournie
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                s3_key = f"backups/{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            
            # Upload avec streaming pour gros fichiers
            with open(backup_file_path, 'rb') as f:
                self.client.upload_fileobj(
                    f,
                    self.config['bucket'],
                    s3_key,
                    ExtraArgs={
                        'ContentType': 'application/zip',
                        'Metadata': {
                            'original_filename': backup_file_path.name,
                            'upload_timestamp': datetime.utcnow().isoformat()
                        }
                    }
                )
            
            print(f"‚úÖ Backup upload√© vers S3: {s3_key}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur upload backup S3: {e}")
            return False
    
    def download_backup(self, s3_key: str, local_path: Path) -> bool:
        """
        Download d'un backup depuis S3
        """
        try:
            # Cr√©er le r√©pertoire parent si n√©cessaire
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Download avec streaming
            with open(local_path, 'wb') as f:
                self.client.download_fileobj(
                    self.config['bucket'],
                    s3_key,
                    f
                )
            
            print(f"‚úÖ Backup t√©l√©charg√© depuis S3: {s3_key} ‚Üí {local_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur download backup S3 {s3_key}: {e}")
            return False
    
    def list_backups(self, prefix: str = "backups/") -> list:
        """
        Liste des backups disponibles sur S3
        """
        try:
            response = self.client.list_objects_v2(
                Bucket=self.config['bucket'],
                Prefix=prefix
            )
            
            backups = []
            if 'Contents' in response:
                for obj in response['Contents']:
                    backups.append({
                        'key': obj['Key'],
                        'size': obj['Size'],
                        'last_modified': obj['LastModified'],
                        'filename': Path(obj['Key']).name
                    })
                
                # Trier par date de modification (plus r√©cent en premier)
                backups.sort(key=lambda x: x['last_modified'], reverse=True)
            
            return backups
            
        except Exception as e:
            print(f"‚ùå Erreur liste backups S3: {e}")
            return []
    
    def delete_file(self, key: str) -> bool:
        """
        Suppression d'un fichier sur S3
        """
        try:
            self.client.delete_object(
                Bucket=self.config['bucket'],
                Key=key
            )
            print(f"‚úÖ Fichier supprim√© de S3: {key}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur suppression S3 {key}: {e}")
            return False
    
    def file_exists(self, key: str) -> bool:
        """
        V√©rifier si un fichier existe sur S3
        """
        try:
            self.client.head_object(
                Bucket=self.config['bucket'],
                Key=key
            )
            return True
        except self.client.exceptions.NoSuchKey:
            return False
        except Exception as e:
            print(f"‚ùå Erreur v√©rification existence S3 {key}: {e}")
            return False
    
    def get_public_url(self, key: str, expires_in: int = 3600) -> Optional[str]:
        """
        G√©n√©rer URL publique sign√©e pour un fichier S3
        """
        try:
            # Si CDN configur√©, utiliser l'URL CDN
            if self.cdn_base_url:
                return f"{self.cdn_base_url.rstrip('/')}/{key}"
            
            # Sinon, g√©n√©rer URL sign√©e S3
            url = self.client.generate_presigned_url(
                'get_object',
                Params={'Bucket': self.config['bucket'], 'Key': key},
                ExpiresIn=expires_in
            )
            return url
            
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration URL publique S3 {key}: {e}")
            return None


# Instance globale pour cache Streamlit
@st.cache_resource
def get_s3_client() -> S3StorageClient:
    """Factory avec cache Streamlit pour client S3"""
    return S3StorageClient()


# API simplifi√©e pour les fonctions m√©tier
def upload_osm_cache(cache_data: Dict[Any, Any]) -> bool:
    """Upload du cache OSM vers S3"""
    client = get_s3_client()
    return client.upload_json_file(
        "osm_cache.json", 
        cache_data,
        metadata={
            'type': 'osm_cache',
            'updated_at': datetime.utcnow().isoformat()
        }
    )


def download_osm_cache() -> Optional[Dict[Any, Any]]:
    """Download du cache OSM depuis S3"""
    client = get_s3_client()
    return client.download_json_file("osm_cache.json")


def upload_backup_to_cloud(backup_path: Path) -> bool:
    """Upload d'un backup vers S3"""
    client = get_s3_client()
    return client.upload_backup(backup_path)


def list_cloud_backups() -> list:
    """Liste des backups cloud disponibles"""
    client = get_s3_client()
    return client.list_backups()


def download_backup_from_cloud(s3_key: str, local_path: Path) -> bool:
    """Download d'un backup depuis S3"""
    client = get_s3_client()
    return client.download_backup(s3_key, local_path)
# --------------------------------------------------------------------------------
# <<< END FILE: src/storage/cloud.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/storage/local.py
# --- encoding: utf-8 | size: 9803 bytes | sha256: 97d7ac65d9d8a92851ce7ca538629da28ac39e111455b8e915303a94358a73e7 | mtime: 2025-09-15T16:02:23-04:00
# --------------------------------------------------------------------------------
"""
Stockage local pour GuignoMap v5.0  
Fallback avec API identique √† cloud.py
"""
import json
import shutil
import os
from typing import Optional, Dict, Any
from pathlib import Path
from datetime import datetime


class LocalStorageClient:
    """Client stockage local avec API identique au client S3"""
    
    def __init__(self, base_path: Optional[Path] = None):
        # R√©pertoire de base pour le stockage local
        if base_path is None:
            base_path = Path(__file__).parent.parent.parent / "storage_local"
        
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
        
        # Sous-r√©pertoires
        self.backups_dir = self.base_path / "backups"
        self.backups_dir.mkdir(exist_ok=True)
    
    def upload_json_file(self, key: str, data: Dict[Any, Any], metadata: Optional[Dict[str, str]] = None) -> bool:
        """
        Sauvegarde d'un fichier JSON en local
        """
        try:
            file_path = self.base_path / key
            file_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarder les donn√©es JSON
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # Sauvegarder les m√©tadonn√©es si fournies
            if metadata:
                metadata_path = file_path.with_suffix('.metadata.json')
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ JSON sauv√© localement: {file_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur sauvegarde JSON local {key}: {e}")
            return False
    
    def download_json_file(self, key: str) -> Optional[Dict[Any, Any]]:
        """
        Lecture d'un fichier JSON local
        """
        try:
            file_path = self.base_path / key
            
            if not file_path.exists():
                print(f"‚ÑπÔ∏è Fichier JSON local non trouv√©: {file_path}")
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"‚úÖ JSON lu localement: {file_path}")
            return data
            
        except Exception as e:
            print(f"‚ùå Erreur lecture JSON local {key}: {e}")
            return None
    
    def upload_backup(self, backup_file_path: Path, s3_key: Optional[str] = None) -> bool:
        """
        Copie d'un fichier backup vers le r√©pertoire local
        """
        try:
            if not backup_file_path.exists():
                print(f"‚ùå Fichier backup non trouv√©: {backup_file_path}")
                return False
            
            # G√©n√©rer le nom de destination si non fourni
            if not s3_key:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                dest_name = f"{backup_file_path.stem}_{timestamp}{backup_file_path.suffix}"
            else:
                # Extraire le nom du fichier de la cl√© S3
                dest_name = Path(s3_key).name
            
            dest_path = self.backups_dir / dest_name
            
            # Copier le fichier
            shutil.copy2(backup_file_path, dest_path)
            
            # Cr√©er un fichier de m√©tadonn√©es
            metadata = {
                'original_filename': backup_file_path.name,
                'original_path': str(backup_file_path),
                'upload_timestamp': datetime.utcnow().isoformat(),
                'size': backup_file_path.stat().st_size
            }
            
            metadata_path = dest_path.with_suffix(dest_path.suffix + '.metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"‚úÖ Backup copi√© localement: {dest_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur copie backup local: {e}")
            return False
    
    def download_backup(self, s3_key: str, local_path: Path) -> bool:
        """
        Copie d'un backup depuis le stockage local
        """
        try:
            # Trouver le fichier source
            source_name = Path(s3_key).name
            source_path = self.backups_dir / source_name
            
            if not source_path.exists():
                print(f"‚ùå Backup local non trouv√©: {source_path}")
                return False
            
            # Cr√©er le r√©pertoire de destination
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Copier le fichier
            shutil.copy2(source_path, local_path)
            
            print(f"‚úÖ Backup copi√© depuis local: {source_path} ‚Üí {local_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur copie backup depuis local {s3_key}: {e}")
            return False
    
    def list_backups(self, prefix: str = "backups/") -> list:
        """
        Liste des backups disponibles en local
        """
        try:
            backups = []
            
            # Lister tous les fichiers (sauf m√©tadonn√©es)
            for backup_file in self.backups_dir.glob("*"):
                if backup_file.is_file() and not backup_file.name.endswith('.metadata.json'):
                    # Lire les m√©tadonn√©es si disponibles
                    metadata_path = backup_file.with_suffix(backup_file.suffix + '.metadata.json')
                    metadata = {}
                    if metadata_path.exists():
                        try:
                            with open(metadata_path, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                        except:
                            pass
                    
                    stat = backup_file.stat()
                    backups.append({
                        'key': f"backups/{backup_file.name}",
                        'size': stat.st_size,
                        'last_modified': datetime.fromtimestamp(stat.st_mtime),
                        'filename': backup_file.name,
                        'metadata': metadata
                    })
            
            # Trier par date de modification (plus r√©cent en premier)
            backups.sort(key=lambda x: x['last_modified'], reverse=True)
            return backups
            
        except Exception as e:
            print(f"‚ùå Erreur liste backups locaux: {e}")
            return []
    
    def delete_file(self, key: str) -> bool:
        """
        Suppression d'un fichier local
        """
        try:
            file_path = self.base_path / key
            
            if file_path.exists():
                file_path.unlink()
                
                # Supprimer les m√©tadonn√©es si elles existent
                metadata_path = file_path.with_suffix('.metadata.json')
                if metadata_path.exists():
                    metadata_path.unlink()
                
                print(f"‚úÖ Fichier supprim√© localement: {file_path}")
                return True
            else:
                print(f"‚ÑπÔ∏è Fichier local non trouv√©: {file_path}")
                return False
            
        except Exception as e:
            print(f"‚ùå Erreur suppression fichier local {key}: {e}")
            return False
    
    def file_exists(self, key: str) -> bool:
        """
        V√©rifier si un fichier existe en local
        """
        file_path = self.base_path / key
        return file_path.exists()
    
    def get_public_url(self, key: str, expires_in: int = 3600) -> Optional[str]:
        """
        G√©n√©rer un chemin local pour un fichier (pas d'URL publique)
        """
        file_path = self.base_path / key
        if file_path.exists():
            return f"file://{file_path.absolute()}"
        return None


# Instance globale pour le stockage local
_local_client = None

def get_local_client() -> LocalStorageClient:
    """Factory pour client de stockage local"""
    global _local_client
    if _local_client is None:
        _local_client = LocalStorageClient()
    return _local_client


# API simplifi√©e pour les fonctions m√©tier (identique √† cloud.py)
def upload_osm_cache(cache_data: Dict[Any, Any]) -> bool:
    """Upload du cache OSM en local"""
    client = get_local_client()
    return client.upload_json_file(
        "osm_cache.json", 
        cache_data,
        metadata={
            'type': 'osm_cache',
            'updated_at': datetime.utcnow().isoformat()
        }
    )


def download_osm_cache() -> Optional[Dict[Any, Any]]:
    """Download du cache OSM depuis local"""
    client = get_local_client()
    return client.download_json_file("osm_cache.json")


def upload_backup_to_cloud(backup_path: Path) -> bool:
    """Upload d'un backup en local"""
    client = get_local_client()
    return client.upload_backup(backup_path)


def list_cloud_backups() -> list:
    """Liste des backups locaux disponibles"""
    client = get_local_client()
    return client.list_backups()


def download_backup_from_cloud(s3_key: str, local_path: Path) -> bool:
    """Download d'un backup depuis local"""
    client = get_local_client()
    return client.download_backup(s3_key, local_path)
# --------------------------------------------------------------------------------
# <<< END FILE: src/storage/local.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/utils/__init__.py
# --- encoding: utf-8 | size: 28 bytes | sha256: f99714d689dff3eacaf8ffe691ccf22445b944d6a8902885a19d5fc3e9d48753 | mtime: 2025-09-15T17:40:53-04:00
# --------------------------------------------------------------------------------
# Utils module for GuignoMap
# --------------------------------------------------------------------------------
# <<< END FILE: src/utils/__init__.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: src/utils/adapters.py
# --- encoding: utf-8 | size: 1007 bytes | sha256: 1fd5bf01323890146b3c591228428580bfe0b2088f9cf570cbfdc9f3af48e38c | mtime: 2025-09-15T17:40:53-04:00
# --------------------------------------------------------------------------------
import pandas as pd
from typing import Any, Iterable

def to_dataframe(records: Any) -> pd.DataFrame:
    if isinstance(records, pd.DataFrame):
        return records
    # RowMapping unique
    try:
        if hasattr(records, "keys") and hasattr(records, "__getitem__"):
            return pd.DataFrame([dict(records)])
    except Exception:
        pass
    # S√©quences (list[dict]/list[Row]/list[ORM])
    if isinstance(records, Iterable):
        items = list(records)
        if items and not isinstance(items[0], dict):
            dicts = []
            for r in items:
                if hasattr(r, "__dict__"):
                    d = {k:v for k,v in r.__dict__.items() if not k.startswith("_sa_")}
                    dicts.append(d)
                else:
                    try: dicts.append(dict(r))
                    except Exception: dicts.append({"value": r})
            return pd.DataFrame(dicts)
        return pd.DataFrame(items)
    return pd.DataFrame([])
# --------------------------------------------------------------------------------
# <<< END FILE: src/utils/adapters.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: streamlit_app.py
# --- encoding: utf-8 | size: 845 bytes | sha256: bcc5adcc241667c884a543c8d6c6aac367898b68e13bdb85c7466c930d6c7b83 | mtime: 2025-09-15T23:08:55-04:00
# --------------------------------------------------------------------------------
# coding: utf-8
import sys, pathlib, importlib, traceback
import streamlit as st

ROOT = pathlib.Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

try:
    app = importlib.import_module("guignomap.app")
    for fn_name in ("main", "run", "render"):
        fn = getattr(app, fn_name, None)
        if callable(fn):
            fn()
            break
    else:
        st.caption("GuignoMap charg√© (mode import).")
except Exception:
    st.error("Erreur de d√©marrage :")
    st.code(traceback.format_exc())
    import platform, streamlit, sqlalchemy
    st.write({
        "python": platform.python_version(),
        "streamlit": streamlit.__version__,
        "sqlalchemy": sqlalchemy.__version__,
        "secrets_keys": list(getattr(st, "secrets", {}).keys()),
    })
# --------------------------------------------------------------------------------
# <<< END FILE: streamlit_app.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: tests/manual/test_db_connection.py
# --- encoding: utf-8 | size: 3001 bytes | sha256: af57057a1127821f31e61ce6effe055a68bc69f4941732c1797e54289ed442fe | mtime: 2025-09-15T16:30:09-04:00
# --------------------------------------------------------------------------------
import os
import sys
import socket
import psycopg2

def test_connection_with_ip():
    """Test connection with direct IPv6 address"""
    # Configuration manuelle avec l'adresse IPv6 r√©solue
    ipv6_host = "2600:1f11:4e2:e202:6514:7431:494f:c00f"
    
    connection_string = f"postgresql://postgres:4everSab!2304@[{ipv6_host}]:5432/postgres"
    
    print(f"Test de connexion avec IPv6 directe: {ipv6_host}")
    
    try:
        conn = psycopg2.connect(connection_string)
        print("‚úÖ Connexion IPv6 r√©ussie!")
        
        with conn.cursor() as cursor:
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            print(f"Version PostgreSQL: {version}")
        
        conn.close()
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur de connexion IPv6: {e}")
        return False

def test_connection_with_hostname():
    """Test connection with hostname via custom DNS"""
    import socket
    
    # Forcer IPv4 si possible
    try:
        socket.setdefaulttimeout(10)
        original_getaddrinfo = socket.getaddrinfo
        
        def custom_getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):
            if host == "db.kdxqspmfycnwzzrmhzpa.supabase.co":
                # Retourner directement l'IPv6 connue
                return [(socket.AF_INET6, socket.SOCK_STREAM, 6, '', 
                        ('2600:1f11:4e2:e202:6514:7431:494f:c00f', port, 0, 0))]
            return original_getaddrinfo(host, port, family, type, proto, flags)
        
        socket.getaddrinfo = custom_getaddrinfo
        
        connection_string = "postgresql://postgres:4everSab!2304@db.kdxqspmfycnwzzrmhzpa.supabase.co:5432/postgres"
        
        print("Test de connexion avec hostname (DNS custom)")
        conn = psycopg2.connect(connection_string)
        print("‚úÖ Connexion hostname r√©ussie!")
        
        with conn.cursor() as cursor:
            cursor.execute("SELECT version();")
            version = cursor.fetchone()[0]
            print(f"Version PostgreSQL: {version}")
        
        conn.close()
        socket.getaddrinfo = original_getaddrinfo
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur de connexion hostname: {e}")
        socket.getaddrinfo = original_getaddrinfo
        return False

if __name__ == "__main__":
    print("=== Test de connectivit√© Supabase PostgreSQL ===")
    
    print("\n1. Test avec adresse IPv6 directe:")
    ipv6_success = test_connection_with_ip()
    
    print("\n2. Test avec hostname (DNS custom):")
    hostname_success = test_connection_with_hostname()
    
    if ipv6_success or hostname_success:
        print("\n‚úÖ Au moins une m√©thode de connexion fonctionne!")
    else:
        print("\n‚ùå Aucune m√©thode de connexion ne fonctionne")
        print("Probl√®me potentiel: connectivit√© IPv6 ou firewall")
# --------------------------------------------------------------------------------
# <<< END FILE: tests/manual/test_db_connection.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: tests/manual/test_db_simple.py
# --- encoding: utf-8 | size: 1202 bytes | sha256: 79f1d7059af661939e4d1e4ca7e64e1eb817fa6c3cab50158e01cacf578a4b81 | mtime: 2025-09-15T16:30:09-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""Test simple de connexion base de donn√©es"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_db_simple():
    try:
        from src.database.connection import get_engine, test_connection
        
        print("=== Test de connexion base de donn√©es ===")
        
        # Test avec la fonction d√©di√©e
        print("1. Test avec test_connection():")
        test_connection()
        
        # Test manuel avec engine
        print("\n2. Test manuel avec engine:")
        engine = get_engine()
        print(f"‚úÖ Engine cr√©√©: {engine.url}")
        
        # Tester la connexion
        with engine.connect() as conn:
            result = conn.execute("SELECT 1 as test")
            test_value = result.fetchone()[0]
            print(f"‚úÖ Connexion r√©ussie! Test query result: {test_value}")
        
        print("‚úÖ Connexion base de donn√©es fonctionnelle!")
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur de connexion: {e}")
        return False

if __name__ == "__main__":
    success = test_db_simple()
    sys.exit(0 if success else 1)
# --------------------------------------------------------------------------------
# <<< END FILE: tests/manual/test_db_simple.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# >>> BEGIN FILE: tools/quick_sanity.py
# --- encoding: utf-8 | size: 7074 bytes | sha256: 0f3d5be22a7582185ed439012cd87a859139d48b9bc9ef0be378fd84e24216db | mtime: 2025-09-14T16:46:24-04:00
# --------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
Quick sanity check script for GuignoMap v4.1
Generates audit CSV files and validates data integrity with assertions.
"""

import sqlite3
import csv
import sys
from datetime import datetime
from pathlib import Path

def main():
    """Main sanity check function with data integrity assertions."""
    # Paths
    db_path = Path("guignomap") / "guigno_map.db"
    exports_dir = Path("exports")
    
    # Create exports directory if missing
    exports_dir.mkdir(exist_ok=True)
    
    # Check if database exists
    if not db_path.exists():
        print(f"‚ùå Database not found: {db_path}")
        print("‚ÑπÔ∏è  Run the application first to create the database.")
        print("SANITY: FAIL - Database missing")
        return 1
    
    try:
        # Connect to database
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # Generate timestamp for files
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        
        print("üîç GuignoMap v4.1 - Quick Sanity Check with Assertions")
        print("=" * 60)
        
        # Check if required tables exist
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [row[0] for row in cursor.fetchall()]
        
        if 'streets' not in tables:
            print("‚ùå Table 'streets' not found")
            print("SANITY: FAIL - Missing streets table")
            return 1
        
        # === DATA COLLECTION ===
        
        # 1. Total streets count
        cursor.execute("SELECT COUNT(*) FROM streets")
        total_streets = cursor.fetchone()[0]
        
        # 2. Count unassigned streets (team IS NULL OR team = '')
        cursor.execute("SELECT COUNT(*) FROM streets WHERE team IS NULL OR team = ''")
        unassigned_count = cursor.fetchone()[0]
        
        # 3. Status distribution with counts
        cursor.execute("""
            SELECT 
                COALESCE(status, 'Non d√©fini') as status,
                COUNT(*) as count
            FROM streets 
            GROUP BY status 
            ORDER BY count DESC
        """)
        status_counts = cursor.fetchall()
        
        # 4. Unassigned streets by sector (for CSV)
        cursor.execute("""
            SELECT 
                COALESCE(sector, 'Aucun') as sector,
                name
            FROM streets 
            WHERE team IS NULL OR team = ''
            ORDER BY sector, name
        """)
        unassigned_streets = cursor.fetchall()
        
        # 5. Top 10 streets (for display)
        cursor.execute("""
            SELECT 
                COALESCE(sector, 'Aucun') as sector,
                name,
                COALESCE(team, 'Non assign√©e') as team,
                COALESCE(status, 'Non d√©fini') as status
            FROM streets 
            ORDER BY name 
            LIMIT 10
        """)
        top_streets = cursor.fetchall()
        
        # === ASSERTIONS & DATA INTEGRITY CHECKS ===
        
        sanity_pass = True
        fail_reasons = []
        
        # Assertion 1: Total should equal sum of status counts
        sum_status_counts = sum(count for _, count in status_counts)
        if total_streets != sum_status_counts:
            sanity_pass = False
            fail_reasons.append(f"Total streets ({total_streets}) != sum of status counts ({sum_status_counts})")
        
        # Assertion 2: Unassigned count should match COUNT(team IS NULL OR team = '')
        # (This is redundant since we're using the same query, but validates consistency)
        cursor.execute("SELECT COUNT(*) FROM streets WHERE team IS NULL OR team = ''")
        unassigned_recheck = cursor.fetchone()[0]
        if unassigned_count != unassigned_recheck:
            sanity_pass = False
            fail_reasons.append(f"Unassigned count inconsistent ({unassigned_count} vs {unassigned_recheck})")
        
        # Assertion 3: No negative counts
        for status, count in status_counts:
            if count < 0:
                sanity_pass = False
                fail_reasons.append(f"Negative count for status '{status}': {count}")
        
        # Assertion 4: Total should be positive if any data exists
        if status_counts and total_streets <= 0:
            sanity_pass = False
            fail_reasons.append(f"Invalid total streets count: {total_streets}")
        
        # === DISPLAY RESULTS ===
        
        print(f"üìä Total des rues: {total_streets}")
        print(f"üìä Rues non assign√©es: {unassigned_count}")
        print()
        
        print("üìà R√©partition par statut:")
        for status, count in status_counts:
            print(f"  ‚Ä¢ {status}: {count}")
        print(f"  üìã Somme des statuts: {sum_status_counts}")
        print()
        
        print("üìç Top 10 rues (alphab√©tique):")
        for sector, name, team, status in top_streets[:10]:
            print(f"  ‚Ä¢ {sector} | {name} | {team} | {status}")
        print()
        
        # === WRITE CSV FILES (ALWAYS) ===
        
        status_file = exports_dir / f"sanity_status_counts_{timestamp}.csv"
        unassigned_file = exports_dir / f"sanity_unassigned_{timestamp}.csv"
        
        try:
            # Status counts CSV
            with open(status_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['status', 'count'])
                writer.writerows(status_counts)
            
            # Unassigned streets CSV
            with open(unassigned_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['secteur', 'rue'])
                writer.writerows(unassigned_streets)
            
            print("üìÅ Fichiers CSV cr√©√©s:")
            print(f"  ‚Ä¢ {status_file}")
            print(f"  ‚Ä¢ {unassigned_file}")
            print()
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de l'√©criture des CSV: {e}")
            print()
        
        # === FINAL SANITY CHECK RESULT ===
        
        conn.close()
        
        if sanity_pass:
            print("‚úÖ Tous les tests de coh√©rence sont pass√©s")
            print("SANITY: PASS")
            return 0
        else:
            print("‚ùå √âchec des tests de coh√©rence:")
            for reason in fail_reasons:
                print(f"  ‚Ä¢ {reason}")
            print("SANITY: FAIL - Data integrity issues")
            return 1
            
    except sqlite3.Error as e:
        print(f"‚ùå Erreur base de donn√©es: {e}")
        print("SANITY: FAIL - Database error")
        return 1
    except Exception as e:
        print(f"‚ùå Erreur inattendue: {e}")
        print("SANITY: FAIL - Unexpected error")
        return 1

if __name__ == "__main__":
    sys.exit(main())
# --------------------------------------------------------------------------------
# <<< END FILE: tools/quick_sanity.py
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# ================================================================================
# R√âSUM√â SOURCES
#   Fichiers .py inclus : 37
# ================================================================================

## ================================================================================
# ARBRE DU PROJET GUIGNOMAP - STRUCTURE COMPL√àTE ET PROPRE
# G√©n√©r√© le: 2025-09-16 11:05:00
# Exclusions: __pycache__, .venv, backups/, exports/, .git, *.pyc, *.log, *.db
# ================================================================================

GuignoMap/
‚îú‚îÄ‚îÄ .devcontainer/
‚îÇ   ‚îî‚îÄ‚îÄ devcontainer.json                    # Configuration Dev Container
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îú‚îÄ‚îÄ config.toml                          # ‚≠ê Configuration Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ secrets.toml                         # üîê Secrets (d√©veloppement)
‚îÇ   ‚îî‚îÄ‚îÄ secrets.toml.example                 # üìã Template secrets
‚îú‚îÄ‚îÄ .vscode/
‚îÇ   ‚îú‚îÄ‚îÄ launch.json                          # Configuration debug VS Code
‚îÇ   ‚îú‚îÄ‚îÄ settings.json                        # Param√®tres VS Code
‚îÇ   ‚îî‚îÄ‚îÄ tasks.json                           # T√¢ches automatis√©es
‚îú‚îÄ‚îÄ docs/                                    # üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ PHASE1_AUDIT_DATAFRAME.md            # Audit DataFrame
‚îÇ   ‚îî‚îÄ‚îÄ PROBLEME_IPV6_SUPABASE.md            # Documentation probl√®me IPv6
‚îú‚îÄ‚îÄ guignomap/                               # üéØ Application principale
‚îÇ   ‚îú‚îÄ‚îÄ assets/                              # üé® Ressources statiques
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ banner.png                       # Banni√®re application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ guignolee.png                    # Logo Guignol√©e
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logo.png                         # Logo Relais
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles.css                       # ‚≠ê Styles CSS
‚îÇ   ‚îú‚îÄ‚îÄ logs/                                # (vide - exclus .log)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                          # Module Python
‚îÇ   ‚îú‚îÄ‚îÄ app.py                               # ‚≠ê Interface Streamlit (2000+ lignes)
‚îÇ   ‚îú‚îÄ‚îÄ backup.py                            # üíæ Syst√®me de sauvegarde
‚îÇ   ‚îú‚îÄ‚îÄ db.py                                # üîÑ Legacy DB (migration)
‚îÇ   ‚îú‚îÄ‚îÄ db_v5.py                             # ‚≠ê Base de donn√©es SQLAlchemy  
‚îÇ   ‚îú‚îÄ‚îÄ osm.py                               # ‚≠ê Int√©gration OpenStreetMap
‚îÇ   ‚îú‚îÄ‚îÄ osm_addresses.json                   # Cache adresses OSM
‚îÇ   ‚îú‚îÄ‚îÄ osm_cache.json                       # Cache g√©om√©trie OSM
‚îÇ   ‚îú‚îÄ‚îÄ reports.py                           # üìä G√©n√©ration rapports
‚îÇ   ‚îî‚îÄ‚îÄ validators.py                        # üõ°Ô∏è Validation s√©curis√©e
‚îú‚îÄ‚îÄ scripts/                                 # üîß Scripts utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ export_repo_audit.py                # Export pour audit
‚îÇ   ‚îú‚îÄ‚îÄ export_repo_min.py                  # Export minimal
‚îÇ   ‚îú‚îÄ‚îÄ export_repo_snapshot.py             # Snapshot repository
‚îÇ   ‚îú‚îÄ‚îÄ fix_app_types.py                    # Corrections types
‚îÇ   ‚îú‚îÄ‚îÄ fix_specific.py                     # Corrections sp√©cifiques
‚îÇ   ‚îú‚îÄ‚îÄ generate_audit_export.py            # üìã G√©n√©rateur export audit
‚îÇ   ‚îú‚îÄ‚îÄ generate_audit_optimise.py          # ‚≠ê G√©n√©rateur export optimis√©
‚îÇ   ‚îú‚îÄ‚îÄ migrate_password_hashes.py          # üîê Migration Argon2
‚îÇ   ‚îú‚îÄ‚îÄ migrate_sqlite_to_postgres.py       # üîÑ Migration PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ sanity_db_pandas.py                 # üîç V√©rification DB
‚îÇ   ‚îú‚îÄ‚îÄ smoke_create_map.py                 # üó∫Ô∏è Test carte
‚îÇ   ‚îî‚îÄ‚îÄ validation_dataframe.ps1            # Script validation PowerShell
‚îú‚îÄ‚îÄ src/                                     # üèóÔ∏è Architecture modulaire
‚îÇ   ‚îú‚îÄ‚îÄ auth/                                # üîê Authentification
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ passwords.py                     # ‚≠ê Authentification Argon2
‚îÇ   ‚îú‚îÄ‚îÄ database/                            # üíæ Base de donn√©es
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ migrations/                      # üîÑ Migrations Alembic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ versions/                    # Versions migrations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ env.py                       # Configuration Alembic
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README                       # Documentation migrations
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ script.py.mako               # Template migration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ connection.py                    # ‚≠ê Connexions PostgreSQL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db_v5.py                         # ‚≠ê Op√©rations base (duplicate?)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py                        # ‚≠ê Mod√®les SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ storage/                             # üíæ Stockage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                      # Configuration stockage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cloud.py                         # ‚òÅÔ∏è Stockage S3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ local.py                         # üíª Stockage local
‚îÇ   ‚îú‚îÄ‚îÄ utils/                               # üõ†Ô∏è Utilitaires
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                      # Module utils
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adapters.py                      # üîÑ Adaptateurs donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                          # Module src
‚îÇ   ‚îî‚îÄ‚îÄ config.py                            # ‚≠ê Configuration centralis√©e
‚îú‚îÄ‚îÄ tests/                                   # üß™ Tests
‚îÇ   ‚îî‚îÄ‚îÄ manual/                              # Tests manuels
‚îÇ       ‚îú‚îÄ‚îÄ test_db_connection.py            # Test connexion DB
‚îÇ       ‚îî‚îÄ‚îÄ test_db_simple.py                # Test simple DB
‚îú‚îÄ‚îÄ tools/                                   # ‚ö° Outils
‚îÇ   ‚îî‚îÄ‚îÄ quick_sanity.py                      # ‚ö° V√©rification rapide
‚îú‚îÄ‚îÄ .gitignore                               # üö´ Exclusions Git
‚îú‚îÄ‚îÄ alembic.ini                              # üîß Configuration Alembic
‚îú‚îÄ‚îÄ audit_dataframe.ps1                     # üìã Script audit PowerShell
‚îú‚îÄ‚îÄ generate_tree_clean.py                  # üå≥ G√©n√©rateur arbre propre
‚îú‚îÄ‚îÄ GuignoMap_code_export_20250914_audit.txt # üìÑ Export audit pr√©c√©dent
‚îú‚îÄ‚îÄ GuignoMap_code_export_20250915_final_UTF8.txt # üìÑ Export final pr√©c√©dent
‚îú‚îÄ‚îÄ lancer_guignomap.bat                    # üöÄ Lanceur Windows Batch
‚îú‚îÄ‚îÄ lancer_guignomap.ps1                    # üöÄ Lanceur PowerShell
‚îú‚îÄ‚îÄ PHASE1_COMMANDS.md                      # üìã Documentation Phase 1
‚îú‚îÄ‚îÄ README.md                               # üìñ Documentation principale
‚îú‚îÄ‚îÄ README_VENV.md                          # üìñ Documentation environnement
‚îú‚îÄ‚îÄ requirements.txt                        # üì¶ D√©pendances production
‚îú‚îÄ‚îÄ requirements_freeze.txt                 # üì¶ Versions exactes
‚îú‚îÄ‚îÄ requirements_freeze_bom.txt             # üì¶ Backup versions (BOM)
‚îú‚îÄ‚îÄ requirements_freeze_old.txt             # üì¶ Backup versions (ancien)
‚îú‚îÄ‚îÄ runtime.txt                             # üêç Version Python (Streamlit Cloud)
‚îî‚îÄ‚îÄ streamlit_app.py                        # ‚≠ê Point d'entr√©e Cloud

# ================================================================================
# STATISTIQUES FINALES
# ================================================================================

üìä Total √©l√©ments inclus: 88
üö´ Exclusions: __pycache__, .venv, backups/, exports/, .git, *.pyc, *.log, *.db
üìÅ Dossiers principaux: 16
üìÑ Fichiers source Python: 24
üìÑ Fichiers configuration: 12
üìÑ Fichiers documentation: 8
üìÑ Autres fichiers: 32

# ================================================================================
# DOSSIERS EXCLUS (polluants)
# ================================================================================

‚ùå __pycache__/                             # Cache Python bytecode
‚ùå .venv/                                   # Environnement virtuel (165MB+)
‚ùå backups/                                 # Sauvegardes automatiques
‚ùå exports/                                 # Exports CSV pr√©c√©dents  
‚ùå .git/                                    # M√©tadonn√©es Git
‚ùå *.pyc, *.pyo, *.pyd                      # Bytecode Python
‚ùå *.log                                    # Fichiers de logs
‚ùå *.db, *.sqlite                           # Bases de donn√©es locales

# ================================================================================
# POINTS CL√âS POUR L'AUDIT
# ================================================================================

üîç FICHIERS CRITIQUES (s√©curit√©):
- src/auth/passwords.py                     # Hachage Argon2
- guignomap/validators.py                   # Anti-XSS/injection
- .streamlit/secrets.toml                   # Configuration secrets
- src/config.py                             # Gestion variables environnement

üìä FICHIERS M√âTIER (application):
- guignomap/app.py                          # Interface utilisateur principale
- guignomap/db_v5.py + src/database/        # Couche acc√®s donn√©es
- guignomap/osm.py                          # Int√©gration cartes
- guignomap/reports.py                      # G√©n√©ration rapports

‚öôÔ∏è FICHIERS INFRASTRUCTURE:
- src/database/connection.py                # Pool connexions PostgreSQL
- src/storage/cloud.py                      # Int√©gration S3
- alembic.ini + src/database/migrations/    # Migrations base
- .streamlit/config.toml                    # Configuration interface

üîß SCRIPTS MAINTENANCE:
- scripts/migrate_*.py                      # Scripts migration
- scripts/sanity_*.py                       # Scripts validation
- tools/quick_sanity.py                     # V√©rifications rapides

ARCHITECTURE: Streamlit + PostgreSQL + Folium + S3 + Alembic
D√âPLOIEMENT: Compatible Streamlit Cloud + Docker
VERSION: v3.0 Production

===============================================================================
FIN ARBRE PROJET GUIGNOMAP - 2025-09-16 11:05:00
===============================================================================
# ================================================================================
