#!/usr/bin/env python3
"""
GuignoMap - V√©rification robuste des adresses import√©es
Compare Excel source vs DB et d√©tecte les anomalies
"""
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime
import pandas as pd

# Ajouter le r√©pertoire parent au path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from guignomap.imports import detect_schema, prepare_dataframe
from guignomap.database import get_conn


def setup_logging(verbose: bool = False):
    """Configure le logging"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )


def verify_addresses_robust(city: str, excel_file: Path, verbose: bool = False) -> bool:
    """
    V√©rification robuste avec comparaison Excel vs DB
    
    Returns:
        bool: True si v√©rification OK, False si anomalies d√©tect√©es
    """
    setup_logging(verbose)
    
    try:
        # 1. Rechargement et pr√©paration Excel
        logging.info(f"üîÑ Rechargement du fichier Excel: {excel_file}")
        if not excel_file.exists():
            logging.error(f"Fichier Excel introuvable: {excel_file}")
            return False
        
        df_excel = pd.read_excel(excel_file)
        logging.info(f"üìä Excel: {len(df_excel)} lignes, {len(df_excel.columns)} colonnes")
        
        # 2. Pr√©paration des donn√©es Excel (m√™me logique que l'import)
        mapping = detect_schema(df_excel, city)
        if 'street' not in mapping or 'number' not in mapping:
            logging.error("Impossible de d√©tecter les colonnes rue/num√©ro")
            return False
        
        df_prepared = prepare_dataframe(df_excel, mapping, city)
        logging.info(f"‚úÖ Excel pr√©par√©: {len(df_prepared)} adresses uniques")
        
        # 3. Lecture de la base de donn√©es
        logging.info("üîç Analyse de la base de donn√©es...")
        with get_conn() as conn:
            cursor = conn.cursor()
            
            # Statistiques DB
            cursor.execute("SELECT COUNT(*) FROM addresses")
            db_total = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT addr_key) FROM addresses")
            db_unique_keys = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM addresses WHERE assigned_to IS NOT NULL AND assigned_to != ''")
            db_assigned = cursor.fetchone()[0]
            
            logging.info(f"üíæ DB: {db_total} adresses, {db_unique_keys} cl√©s uniques, {db_assigned} assign√©es")
        
        # 4. Comparaisons et analyses
        anomalies = []
        verification_ok = True
        
        # Test 1: Comparaison des totaux
        excel_unique = len(df_prepared)
        doublons_excel = len(df_excel) - excel_unique
        delta_total = abs(db_total - excel_unique)
        
        logging.info("üìà ANALYSE COMPARATIVE")
        logging.info(f"  ‚Ä¢ Excel original: {len(df_excel):,}")
        logging.info(f"  ‚Ä¢ Excel unique: {excel_unique:,}")
        logging.info(f"  ‚Ä¢ Doublons Excel supprim√©s: {doublons_excel:,}")
        logging.info(f"  ‚Ä¢ DB total: {db_total:,}")
        logging.info(f"  ‚Ä¢ Delta (DB vs Excel unique): {delta_total:,}")
        
        if delta_total > 0:
            verification_ok = False
            anomalies.append({
                'type': 'DELTA_TOTAL',
                'description': f'DB ({db_total}) != Excel unique ({excel_unique})',
                'valeur_attendue': excel_unique,
                'valeur_reelle': db_total,
                'ecart': delta_total
            })
        
        # Test 2: V√©rification unicit√© cl√©s
        if db_unique_keys != db_total:
            verification_ok = False
            doublons_db = db_total - db_unique_keys
            anomalies.append({
                'type': 'DOUBLONS_DB',
                'description': f'Cl√©s non uniques en DB: {doublons_db} doublons',
                'valeur_attendue': db_total,
                'valeur_reelle': db_unique_keys,
                'ecart': doublons_db
            })
            logging.warning(f"‚ö†Ô∏è Doublons en DB: {doublons_db}")
        
        # Test 3: V√©rification coh√©rence des cl√©s (√©chantillon)
        if db_total > 0:
            with get_conn() as conn:
                sample_db = pd.read_sql_query("""
                    SELECT street_name, house_number, postal_code, addr_key 
                    FROM addresses 
                    LIMIT 100
                """, conn)
                
                # Recalculer les cl√©s et comparer
                from guignomap.imports import build_addr_key
                sample_db['addr_key_recalc'] = sample_db.apply(
                    lambda row: build_addr_key(row['street_name'], row['house_number'], row['postal_code']),
                    axis=1
                )
                
                key_mismatches = sample_db[sample_db['addr_key'] != sample_db['addr_key_recalc']]
                if len(key_mismatches) > 0:
                    verification_ok = False
                    anomalies.append({
                        'type': 'CLES_INCOHERENTES',
                        'description': f'Cl√©s addr_key incoh√©rentes: {len(key_mismatches)} sur 100 √©chantillonn√©es',
                        'valeur_attendue': 0,
                        'valeur_reelle': len(key_mismatches),
                        'ecart': len(key_mismatches)
                    })
                    logging.warning(f"‚ö†Ô∏è Cl√©s incoh√©rentes d√©tect√©es: {len(key_mismatches)}")
        
        # 5. Export des anomalies si n√©cessaire
        if anomalies:
            exports_dir = Path("exports/maintenance")
            exports_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            anomalies_file = exports_dir / f"verify_anomalies_{city}_{timestamp}.csv"
            
            df_anomalies = pd.DataFrame(anomalies)
            df_anomalies.to_csv(anomalies_file, index=False, encoding='utf-8')
            
            logging.warning(f"üìÅ Anomalies export√©es: {anomalies_file}")
        
        # 6. Rapport final
        if verification_ok:
            logging.info("‚úÖ V√âRIFICATION R√âUSSIE - Aucune anomalie d√©tect√©e")
            logging.info(f"  ‚Ä¢ Coh√©rence Excel ‚Üî DB: ‚úÖ")
            logging.info(f"  ‚Ä¢ Unicit√© cl√©s DB: ‚úÖ") 
            logging.info(f"  ‚Ä¢ Int√©grit√© addr_key: ‚úÖ")
        else:
            logging.error("‚ùå ANOMALIES D√âTECT√âES")
            for anomalie in anomalies:
                logging.error(f"  ‚Ä¢ {anomalie['type']}: {anomalie['description']}")
        
        return verification_ok
        
    except Exception as e:
        logging.error(f"‚ùå Erreur lors de la v√©rification: {e}")
        if verbose:
            logging.exception("D√©tails de l'erreur:")
        return False


def main():
    """Point d'entr√©e principal"""
    parser = argparse.ArgumentParser(
        description="V√©rification robuste des adresses GuignoMap",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  %(prog)s --city mascouche --file imports/mascouche_adresses.xlsx
  %(prog)s --city montreal --file data/mtl.xlsx --verbose
        """
    )
    
    parser.add_argument(
        '--city',
        required=True,
        help='Nom de la ville'
    )
    
    parser.add_argument(
        '--file',
        required=True,
        type=Path,
        help='Chemin vers le fichier Excel source'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='Affichage d√©taill√© (debug)'
    )
    
    args = parser.parse_args()
    
    # Ex√©cution de la v√©rification
    success = verify_addresses_robust(args.city, args.file, args.verbose)
    
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())